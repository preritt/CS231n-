{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "  \"\"\"\n",
    "  Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "  it for the linear classifier. These are the same steps as we used for the\n",
    "  SVM, but condensed to a single function.  \n",
    "  \"\"\"\n",
    "  # Load the raw CIFAR-10 data\n",
    "  cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "  X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "  # subsample the data\n",
    "  mask = range(num_training, num_training + num_validation)\n",
    "  X_val = X_train[mask]\n",
    "  y_val = y_train[mask]\n",
    "  mask = range(num_training)\n",
    "  X_train = X_train[mask]\n",
    "  y_train = y_train[mask]\n",
    "  mask = range(num_test)\n",
    "  X_test = X_test[mask]\n",
    "  y_test = y_test[mask]\n",
    "  mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "  X_dev = X_train[mask]\n",
    "  y_dev = y_train[mask]\n",
    "  \n",
    "  # Preprocessing: reshape the image data into rows\n",
    "  X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "  X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "  X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "  X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "  \n",
    "  # Normalize the data: subtract the mean image\n",
    "  mean_image = np.mean(X_train, axis = 0)\n",
    "  X_train -= mean_image\n",
    "  X_val -= mean_image\n",
    "  X_test -= mean_image\n",
    "  X_dev -= mean_image\n",
    "  \n",
    "  # add bias dimension and transform into columns\n",
    "  X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "  X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "  X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "  X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "  \n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print ('Train data shape: ', X_train.shape)\n",
    "print ('Train labels shape: ', y_train.shape)\n",
    "print ('Validation data shape: ', X_val.shape)\n",
    "print ('Validation labels shape: ', y_val.shape)\n",
    "print ('Test data shape: ', X_test.shape)\n",
    "print ('Test labels shape: ', y_test.shape)\n",
    "print ('dev data shape: ', X_dev.shape)\n",
    "print ('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.367704\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print ('loss: %f' % loss)\n",
    "print ('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -1.208439 analytic: -1.208439, relative error: 1.298383e-08\n",
      "numerical: -0.034207 analytic: -0.034207, relative error: 1.795720e-06\n",
      "numerical: -0.800635 analytic: -0.800635, relative error: 1.273949e-08\n",
      "numerical: -1.968532 analytic: -1.968532, relative error: 8.913408e-09\n",
      "numerical: -0.618644 analytic: -0.618644, relative error: 7.803890e-08\n",
      "numerical: 0.451367 analytic: 0.451367, relative error: 5.694971e-08\n",
      "numerical: 2.575662 analytic: 2.575661, relative error: 4.620142e-08\n",
      "numerical: 2.338571 analytic: 2.338571, relative error: 1.789553e-08\n",
      "numerical: -2.540606 analytic: -2.540606, relative error: 1.409694e-08\n",
      "numerical: -1.995076 analytic: -1.995076, relative error: 7.394463e-09\n",
      "numerical: -0.780105 analytic: -0.780105, relative error: 5.314081e-08\n",
      "numerical: -1.282244 analytic: -1.282244, relative error: 2.272730e-08\n",
      "numerical: -0.737350 analytic: -0.737350, relative error: 2.176383e-08\n",
      "numerical: -0.768232 analytic: -0.768232, relative error: 8.573921e-09\n",
      "numerical: -1.430036 analytic: -1.430037, relative error: 4.491434e-08\n",
      "numerical: 2.632227 analytic: 2.632226, relative error: 8.867485e-09\n",
      "numerical: 2.672766 analytic: 2.672766, relative error: 6.786807e-09\n",
      "numerical: 1.485343 analytic: 1.485342, relative error: 3.239413e-08\n",
      "numerical: 1.488505 analytic: 1.488505, relative error: 4.629097e-08\n",
      "numerical: 0.553234 analytic: 0.553234, relative error: 9.934985e-09\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.367704e+00 computed in 0.062501s\n",
      "(500, 10) delta\n",
      "vectorized loss: 2.367704e+00 computed in 0.000000s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print ('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print ('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print ('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print ('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Use the validation set to tune hyperparameters (regularization strength and\n",
    "# # learning rate). You should experiment with different ranges for the learning\n",
    "# # rates and regularization strengths; if you are careful you should be able to\n",
    "# # get a classification accuracy of over 0.35 on the validation set.\n",
    "# from cs231n.classifiers import Softmax\n",
    "# results = {}\n",
    "# best_val = -1\n",
    "# best_softmax = None\n",
    "# learning_rates = [1e-7, 5e-7]\n",
    "# regularization_strengths = [5e4, 1e8]\n",
    "\n",
    "# ################################################################################\n",
    "# # TODO:                                                                        #\n",
    "# # Use the validation set to set the learning rate and regularization strength. #\n",
    "# # This should be identical to the validation that you did for the SVM; save    #\n",
    "# # the best trained softmax classifer in best_softmax.                          #\n",
    "# ################################################################################\n",
    "# pass\n",
    "# ################################################################################\n",
    "# #                              END OF YOUR CODE                                #\n",
    "# ################################################################################\n",
    "    \n",
    "# # Print out results.\n",
    "# for lr, reg in sorted(results):\n",
    "#     train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "#     print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "#                 lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "# print 'best validation accuracy achieved during cross-validation: %f' % best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 5000: loss 5.780843\n",
      "iteration 100 / 5000: loss 5.658726\n",
      "iteration 200 / 5000: loss 5.286932\n",
      "iteration 300 / 5000: loss 5.772909\n",
      "iteration 400 / 5000: loss 5.523474\n",
      "iteration 500 / 5000: loss 5.606298\n",
      "iteration 600 / 5000: loss 5.473097\n",
      "iteration 700 / 5000: loss 5.802136\n",
      "iteration 800 / 5000: loss 5.486262\n",
      "iteration 900 / 5000: loss 6.087635\n",
      "iteration 1000 / 5000: loss 5.310291\n",
      "iteration 1100 / 5000: loss 5.550161\n",
      "iteration 1200 / 5000: loss 4.955121\n",
      "iteration 1300 / 5000: loss 4.888318\n",
      "iteration 1400 / 5000: loss 5.460368\n",
      "iteration 1500 / 5000: loss 5.197233\n",
      "iteration 1600 / 5000: loss 4.980639\n",
      "iteration 1700 / 5000: loss 4.946643\n",
      "iteration 1800 / 5000: loss 5.259288\n",
      "iteration 1900 / 5000: loss 5.523014\n",
      "iteration 2000 / 5000: loss 5.238477\n",
      "iteration 2100 / 5000: loss 5.170833\n",
      "iteration 2200 / 5000: loss 4.865508\n",
      "iteration 2300 / 5000: loss 5.261767\n",
      "iteration 2400 / 5000: loss 4.568608\n",
      "iteration 2500 / 5000: loss 4.565375\n",
      "iteration 2600 / 5000: loss 5.145397\n",
      "iteration 2700 / 5000: loss 4.854328\n",
      "iteration 2800 / 5000: loss 4.948628\n",
      "iteration 2900 / 5000: loss 4.887981\n",
      "iteration 3000 / 5000: loss 4.717014\n",
      "iteration 3100 / 5000: loss 4.859106\n",
      "iteration 3200 / 5000: loss 5.216318\n",
      "iteration 3300 / 5000: loss 4.838052\n",
      "iteration 3400 / 5000: loss 4.626568\n",
      "iteration 3500 / 5000: loss 4.458786\n",
      "iteration 3600 / 5000: loss 4.603611\n",
      "iteration 3700 / 5000: loss 4.888079\n",
      "iteration 3800 / 5000: loss 4.426809\n",
      "iteration 3900 / 5000: loss 4.788782\n",
      "iteration 4000 / 5000: loss 4.597318\n",
      "iteration 4100 / 5000: loss 4.496314\n",
      "iteration 4200 / 5000: loss 4.548700\n",
      "iteration 4300 / 5000: loss 4.599298\n",
      "iteration 4400 / 5000: loss 4.920980\n",
      "iteration 4500 / 5000: loss 4.599771\n",
      "iteration 4600 / 5000: loss 4.329153\n",
      "iteration 4700 / 5000: loss 4.179287\n",
      "iteration 4800 / 5000: loss 4.105243\n",
      "iteration 4900 / 5000: loss 4.488805\n",
      "training accuracy: 0.126347\n",
      "validation accuracy: 0.127000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 7.347262\n",
      "iteration 100 / 5000: loss 7.752653\n",
      "iteration 200 / 5000: loss 6.626905\n",
      "iteration 300 / 5000: loss 6.914875\n",
      "iteration 400 / 5000: loss 7.293186\n",
      "iteration 500 / 5000: loss 6.989886\n",
      "iteration 600 / 5000: loss 6.832192\n",
      "iteration 700 / 5000: loss 7.113494\n",
      "iteration 800 / 5000: loss 6.937626\n",
      "iteration 900 / 5000: loss 6.558070\n",
      "iteration 1000 / 5000: loss 6.438706\n",
      "iteration 1100 / 5000: loss 6.982654\n",
      "iteration 1200 / 5000: loss 7.442137\n",
      "iteration 1300 / 5000: loss 6.782202\n",
      "iteration 1400 / 5000: loss 6.848452\n",
      "iteration 1500 / 5000: loss 6.760850\n",
      "iteration 1600 / 5000: loss 6.824814\n",
      "iteration 1700 / 5000: loss 6.408376\n",
      "iteration 1800 / 5000: loss 7.290968\n",
      "iteration 1900 / 5000: loss 6.533459\n",
      "iteration 2000 / 5000: loss 6.822317\n",
      "iteration 2100 / 5000: loss 7.306136\n",
      "iteration 2200 / 5000: loss 6.981398\n",
      "iteration 2300 / 5000: loss 6.674675\n",
      "iteration 2400 / 5000: loss 6.235841\n",
      "iteration 2500 / 5000: loss 6.620371\n",
      "iteration 2600 / 5000: loss 6.709998\n",
      "iteration 2700 / 5000: loss 6.596048\n",
      "iteration 2800 / 5000: loss 6.689745\n",
      "iteration 2900 / 5000: loss 6.595918\n",
      "iteration 3000 / 5000: loss 6.505719\n",
      "iteration 3100 / 5000: loss 6.445388\n",
      "iteration 3200 / 5000: loss 6.831374\n",
      "iteration 3300 / 5000: loss 5.924480\n",
      "iteration 3400 / 5000: loss 6.193870\n",
      "iteration 3500 / 5000: loss 6.733344\n",
      "iteration 3600 / 5000: loss 6.655594\n",
      "iteration 3700 / 5000: loss 6.427080\n",
      "iteration 3800 / 5000: loss 6.096846\n",
      "iteration 3900 / 5000: loss 6.659682\n",
      "iteration 4000 / 5000: loss 6.699960\n",
      "iteration 4100 / 5000: loss 6.260095\n",
      "iteration 4200 / 5000: loss 6.127335\n",
      "iteration 4300 / 5000: loss 6.110816\n",
      "iteration 4400 / 5000: loss 6.525081\n",
      "iteration 4500 / 5000: loss 5.993389\n",
      "iteration 4600 / 5000: loss 6.484874\n",
      "iteration 4700 / 5000: loss 6.232083\n",
      "iteration 4800 / 5000: loss 6.303201\n",
      "iteration 4900 / 5000: loss 6.668574\n",
      "training accuracy: 0.123714\n",
      "validation accuracy: 0.114000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 21.851002\n",
      "iteration 100 / 5000: loss 21.614063\n",
      "iteration 200 / 5000: loss 21.102053\n",
      "iteration 300 / 5000: loss 22.054741\n",
      "iteration 400 / 5000: loss 21.280278\n",
      "iteration 500 / 5000: loss 21.174249\n",
      "iteration 600 / 5000: loss 21.280243\n",
      "iteration 700 / 5000: loss 21.602482\n",
      "iteration 800 / 5000: loss 21.426017\n",
      "iteration 900 / 5000: loss 21.445242\n",
      "iteration 1000 / 5000: loss 21.333039\n",
      "iteration 1100 / 5000: loss 21.108994\n",
      "iteration 1200 / 5000: loss 21.460680\n",
      "iteration 1300 / 5000: loss 20.986928\n",
      "iteration 1400 / 5000: loss 21.399478\n",
      "iteration 1500 / 5000: loss 20.876789\n",
      "iteration 1600 / 5000: loss 20.913897\n",
      "iteration 1700 / 5000: loss 20.789471\n",
      "iteration 1800 / 5000: loss 20.677779\n",
      "iteration 1900 / 5000: loss 21.101176\n",
      "iteration 2000 / 5000: loss 20.643729\n",
      "iteration 2100 / 5000: loss 20.868447\n",
      "iteration 2200 / 5000: loss 20.955793\n",
      "iteration 2300 / 5000: loss 20.835921\n",
      "iteration 2400 / 5000: loss 20.580843\n",
      "iteration 2500 / 5000: loss 20.570847\n",
      "iteration 2600 / 5000: loss 20.972821\n",
      "iteration 2700 / 5000: loss 20.423151\n",
      "iteration 2800 / 5000: loss 20.942487\n",
      "iteration 2900 / 5000: loss 20.616276\n",
      "iteration 3000 / 5000: loss 20.552707\n",
      "iteration 3100 / 5000: loss 20.436541\n",
      "iteration 3200 / 5000: loss 21.040198\n",
      "iteration 3300 / 5000: loss 20.531308\n",
      "iteration 3400 / 5000: loss 20.240687\n",
      "iteration 3500 / 5000: loss 20.553708\n",
      "iteration 3600 / 5000: loss 20.056951\n",
      "iteration 3700 / 5000: loss 20.314829\n",
      "iteration 3800 / 5000: loss 20.557158\n",
      "iteration 3900 / 5000: loss 20.481790\n",
      "iteration 4000 / 5000: loss 20.450614\n",
      "iteration 4100 / 5000: loss 20.408971\n",
      "iteration 4200 / 5000: loss 20.209103\n",
      "iteration 4300 / 5000: loss 20.154514\n",
      "iteration 4400 / 5000: loss 19.970113\n",
      "iteration 4500 / 5000: loss 20.274153\n",
      "iteration 4600 / 5000: loss 20.013791\n",
      "iteration 4700 / 5000: loss 20.179833\n",
      "iteration 4800 / 5000: loss 20.091401\n",
      "iteration 4900 / 5000: loss 20.192070\n",
      "training accuracy: 0.103816\n",
      "validation accuracy: 0.108000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 83.638601\n",
      "iteration 100 / 5000: loss 83.383987\n",
      "iteration 200 / 5000: loss 82.951753\n",
      "iteration 300 / 5000: loss 82.714493\n",
      "iteration 400 / 5000: loss 83.000293\n",
      "iteration 500 / 5000: loss 83.295503\n",
      "iteration 600 / 5000: loss 83.163122\n",
      "iteration 700 / 5000: loss 82.504709\n",
      "iteration 800 / 5000: loss 82.575796\n",
      "iteration 900 / 5000: loss 82.183099\n",
      "iteration 1000 / 5000: loss 82.346498\n",
      "iteration 1100 / 5000: loss 81.932926\n",
      "iteration 1200 / 5000: loss 81.548383\n",
      "iteration 1300 / 5000: loss 81.904218\n",
      "iteration 1400 / 5000: loss 81.718313\n",
      "iteration 1500 / 5000: loss 81.267474\n",
      "iteration 1600 / 5000: loss 81.909951\n",
      "iteration 1700 / 5000: loss 81.934857\n",
      "iteration 1800 / 5000: loss 81.433128\n",
      "iteration 1900 / 5000: loss 80.871518\n",
      "iteration 2000 / 5000: loss 80.897608\n",
      "iteration 2100 / 5000: loss 80.694090\n",
      "iteration 2200 / 5000: loss 81.208184\n",
      "iteration 2300 / 5000: loss 81.229722\n",
      "iteration 2400 / 5000: loss 80.486306\n",
      "iteration 2500 / 5000: loss 80.327732\n",
      "iteration 2600 / 5000: loss 80.456581\n",
      "iteration 2700 / 5000: loss 80.113371\n",
      "iteration 2800 / 5000: loss 80.377218\n",
      "iteration 2900 / 5000: loss 79.927602\n",
      "iteration 3000 / 5000: loss 80.151940\n",
      "iteration 3100 / 5000: loss 79.886491\n",
      "iteration 3200 / 5000: loss 79.781734\n",
      "iteration 3300 / 5000: loss 79.724051\n",
      "iteration 3400 / 5000: loss 79.720305\n",
      "iteration 3500 / 5000: loss 79.417598\n",
      "iteration 3600 / 5000: loss 79.137031\n",
      "iteration 3700 / 5000: loss 78.750608\n",
      "iteration 3800 / 5000: loss 78.891363\n",
      "iteration 3900 / 5000: loss 79.279544\n",
      "iteration 4000 / 5000: loss 78.948722\n",
      "iteration 4100 / 5000: loss 78.748658\n",
      "iteration 4200 / 5000: loss 78.558064\n",
      "iteration 4300 / 5000: loss 78.559242\n",
      "iteration 4400 / 5000: loss 78.380361\n",
      "iteration 4500 / 5000: loss 78.409043\n",
      "iteration 4600 / 5000: loss 78.124842\n",
      "iteration 4700 / 5000: loss 78.394269\n",
      "iteration 4800 / 5000: loss 77.780430\n",
      "iteration 4900 / 5000: loss 78.333503\n",
      "training accuracy: 0.089816\n",
      "validation accuracy: 0.105000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 157.592539\n",
      "iteration 100 / 5000: loss 157.481512\n",
      "iteration 200 / 5000: loss 157.527167\n",
      "iteration 300 / 5000: loss 156.649657\n",
      "iteration 400 / 5000: loss 156.745436\n",
      "iteration 500 / 5000: loss 155.849172\n",
      "iteration 600 / 5000: loss 155.516240\n",
      "iteration 700 / 5000: loss 155.178075\n",
      "iteration 800 / 5000: loss 154.977166\n",
      "iteration 900 / 5000: loss 154.516806\n",
      "iteration 1000 / 5000: loss 154.705021\n",
      "iteration 1100 / 5000: loss 153.865176\n",
      "iteration 1200 / 5000: loss 153.873580\n",
      "iteration 1300 / 5000: loss 153.589113\n",
      "iteration 1400 / 5000: loss 153.660661\n",
      "iteration 1500 / 5000: loss 152.584386\n",
      "iteration 1600 / 5000: loss 152.517212\n",
      "iteration 1700 / 5000: loss 152.106741\n",
      "iteration 1800 / 5000: loss 152.004782\n",
      "iteration 1900 / 5000: loss 151.397931\n",
      "iteration 2000 / 5000: loss 151.140516\n",
      "iteration 2100 / 5000: loss 150.547347\n",
      "iteration 2200 / 5000: loss 150.306683\n",
      "iteration 2300 / 5000: loss 150.458559\n",
      "iteration 2400 / 5000: loss 149.652330\n",
      "iteration 2500 / 5000: loss 149.490920\n",
      "iteration 2600 / 5000: loss 149.505173\n",
      "iteration 2700 / 5000: loss 148.550262\n",
      "iteration 2800 / 5000: loss 148.837225\n",
      "iteration 2900 / 5000: loss 148.298797\n",
      "iteration 3000 / 5000: loss 147.991939\n",
      "iteration 3100 / 5000: loss 147.622232\n",
      "iteration 3200 / 5000: loss 147.268044\n",
      "iteration 3300 / 5000: loss 146.525344\n",
      "iteration 3400 / 5000: loss 146.795446\n",
      "iteration 3500 / 5000: loss 146.303094\n",
      "iteration 3600 / 5000: loss 146.082057\n",
      "iteration 3700 / 5000: loss 145.697558\n",
      "iteration 3800 / 5000: loss 145.357598\n",
      "iteration 3900 / 5000: loss 145.191681\n",
      "iteration 4000 / 5000: loss 144.857374\n",
      "iteration 4100 / 5000: loss 145.087710\n",
      "iteration 4200 / 5000: loss 144.601719\n",
      "iteration 4300 / 5000: loss 143.732600\n",
      "iteration 4400 / 5000: loss 143.836916\n",
      "iteration 4500 / 5000: loss 142.978632\n",
      "iteration 4600 / 5000: loss 142.765502\n",
      "iteration 4700 / 5000: loss 142.732055\n",
      "iteration 4800 / 5000: loss 142.816130\n",
      "iteration 4900 / 5000: loss 142.238548\n",
      "training accuracy: 0.129408\n",
      "validation accuracy: 0.132000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 5.684569\n",
      "iteration 100 / 5000: loss 4.170312\n",
      "iteration 200 / 5000: loss 3.613678\n",
      "iteration 300 / 5000: loss 3.764727\n",
      "iteration 400 / 5000: loss 3.100798\n",
      "iteration 500 / 5000: loss 3.134759\n",
      "iteration 600 / 5000: loss 3.035860\n",
      "iteration 700 / 5000: loss 3.169838\n",
      "iteration 800 / 5000: loss 3.101803\n",
      "iteration 900 / 5000: loss 3.077622\n",
      "iteration 1000 / 5000: loss 2.949032\n",
      "iteration 1100 / 5000: loss 2.619503\n",
      "iteration 1200 / 5000: loss 2.617370\n",
      "iteration 1300 / 5000: loss 2.644629\n",
      "iteration 1400 / 5000: loss 2.345727\n",
      "iteration 1500 / 5000: loss 2.610306\n",
      "iteration 1600 / 5000: loss 2.547747\n",
      "iteration 1700 / 5000: loss 2.397960\n",
      "iteration 1800 / 5000: loss 2.568131\n",
      "iteration 1900 / 5000: loss 2.611873\n",
      "iteration 2000 / 5000: loss 2.538359\n",
      "iteration 2100 / 5000: loss 2.506485\n",
      "iteration 2200 / 5000: loss 2.550064\n",
      "iteration 2300 / 5000: loss 2.452419\n",
      "iteration 2400 / 5000: loss 2.406069\n",
      "iteration 2500 / 5000: loss 2.267057\n",
      "iteration 2600 / 5000: loss 2.406343\n",
      "iteration 2700 / 5000: loss 2.786336\n",
      "iteration 2800 / 5000: loss 2.317253\n",
      "iteration 2900 / 5000: loss 2.256412\n",
      "iteration 3000 / 5000: loss 2.278066\n",
      "iteration 3100 / 5000: loss 2.243756\n",
      "iteration 3200 / 5000: loss 2.297644\n",
      "iteration 3300 / 5000: loss 2.318990\n",
      "iteration 3400 / 5000: loss 2.304408\n",
      "iteration 3500 / 5000: loss 2.353286\n",
      "iteration 3600 / 5000: loss 2.412788\n",
      "iteration 3700 / 5000: loss 2.453030\n",
      "iteration 3800 / 5000: loss 2.308433\n",
      "iteration 3900 / 5000: loss 2.500511\n",
      "iteration 4000 / 5000: loss 2.380509\n",
      "iteration 4100 / 5000: loss 2.229054\n",
      "iteration 4200 / 5000: loss 2.299961\n",
      "iteration 4300 / 5000: loss 2.207590\n",
      "iteration 4400 / 5000: loss 2.115921\n",
      "iteration 4500 / 5000: loss 2.325571\n",
      "iteration 4600 / 5000: loss 2.217224\n",
      "iteration 4700 / 5000: loss 2.354349\n",
      "iteration 4800 / 5000: loss 2.466263\n",
      "iteration 4900 / 5000: loss 2.320764\n",
      "training accuracy: 0.303571\n",
      "validation accuracy: 0.294000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 8.047316\n",
      "iteration 100 / 5000: loss 5.775706\n",
      "iteration 200 / 5000: loss 5.202885\n",
      "iteration 300 / 5000: loss 5.171606\n",
      "iteration 400 / 5000: loss 4.885627\n",
      "iteration 500 / 5000: loss 4.943864\n",
      "iteration 600 / 5000: loss 4.784387\n",
      "iteration 700 / 5000: loss 4.540221\n",
      "iteration 800 / 5000: loss 4.523789\n",
      "iteration 900 / 5000: loss 4.365262\n",
      "iteration 1000 / 5000: loss 4.152406\n",
      "iteration 1100 / 5000: loss 4.255610\n",
      "iteration 1200 / 5000: loss 4.144934\n",
      "iteration 1300 / 5000: loss 4.121063\n",
      "iteration 1400 / 5000: loss 4.015813\n",
      "iteration 1500 / 5000: loss 4.031303\n",
      "iteration 1600 / 5000: loss 3.882744\n",
      "iteration 1700 / 5000: loss 3.977997\n",
      "iteration 1800 / 5000: loss 3.869689\n",
      "iteration 1900 / 5000: loss 3.887908\n",
      "iteration 2000 / 5000: loss 4.046229\n",
      "iteration 2100 / 5000: loss 3.915512\n",
      "iteration 2200 / 5000: loss 4.012989\n",
      "iteration 2300 / 5000: loss 4.028039\n",
      "iteration 2400 / 5000: loss 3.931718\n",
      "iteration 2500 / 5000: loss 3.892152\n",
      "iteration 2600 / 5000: loss 3.810258\n",
      "iteration 2700 / 5000: loss 3.775699\n",
      "iteration 2800 / 5000: loss 3.842797\n",
      "iteration 2900 / 5000: loss 3.731465\n",
      "iteration 3000 / 5000: loss 3.656260\n",
      "iteration 3100 / 5000: loss 3.637755\n",
      "iteration 3200 / 5000: loss 3.689418\n",
      "iteration 3300 / 5000: loss 3.949301\n",
      "iteration 3400 / 5000: loss 3.672460\n",
      "iteration 3500 / 5000: loss 3.612642\n",
      "iteration 3600 / 5000: loss 3.731513\n",
      "iteration 3700 / 5000: loss 3.570273\n",
      "iteration 3800 / 5000: loss 3.772624\n",
      "iteration 3900 / 5000: loss 3.546957\n",
      "iteration 4000 / 5000: loss 3.732011\n",
      "iteration 4100 / 5000: loss 3.660487\n",
      "iteration 4200 / 5000: loss 3.465025\n",
      "iteration 4300 / 5000: loss 3.542059\n",
      "iteration 4400 / 5000: loss 3.538845\n",
      "iteration 4500 / 5000: loss 3.439218\n",
      "iteration 4600 / 5000: loss 3.493976\n",
      "iteration 4700 / 5000: loss 3.553427\n",
      "iteration 4800 / 5000: loss 3.370590\n",
      "iteration 4900 / 5000: loss 3.546714\n",
      "training accuracy: 0.303041\n",
      "validation accuracy: 0.303000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 22.382675\n",
      "iteration 100 / 5000: loss 19.456118\n",
      "iteration 200 / 5000: loss 18.850751\n",
      "iteration 300 / 5000: loss 18.252137\n",
      "iteration 400 / 5000: loss 17.412718\n",
      "iteration 500 / 5000: loss 16.992482\n",
      "iteration 600 / 5000: loss 16.677012\n",
      "iteration 700 / 5000: loss 16.186799\n",
      "iteration 800 / 5000: loss 15.651880\n",
      "iteration 900 / 5000: loss 15.498823\n",
      "iteration 1000 / 5000: loss 14.997272\n",
      "iteration 1100 / 5000: loss 14.963839\n",
      "iteration 1200 / 5000: loss 14.719830\n",
      "iteration 1300 / 5000: loss 14.251844\n",
      "iteration 1400 / 5000: loss 14.080174\n",
      "iteration 1500 / 5000: loss 13.916650\n",
      "iteration 1600 / 5000: loss 13.514246\n",
      "iteration 1700 / 5000: loss 13.183122\n",
      "iteration 1800 / 5000: loss 12.758155\n",
      "iteration 1900 / 5000: loss 12.672651\n",
      "iteration 2000 / 5000: loss 12.364079\n",
      "iteration 2100 / 5000: loss 12.309723\n",
      "iteration 2200 / 5000: loss 11.931752\n",
      "iteration 2300 / 5000: loss 11.889209\n",
      "iteration 2400 / 5000: loss 11.592911\n",
      "iteration 2500 / 5000: loss 11.364707\n",
      "iteration 2600 / 5000: loss 11.007104\n",
      "iteration 2700 / 5000: loss 10.951891\n",
      "iteration 2800 / 5000: loss 10.756395\n",
      "iteration 2900 / 5000: loss 10.437361\n",
      "iteration 3000 / 5000: loss 10.371997\n",
      "iteration 3100 / 5000: loss 10.137153\n",
      "iteration 3200 / 5000: loss 10.145643\n",
      "iteration 3300 / 5000: loss 9.738664\n",
      "iteration 3400 / 5000: loss 9.675906\n",
      "iteration 3500 / 5000: loss 9.486580\n",
      "iteration 3600 / 5000: loss 9.167494\n",
      "iteration 3700 / 5000: loss 9.098782\n",
      "iteration 3800 / 5000: loss 8.997007\n",
      "iteration 3900 / 5000: loss 8.753744\n",
      "iteration 4000 / 5000: loss 8.763903\n",
      "iteration 4100 / 5000: loss 8.735462\n",
      "iteration 4200 / 5000: loss 8.416793\n",
      "iteration 4300 / 5000: loss 8.152131\n",
      "iteration 4400 / 5000: loss 8.212207\n",
      "iteration 4500 / 5000: loss 8.106144\n",
      "iteration 4600 / 5000: loss 7.950950\n",
      "iteration 4700 / 5000: loss 7.816955\n",
      "iteration 4800 / 5000: loss 7.564227\n",
      "iteration 4900 / 5000: loss 7.409341\n",
      "training accuracy: 0.335102\n",
      "validation accuracy: 0.326000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 81.558192\n",
      "iteration 100 / 5000: loss 73.175480\n",
      "iteration 200 / 5000: loss 66.086355\n",
      "iteration 300 / 5000: loss 59.635727\n",
      "iteration 400 / 5000: loss 53.798359\n",
      "iteration 500 / 5000: loss 48.886749\n",
      "iteration 600 / 5000: loss 44.405686\n",
      "iteration 700 / 5000: loss 39.892349\n",
      "iteration 800 / 5000: loss 36.347720\n",
      "iteration 900 / 5000: loss 33.137182\n",
      "iteration 1000 / 5000: loss 30.086855\n",
      "iteration 1100 / 5000: loss 27.374811\n",
      "iteration 1200 / 5000: loss 24.905799\n",
      "iteration 1300 / 5000: loss 22.640015\n",
      "iteration 1400 / 5000: loss 20.707448\n",
      "iteration 1500 / 5000: loss 18.850494\n",
      "iteration 1600 / 5000: loss 17.231723\n",
      "iteration 1700 / 5000: loss 15.730104\n",
      "iteration 1800 / 5000: loss 14.360256\n",
      "iteration 1900 / 5000: loss 13.166371\n",
      "iteration 2000 / 5000: loss 12.143740\n",
      "iteration 2100 / 5000: loss 11.093824\n",
      "iteration 2200 / 5000: loss 10.162258\n",
      "iteration 2300 / 5000: loss 9.490348\n",
      "iteration 2400 / 5000: loss 8.750106\n",
      "iteration 2500 / 5000: loss 8.123444\n",
      "iteration 2600 / 5000: loss 7.432625\n",
      "iteration 2700 / 5000: loss 6.853248\n",
      "iteration 2800 / 5000: loss 6.444600\n",
      "iteration 2900 / 5000: loss 5.925825\n",
      "iteration 3000 / 5000: loss 5.487698\n",
      "iteration 3100 / 5000: loss 5.246344\n",
      "iteration 3200 / 5000: loss 4.969608\n",
      "iteration 3300 / 5000: loss 4.610356\n",
      "iteration 3400 / 5000: loss 4.406242\n",
      "iteration 3500 / 5000: loss 4.135063\n",
      "iteration 3600 / 5000: loss 4.057740\n",
      "iteration 3700 / 5000: loss 3.861317\n",
      "iteration 3800 / 5000: loss 3.611081\n",
      "iteration 3900 / 5000: loss 3.380903\n",
      "iteration 4000 / 5000: loss 3.339234\n",
      "iteration 4100 / 5000: loss 3.084833\n",
      "iteration 4200 / 5000: loss 3.027803\n",
      "iteration 4300 / 5000: loss 2.835561\n",
      "iteration 4400 / 5000: loss 2.837609\n",
      "iteration 4500 / 5000: loss 2.713753\n",
      "iteration 4600 / 5000: loss 2.622988\n",
      "iteration 4700 / 5000: loss 2.520063\n",
      "iteration 4800 / 5000: loss 2.550596\n",
      "iteration 4900 / 5000: loss 2.433444\n",
      "training accuracy: 0.387265\n",
      "validation accuracy: 0.386000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 159.719486\n",
      "iteration 100 / 5000: loss 127.968206\n",
      "iteration 200 / 5000: loss 104.594247\n",
      "iteration 300 / 5000: loss 85.926612\n",
      "iteration 400 / 5000: loss 70.289979\n",
      "iteration 500 / 5000: loss 57.904234\n",
      "iteration 600 / 5000: loss 47.351788\n",
      "iteration 700 / 5000: loss 39.338103\n",
      "iteration 800 / 5000: loss 32.438719\n",
      "iteration 900 / 5000: loss 26.854522\n",
      "iteration 1000 / 5000: loss 22.334337\n",
      "iteration 1100 / 5000: loss 18.610455\n",
      "iteration 1200 / 5000: loss 15.480098\n",
      "iteration 1300 / 5000: loss 13.041783\n",
      "iteration 1400 / 5000: loss 11.033346\n",
      "iteration 1500 / 5000: loss 9.357041\n",
      "iteration 1600 / 5000: loss 7.979530\n",
      "iteration 1700 / 5000: loss 6.860894\n",
      "iteration 1800 / 5000: loss 6.002752\n",
      "iteration 1900 / 5000: loss 5.311407\n",
      "iteration 2000 / 5000: loss 4.582112\n",
      "iteration 2100 / 5000: loss 4.175240\n",
      "iteration 2200 / 5000: loss 3.768337\n",
      "iteration 2300 / 5000: loss 3.448691\n",
      "iteration 2400 / 5000: loss 3.018766\n",
      "iteration 2500 / 5000: loss 2.914571\n",
      "iteration 2600 / 5000: loss 2.681371\n",
      "iteration 2700 / 5000: loss 2.696348\n",
      "iteration 2800 / 5000: loss 2.468047\n",
      "iteration 2900 / 5000: loss 2.401813\n",
      "iteration 3000 / 5000: loss 2.299769\n",
      "iteration 3100 / 5000: loss 2.295204\n",
      "iteration 3200 / 5000: loss 2.163051\n",
      "iteration 3300 / 5000: loss 2.117042\n",
      "iteration 3400 / 5000: loss 2.111191\n",
      "iteration 3500 / 5000: loss 2.133116\n",
      "iteration 3600 / 5000: loss 2.013696\n",
      "iteration 3700 / 5000: loss 2.087538\n",
      "iteration 3800 / 5000: loss 1.983229\n",
      "iteration 3900 / 5000: loss 2.011958\n",
      "iteration 4000 / 5000: loss 2.033373\n",
      "iteration 4100 / 5000: loss 1.974911\n",
      "iteration 4200 / 5000: loss 1.918907\n",
      "iteration 4300 / 5000: loss 1.912577\n",
      "iteration 4400 / 5000: loss 1.999062\n",
      "iteration 4500 / 5000: loss 1.934160\n",
      "iteration 4600 / 5000: loss 1.997053\n",
      "iteration 4700 / 5000: loss 1.929011\n",
      "iteration 4800 / 5000: loss 1.882992\n",
      "iteration 4900 / 5000: loss 1.978261\n",
      "training accuracy: 0.376082\n",
      "validation accuracy: 0.389000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 5.802058\n",
      "iteration 100 / 5000: loss 15.132485\n",
      "iteration 200 / 5000: loss 17.200541\n",
      "iteration 300 / 5000: loss 12.377451\n",
      "iteration 400 / 5000: loss 15.175059\n",
      "iteration 500 / 5000: loss 13.907977\n",
      "iteration 600 / 5000: loss 14.162264\n",
      "iteration 700 / 5000: loss 16.992102\n",
      "iteration 800 / 5000: loss 11.792643\n",
      "iteration 900 / 5000: loss 14.374467\n",
      "iteration 1000 / 5000: loss 8.014388\n",
      "iteration 1100 / 5000: loss 12.492766\n",
      "iteration 1200 / 5000: loss 13.400327\n",
      "iteration 1300 / 5000: loss 14.677810\n",
      "iteration 1400 / 5000: loss 18.311259\n",
      "iteration 1500 / 5000: loss 12.410712\n",
      "iteration 1600 / 5000: loss 12.593321\n",
      "iteration 1700 / 5000: loss 15.050312\n",
      "iteration 1800 / 5000: loss 16.192294\n",
      "iteration 1900 / 5000: loss 15.486789\n",
      "iteration 2000 / 5000: loss 10.587561\n",
      "iteration 2100 / 5000: loss 12.648967\n",
      "iteration 2200 / 5000: loss 13.272727\n",
      "iteration 2300 / 5000: loss 13.851636\n",
      "iteration 2400 / 5000: loss 18.742710\n",
      "iteration 2500 / 5000: loss 13.444527\n",
      "iteration 2600 / 5000: loss 23.365370\n",
      "iteration 2700 / 5000: loss 10.388359\n",
      "iteration 2800 / 5000: loss 21.545257\n",
      "iteration 2900 / 5000: loss 13.388050\n",
      "iteration 3000 / 5000: loss 14.205567\n",
      "iteration 3100 / 5000: loss 10.620271\n",
      "iteration 3200 / 5000: loss 12.430473\n",
      "iteration 3300 / 5000: loss 12.936604\n",
      "iteration 3400 / 5000: loss 12.512998\n",
      "iteration 3500 / 5000: loss 13.681879\n",
      "iteration 3600 / 5000: loss 15.187080\n",
      "iteration 3700 / 5000: loss 15.914137\n",
      "iteration 3800 / 5000: loss 14.109733\n",
      "iteration 3900 / 5000: loss 17.478598\n",
      "iteration 4000 / 5000: loss 20.489371\n",
      "iteration 4100 / 5000: loss 9.648724\n",
      "iteration 4200 / 5000: loss 17.538858\n",
      "iteration 4300 / 5000: loss 11.829027\n",
      "iteration 4400 / 5000: loss 10.499344\n",
      "iteration 4500 / 5000: loss 10.068207\n",
      "iteration 4600 / 5000: loss 18.148211\n",
      "iteration 4700 / 5000: loss 12.523999\n",
      "iteration 4800 / 5000: loss 12.919091\n",
      "iteration 4900 / 5000: loss 12.019598\n",
      "training accuracy: 0.318082\n",
      "validation accuracy: 0.289000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 7.292942\n",
      "iteration 100 / 5000: loss 16.461255\n",
      "iteration 200 / 5000: loss 15.528694\n",
      "iteration 300 / 5000: loss 19.043819\n",
      "iteration 400 / 5000: loss 10.286093\n",
      "iteration 500 / 5000: loss 15.451675\n",
      "iteration 600 / 5000: loss 11.693327\n",
      "iteration 700 / 5000: loss 8.699298\n",
      "iteration 800 / 5000: loss 15.793929\n",
      "iteration 900 / 5000: loss 18.241399\n",
      "iteration 1000 / 5000: loss 20.165455\n",
      "iteration 1100 / 5000: loss 17.318676\n",
      "iteration 1200 / 5000: loss 13.551388\n",
      "iteration 1300 / 5000: loss 16.619254\n",
      "iteration 1400 / 5000: loss 16.623739\n",
      "iteration 1500 / 5000: loss 14.820319\n",
      "iteration 1600 / 5000: loss 12.656673\n",
      "iteration 1700 / 5000: loss 12.656292\n",
      "iteration 1800 / 5000: loss 14.877352\n",
      "iteration 1900 / 5000: loss 15.445706\n",
      "iteration 2000 / 5000: loss 17.174258\n",
      "iteration 2100 / 5000: loss 14.767709\n",
      "iteration 2200 / 5000: loss 15.158825\n",
      "iteration 2300 / 5000: loss 13.164091\n",
      "iteration 2400 / 5000: loss 9.447500\n",
      "iteration 2500 / 5000: loss 19.638286\n",
      "iteration 2600 / 5000: loss 14.724904\n",
      "iteration 2700 / 5000: loss 20.867209\n",
      "iteration 2800 / 5000: loss 19.081891\n",
      "iteration 2900 / 5000: loss 16.440204\n",
      "iteration 3000 / 5000: loss 16.008782\n",
      "iteration 3100 / 5000: loss 16.872829\n",
      "iteration 3200 / 5000: loss 10.182781\n",
      "iteration 3300 / 5000: loss 14.903497\n",
      "iteration 3400 / 5000: loss 20.790802\n",
      "iteration 3500 / 5000: loss 16.220612\n",
      "iteration 3600 / 5000: loss 15.575204\n",
      "iteration 3700 / 5000: loss 8.669111\n",
      "iteration 3800 / 5000: loss 17.384267\n",
      "iteration 3900 / 5000: loss 12.769225\n",
      "iteration 4000 / 5000: loss 10.969633\n",
      "iteration 4100 / 5000: loss 17.954251\n",
      "iteration 4200 / 5000: loss 13.274672\n",
      "iteration 4300 / 5000: loss 9.066408\n",
      "iteration 4400 / 5000: loss 19.094642\n",
      "iteration 4500 / 5000: loss 14.353961\n",
      "iteration 4600 / 5000: loss 22.885651\n",
      "iteration 4700 / 5000: loss 19.600748\n",
      "iteration 4800 / 5000: loss 21.432491\n",
      "iteration 4900 / 5000: loss 19.282076\n",
      "training accuracy: 0.244510\n",
      "validation accuracy: 0.244000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 22.108967\n",
      "iteration 100 / 5000: loss 26.394948\n",
      "iteration 200 / 5000: loss 24.083191\n",
      "iteration 300 / 5000: loss 14.236807\n",
      "iteration 400 / 5000: loss 18.506913\n",
      "iteration 500 / 5000: loss 25.728807\n",
      "iteration 600 / 5000: loss 15.977599\n",
      "iteration 700 / 5000: loss 23.082884\n",
      "iteration 800 / 5000: loss 19.535240\n",
      "iteration 900 / 5000: loss 14.379502\n",
      "iteration 1000 / 5000: loss 13.319452\n",
      "iteration 1100 / 5000: loss 19.413478\n",
      "iteration 1200 / 5000: loss 18.912233\n",
      "iteration 1300 / 5000: loss 20.582071\n",
      "iteration 1400 / 5000: loss 15.865905\n",
      "iteration 1500 / 5000: loss 15.764572\n",
      "iteration 1600 / 5000: loss 18.762095\n",
      "iteration 1700 / 5000: loss 18.990506\n",
      "iteration 1800 / 5000: loss 12.701992\n",
      "iteration 1900 / 5000: loss 19.836836\n",
      "iteration 2000 / 5000: loss 18.661703\n",
      "iteration 2100 / 5000: loss 21.012143\n",
      "iteration 2200 / 5000: loss 26.487501\n",
      "iteration 2300 / 5000: loss 14.731721\n",
      "iteration 2400 / 5000: loss 18.832006\n",
      "iteration 2500 / 5000: loss 21.721564\n",
      "iteration 2600 / 5000: loss 24.083156\n",
      "iteration 2700 / 5000: loss 17.518469\n",
      "iteration 2800 / 5000: loss 17.798610\n",
      "iteration 2900 / 5000: loss 17.845076\n",
      "iteration 3000 / 5000: loss 15.712801\n",
      "iteration 3100 / 5000: loss 14.846037\n",
      "iteration 3200 / 5000: loss 17.703235\n",
      "iteration 3300 / 5000: loss 15.124317\n",
      "iteration 3400 / 5000: loss 21.933306\n",
      "iteration 3500 / 5000: loss 14.073423\n",
      "iteration 3600 / 5000: loss 18.559852\n",
      "iteration 3700 / 5000: loss 22.918163\n",
      "iteration 3800 / 5000: loss 18.970962\n",
      "iteration 3900 / 5000: loss 23.317757\n",
      "iteration 4000 / 5000: loss 20.975435\n",
      "iteration 4100 / 5000: loss 21.418499\n",
      "iteration 4200 / 5000: loss 25.111113\n",
      "iteration 4300 / 5000: loss 17.985938\n",
      "iteration 4400 / 5000: loss 11.297371\n",
      "iteration 4500 / 5000: loss 21.757557\n",
      "iteration 4600 / 5000: loss 20.937610\n",
      "iteration 4700 / 5000: loss 25.299030\n",
      "iteration 4800 / 5000: loss 12.637503\n",
      "iteration 4900 / 5000: loss 21.481692\n",
      "training accuracy: 0.186857\n",
      "validation accuracy: 0.195000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 81.385879\n",
      "iteration 100 / 5000: loss 27.875122\n",
      "iteration 200 / 5000: loss 26.016377\n",
      "iteration 300 / 5000: loss 40.112347\n",
      "iteration 400 / 5000: loss 16.528817\n",
      "iteration 500 / 5000: loss 38.529744\n",
      "iteration 600 / 5000: loss 16.573834\n",
      "iteration 700 / 5000: loss 32.867534\n",
      "iteration 800 / 5000: loss 24.751549\n",
      "iteration 900 / 5000: loss 24.663210\n",
      "iteration 1000 / 5000: loss 29.460368\n",
      "iteration 1100 / 5000: loss 28.635932\n",
      "iteration 1200 / 5000: loss 29.610281\n",
      "iteration 1300 / 5000: loss 28.501679\n",
      "iteration 1400 / 5000: loss 37.227304\n",
      "iteration 1500 / 5000: loss 34.555218\n",
      "iteration 1600 / 5000: loss 31.364621\n",
      "iteration 1700 / 5000: loss 24.439802\n",
      "iteration 1800 / 5000: loss 26.738563\n",
      "iteration 1900 / 5000: loss 24.826606\n",
      "iteration 2000 / 5000: loss 30.014346\n",
      "iteration 2100 / 5000: loss 22.670997\n",
      "iteration 2200 / 5000: loss 26.609541\n",
      "iteration 2300 / 5000: loss 32.614080\n",
      "iteration 2400 / 5000: loss 27.203788\n",
      "iteration 2500 / 5000: loss 19.833263\n",
      "iteration 2600 / 5000: loss 26.186919\n",
      "iteration 2700 / 5000: loss 26.346170\n",
      "iteration 2800 / 5000: loss 21.137074\n",
      "iteration 2900 / 5000: loss 33.426476\n",
      "iteration 3000 / 5000: loss 24.471018\n",
      "iteration 3100 / 5000: loss 32.756850\n",
      "iteration 3200 / 5000: loss 23.160472\n",
      "iteration 3300 / 5000: loss 24.823790\n",
      "iteration 3400 / 5000: loss 25.299752\n",
      "iteration 3500 / 5000: loss 38.019665\n",
      "iteration 3600 / 5000: loss 19.531714\n",
      "iteration 3700 / 5000: loss 30.993839\n",
      "iteration 3800 / 5000: loss 21.172827\n",
      "iteration 3900 / 5000: loss 33.262878\n",
      "iteration 4000 / 5000: loss 23.774112\n",
      "iteration 4100 / 5000: loss 29.538230\n",
      "iteration 4200 / 5000: loss 37.815680\n",
      "iteration 4300 / 5000: loss 20.272606\n",
      "iteration 4400 / 5000: loss 25.880665\n",
      "iteration 4500 / 5000: loss 34.807315\n",
      "iteration 4600 / 5000: loss 30.184555\n",
      "iteration 4700 / 5000: loss 43.091793\n",
      "iteration 4800 / 5000: loss 30.326487\n",
      "iteration 4900 / 5000: loss 38.365278\n",
      "training accuracy: 0.188408\n",
      "validation accuracy: 0.190000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 157.624606\n",
      "iteration 100 / 5000: loss 35.695735\n",
      "iteration 200 / 5000: loss 47.969449\n",
      "iteration 300 / 5000: loss 36.858932\n",
      "iteration 400 / 5000: loss 40.634320\n",
      "iteration 500 / 5000: loss 29.349679\n",
      "iteration 600 / 5000: loss 41.171095\n",
      "iteration 700 / 5000: loss 50.471954\n",
      "iteration 800 / 5000: loss 58.218806\n",
      "iteration 900 / 5000: loss 36.832565\n",
      "iteration 1000 / 5000: loss 39.415591\n",
      "iteration 1100 / 5000: loss 53.577243\n",
      "iteration 1200 / 5000: loss 48.117978\n",
      "iteration 1300 / 5000: loss 42.793001\n",
      "iteration 1400 / 5000: loss 55.562371\n",
      "iteration 1500 / 5000: loss 27.232913\n",
      "iteration 1600 / 5000: loss 44.337071\n",
      "iteration 1700 / 5000: loss 49.443546\n",
      "iteration 1800 / 5000: loss 52.824884\n",
      "iteration 1900 / 5000: loss 30.769111\n",
      "iteration 2000 / 5000: loss 44.986351\n",
      "iteration 2100 / 5000: loss 34.136256\n",
      "iteration 2200 / 5000: loss 49.979105\n",
      "iteration 2300 / 5000: loss 33.778967\n",
      "iteration 2400 / 5000: loss 46.741311\n",
      "iteration 2500 / 5000: loss 47.818287\n",
      "iteration 2600 / 5000: loss 33.707093\n",
      "iteration 2700 / 5000: loss 49.231292\n",
      "iteration 2800 / 5000: loss 38.523721\n",
      "iteration 2900 / 5000: loss 38.531221\n",
      "iteration 3000 / 5000: loss 50.102889\n",
      "iteration 3100 / 5000: loss 35.901162\n",
      "iteration 3200 / 5000: loss 42.889614\n",
      "iteration 3300 / 5000: loss 45.108586\n",
      "iteration 3400 / 5000: loss 28.552167\n",
      "iteration 3500 / 5000: loss 37.191543\n",
      "iteration 3600 / 5000: loss 36.348739\n",
      "iteration 3700 / 5000: loss 36.429229\n",
      "iteration 3800 / 5000: loss 19.570752\n",
      "iteration 3900 / 5000: loss 35.414441\n",
      "iteration 4000 / 5000: loss 36.687192\n",
      "iteration 4100 / 5000: loss 48.181706\n",
      "iteration 4200 / 5000: loss 45.021860\n",
      "iteration 4300 / 5000: loss 41.949250\n",
      "iteration 4400 / 5000: loss 28.527133\n",
      "iteration 4500 / 5000: loss 29.123345\n",
      "iteration 4600 / 5000: loss 39.589691\n",
      "iteration 4700 / 5000: loss 38.870877\n",
      "iteration 4800 / 5000: loss 36.757759\n",
      "iteration 4900 / 5000: loss 35.602761\n",
      "training accuracy: 0.166673\n",
      "validation accuracy: 0.187000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 5.342921\n",
      "iteration 100 / 5000: loss 25.024824\n",
      "iteration 200 / 5000: loss 43.810984\n",
      "iteration 300 / 5000: loss 31.206195\n",
      "iteration 400 / 5000: loss 38.242747\n",
      "iteration 500 / 5000: loss 36.855872\n",
      "iteration 600 / 5000: loss 31.820481\n",
      "iteration 700 / 5000: loss 23.540932\n",
      "iteration 800 / 5000: loss 30.184634\n",
      "iteration 900 / 5000: loss 52.163277\n",
      "iteration 1000 / 5000: loss 32.667725\n",
      "iteration 1100 / 5000: loss 31.931951\n",
      "iteration 1200 / 5000: loss 25.993933\n",
      "iteration 1300 / 5000: loss 27.286206\n",
      "iteration 1400 / 5000: loss 24.159350\n",
      "iteration 1500 / 5000: loss 24.142279\n",
      "iteration 1600 / 5000: loss 18.427319\n",
      "iteration 1700 / 5000: loss 26.813449\n",
      "iteration 1800 / 5000: loss 23.066559\n",
      "iteration 1900 / 5000: loss 28.975908\n",
      "iteration 2000 / 5000: loss 25.528694\n",
      "iteration 2100 / 5000: loss 20.161748\n",
      "iteration 2200 / 5000: loss 18.693868\n",
      "iteration 2300 / 5000: loss 41.715663\n",
      "iteration 2400 / 5000: loss 26.232293\n",
      "iteration 2500 / 5000: loss 25.880766\n",
      "iteration 2600 / 5000: loss 33.216692\n",
      "iteration 2700 / 5000: loss 13.271358\n",
      "iteration 2800 / 5000: loss 28.342761\n",
      "iteration 2900 / 5000: loss 24.268585\n",
      "iteration 3000 / 5000: loss 20.128265\n",
      "iteration 3100 / 5000: loss 41.340643\n",
      "iteration 3200 / 5000: loss 15.068074\n",
      "iteration 3300 / 5000: loss 29.481748\n",
      "iteration 3400 / 5000: loss 18.676673\n",
      "iteration 3500 / 5000: loss 21.684248\n",
      "iteration 3600 / 5000: loss 22.634418\n",
      "iteration 3700 / 5000: loss 24.888283\n",
      "iteration 3800 / 5000: loss 17.628076\n",
      "iteration 3900 / 5000: loss 26.627277\n",
      "iteration 4000 / 5000: loss 18.328818\n",
      "iteration 4100 / 5000: loss 28.069923\n",
      "iteration 4200 / 5000: loss 24.597690\n",
      "iteration 4300 / 5000: loss 23.421371\n",
      "iteration 4400 / 5000: loss 24.519676\n",
      "iteration 4500 / 5000: loss 29.547978\n",
      "iteration 4600 / 5000: loss 36.992745\n",
      "iteration 4700 / 5000: loss 27.345712\n",
      "iteration 4800 / 5000: loss 22.877518\n",
      "iteration 4900 / 5000: loss 32.289300\n",
      "training accuracy: 0.334898\n",
      "validation accuracy: 0.316000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 7.200836\n",
      "iteration 100 / 5000: loss 29.848225\n",
      "iteration 200 / 5000: loss 31.506953\n",
      "iteration 300 / 5000: loss 32.008640\n",
      "iteration 400 / 5000: loss 35.563165\n",
      "iteration 500 / 5000: loss 24.219042\n",
      "iteration 600 / 5000: loss 32.633968\n",
      "iteration 700 / 5000: loss 33.222193\n",
      "iteration 800 / 5000: loss 28.940978\n",
      "iteration 900 / 5000: loss 25.643960\n",
      "iteration 1000 / 5000: loss 45.697442\n",
      "iteration 1100 / 5000: loss 39.687434\n",
      "iteration 1200 / 5000: loss 30.698792\n",
      "iteration 1300 / 5000: loss 40.690768\n",
      "iteration 1400 / 5000: loss 25.690670\n",
      "iteration 1500 / 5000: loss 42.906902\n",
      "iteration 1600 / 5000: loss 48.897528\n",
      "iteration 1700 / 5000: loss 34.119967\n",
      "iteration 1800 / 5000: loss 22.510380\n",
      "iteration 1900 / 5000: loss 34.353085\n",
      "iteration 2000 / 5000: loss 25.033902\n",
      "iteration 2100 / 5000: loss 18.947293\n",
      "iteration 2200 / 5000: loss 44.666929\n",
      "iteration 2300 / 5000: loss 30.438703\n",
      "iteration 2400 / 5000: loss 30.348547\n",
      "iteration 2500 / 5000: loss 42.135721\n",
      "iteration 2600 / 5000: loss 46.340841\n",
      "iteration 2700 / 5000: loss 34.960293\n",
      "iteration 2800 / 5000: loss 21.268984\n",
      "iteration 2900 / 5000: loss 41.339300\n",
      "iteration 3000 / 5000: loss 46.275693\n",
      "iteration 3100 / 5000: loss 25.540545\n",
      "iteration 3200 / 5000: loss 40.640303\n",
      "iteration 3300 / 5000: loss 30.499660\n",
      "iteration 3400 / 5000: loss 37.170893\n",
      "iteration 3500 / 5000: loss 32.036640\n",
      "iteration 3600 / 5000: loss 36.953553\n",
      "iteration 3700 / 5000: loss 32.258035\n",
      "iteration 3800 / 5000: loss 33.390581\n",
      "iteration 3900 / 5000: loss 29.408100\n",
      "iteration 4000 / 5000: loss 37.911170\n",
      "iteration 4100 / 5000: loss 48.242571\n",
      "iteration 4200 / 5000: loss 44.229476\n",
      "iteration 4300 / 5000: loss 24.679131\n",
      "iteration 4400 / 5000: loss 44.502322\n",
      "iteration 4500 / 5000: loss 45.556088\n",
      "iteration 4600 / 5000: loss 29.542359\n",
      "iteration 4700 / 5000: loss 42.730228\n",
      "iteration 4800 / 5000: loss 54.834280\n",
      "iteration 4900 / 5000: loss 22.379068\n",
      "training accuracy: 0.226898\n",
      "validation accuracy: 0.222000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 21.901934\n",
      "iteration 100 / 5000: loss 28.530610\n",
      "iteration 200 / 5000: loss 43.035924\n",
      "iteration 300 / 5000: loss 34.640782\n",
      "iteration 400 / 5000: loss 40.740562\n",
      "iteration 500 / 5000: loss 68.904388\n",
      "iteration 600 / 5000: loss 50.990218\n",
      "iteration 700 / 5000: loss 33.854437\n",
      "iteration 800 / 5000: loss 42.779289\n",
      "iteration 900 / 5000: loss 46.011780\n",
      "iteration 1000 / 5000: loss 28.610288\n",
      "iteration 1100 / 5000: loss 54.618346\n",
      "iteration 1200 / 5000: loss 44.294912\n",
      "iteration 1300 / 5000: loss 53.847029\n",
      "iteration 1400 / 5000: loss 46.209496\n",
      "iteration 1500 / 5000: loss 54.953933\n",
      "iteration 1600 / 5000: loss 40.276314\n",
      "iteration 1700 / 5000: loss 46.974764\n",
      "iteration 1800 / 5000: loss 37.807404\n",
      "iteration 1900 / 5000: loss 50.086457\n",
      "iteration 2000 / 5000: loss 46.927955\n",
      "iteration 2100 / 5000: loss 55.074848\n",
      "iteration 2200 / 5000: loss 44.977226\n",
      "iteration 2300 / 5000: loss 62.497770\n",
      "iteration 2400 / 5000: loss 36.953234\n",
      "iteration 2500 / 5000: loss 30.783163\n",
      "iteration 2600 / 5000: loss 34.464225\n",
      "iteration 2700 / 5000: loss 35.165390\n",
      "iteration 2800 / 5000: loss 33.821681\n",
      "iteration 2900 / 5000: loss 43.451838\n",
      "iteration 3000 / 5000: loss 37.342800\n",
      "iteration 3100 / 5000: loss 38.559363\n",
      "iteration 3200 / 5000: loss 43.637863\n",
      "iteration 3300 / 5000: loss 43.303470\n",
      "iteration 3400 / 5000: loss 36.021507\n",
      "iteration 3500 / 5000: loss 47.358992\n",
      "iteration 3600 / 5000: loss 51.599780\n",
      "iteration 3700 / 5000: loss 44.670716\n",
      "iteration 3800 / 5000: loss 54.407141\n",
      "iteration 3900 / 5000: loss 42.670633\n",
      "iteration 4000 / 5000: loss 51.933420\n",
      "iteration 4100 / 5000: loss 54.454133\n",
      "iteration 4200 / 5000: loss 48.158703\n",
      "iteration 4300 / 5000: loss 50.354950\n",
      "iteration 4400 / 5000: loss 64.697856\n",
      "iteration 4500 / 5000: loss 33.897935\n",
      "iteration 4600 / 5000: loss 48.867758\n",
      "iteration 4700 / 5000: loss 18.888043\n",
      "iteration 4800 / 5000: loss 40.724885\n",
      "iteration 4900 / 5000: loss 39.358952\n",
      "training accuracy: 0.232510\n",
      "validation accuracy: 0.221000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 82.885507\n",
      "iteration 100 / 5000: loss 73.865884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preri\\Desktop\\MLGeneral\\CS230n\\winter1516_assignment1\\assignment1\\cs231n\\classifiers\\softmax.py:132: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.sum(np.log(softmax_corresponding_to_correct_labels))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 5000: loss 61.638007\n",
      "iteration 300 / 5000: loss 93.469035\n",
      "iteration 400 / 5000: loss 103.000966\n",
      "iteration 500 / 5000: loss 67.800033\n",
      "iteration 600 / 5000: loss 78.317675\n",
      "iteration 700 / 5000: loss 91.933839\n",
      "iteration 800 / 5000: loss 85.887682\n",
      "iteration 900 / 5000: loss 72.833665\n",
      "iteration 1000 / 5000: loss 66.958695\n",
      "iteration 1100 / 5000: loss 85.587513\n",
      "iteration 1200 / 5000: loss 78.813268\n",
      "iteration 1300 / 5000: loss 65.195506\n",
      "iteration 1400 / 5000: loss 83.184303\n",
      "iteration 1500 / 5000: loss 64.524265\n",
      "iteration 1600 / 5000: loss 83.186477\n",
      "iteration 1700 / 5000: loss 89.606381\n",
      "iteration 1800 / 5000: loss 71.234141\n",
      "iteration 1900 / 5000: loss 80.889507\n",
      "iteration 2000 / 5000: loss 86.934947\n",
      "iteration 2100 / 5000: loss 85.898993\n",
      "iteration 2200 / 5000: loss 69.899930\n",
      "iteration 2300 / 5000: loss 104.319510\n",
      "iteration 2400 / 5000: loss 77.573838\n",
      "iteration 2500 / 5000: loss 98.783204\n",
      "iteration 2600 / 5000: loss 107.804797\n",
      "iteration 2700 / 5000: loss 67.417466\n",
      "iteration 2800 / 5000: loss 87.130136\n",
      "iteration 2900 / 5000: loss 93.008604\n",
      "iteration 3000 / 5000: loss 87.932444\n",
      "iteration 3100 / 5000: loss 90.327098\n",
      "iteration 3200 / 5000: loss 70.745669\n",
      "iteration 3300 / 5000: loss 70.589639\n",
      "iteration 3400 / 5000: loss 47.324768\n",
      "iteration 3500 / 5000: loss 87.277320\n",
      "iteration 3600 / 5000: loss 74.758581\n",
      "iteration 3700 / 5000: loss 85.520753\n",
      "iteration 3800 / 5000: loss 79.161957\n",
      "iteration 3900 / 5000: loss 90.970007\n",
      "iteration 4000 / 5000: loss 99.001981\n",
      "iteration 4100 / 5000: loss 95.570613\n",
      "iteration 4200 / 5000: loss 58.291395\n",
      "iteration 4300 / 5000: loss 95.571999\n",
      "iteration 4400 / 5000: loss 87.143941\n",
      "iteration 4500 / 5000: loss 89.034030\n",
      "iteration 4600 / 5000: loss 74.646594\n",
      "iteration 4700 / 5000: loss 73.701408\n",
      "iteration 4800 / 5000: loss 105.756680\n",
      "iteration 4900 / 5000: loss 60.319660\n",
      "training accuracy: 0.129469\n",
      "validation accuracy: 0.141000\n",
      "==================================================\n",
      "iteration 0 / 5000: loss 161.743818\n",
      "iteration 100 / 5000: loss 145.341544\n",
      "iteration 200 / 5000: loss 142.199769\n",
      "iteration 300 / 5000: loss inf\n",
      "iteration 400 / 5000: loss 198.258401\n",
      "iteration 500 / 5000: loss 174.159735\n",
      "iteration 600 / 5000: loss 164.545594\n",
      "iteration 700 / 5000: loss 150.251511\n",
      "iteration 800 / 5000: loss 171.238419\n",
      "iteration 900 / 5000: loss 178.311669\n",
      "iteration 1000 / 5000: loss 176.163032\n",
      "iteration 1100 / 5000: loss inf\n",
      "iteration 1200 / 5000: loss 175.558931\n",
      "iteration 1300 / 5000: loss 175.770396\n",
      "iteration 1400 / 5000: loss inf\n",
      "iteration 1500 / 5000: loss inf\n",
      "iteration 1600 / 5000: loss 151.847463\n",
      "iteration 1700 / 5000: loss inf\n",
      "iteration 1800 / 5000: loss 163.053909\n",
      "iteration 1900 / 5000: loss 177.670150\n",
      "iteration 2000 / 5000: loss inf\n",
      "iteration 2100 / 5000: loss 174.509889\n",
      "iteration 2200 / 5000: loss 173.487545\n",
      "iteration 2300 / 5000: loss 171.874860\n",
      "iteration 2400 / 5000: loss 158.777362\n",
      "iteration 2500 / 5000: loss 169.090219\n",
      "iteration 2600 / 5000: loss inf\n",
      "iteration 2700 / 5000: loss 153.416696\n",
      "iteration 2800 / 5000: loss 174.856184\n",
      "iteration 2900 / 5000: loss 163.092528\n",
      "iteration 3000 / 5000: loss 192.657396\n",
      "iteration 3100 / 5000: loss 172.518285\n",
      "iteration 3200 / 5000: loss 157.275728\n",
      "iteration 3300 / 5000: loss inf\n",
      "iteration 3400 / 5000: loss inf\n",
      "iteration 3500 / 5000: loss inf\n",
      "iteration 3600 / 5000: loss 172.868888\n",
      "iteration 3700 / 5000: loss inf\n",
      "iteration 3800 / 5000: loss 167.591611\n",
      "iteration 3900 / 5000: loss 173.704510\n",
      "iteration 4000 / 5000: loss inf\n",
      "iteration 4100 / 5000: loss 159.617468\n",
      "iteration 4200 / 5000: loss 173.525405\n",
      "iteration 4300 / 5000: loss inf\n",
      "iteration 4400 / 5000: loss 158.485417\n",
      "iteration 4500 / 5000: loss 176.008074\n",
      "iteration 4600 / 5000: loss 168.031746\n",
      "iteration 4700 / 5000: loss inf\n",
      "iteration 4800 / 5000: loss 185.895640\n",
      "iteration 4900 / 5000: loss 154.325964\n",
      "training accuracy: 0.110265\n",
      "validation accuracy: 0.127000\n",
      "==================================================\n",
      "lr 1.000000e-09 reg 1.000000e+00 train accuracy: 0.126347 val accuracy: 0.127000\n",
      "lr 1.000000e-09 reg 1.000000e+02 train accuracy: 0.123714 val accuracy: 0.114000\n",
      "lr 1.000000e-09 reg 1.000000e+03 train accuracy: 0.103816 val accuracy: 0.108000\n",
      "lr 1.000000e-09 reg 5.000000e+03 train accuracy: 0.089816 val accuracy: 0.105000\n",
      "lr 1.000000e-09 reg 1.000000e+04 train accuracy: 0.129408 val accuracy: 0.132000\n",
      "lr 1.000000e-07 reg 1.000000e+00 train accuracy: 0.303571 val accuracy: 0.294000\n",
      "lr 1.000000e-07 reg 1.000000e+02 train accuracy: 0.303041 val accuracy: 0.303000\n",
      "lr 1.000000e-07 reg 1.000000e+03 train accuracy: 0.335102 val accuracy: 0.326000\n",
      "lr 1.000000e-07 reg 5.000000e+03 train accuracy: 0.387265 val accuracy: 0.386000\n",
      "lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.376082 val accuracy: 0.389000\n",
      "lr 5.000000e-05 reg 1.000000e+00 train accuracy: 0.318082 val accuracy: 0.289000\n",
      "lr 5.000000e-05 reg 1.000000e+02 train accuracy: 0.244510 val accuracy: 0.244000\n",
      "lr 5.000000e-05 reg 1.000000e+03 train accuracy: 0.186857 val accuracy: 0.195000\n",
      "lr 5.000000e-05 reg 5.000000e+03 train accuracy: 0.188408 val accuracy: 0.190000\n",
      "lr 5.000000e-05 reg 1.000000e+04 train accuracy: 0.166673 val accuracy: 0.187000\n",
      "lr 1.000000e-04 reg 1.000000e+00 train accuracy: 0.334898 val accuracy: 0.316000\n",
      "lr 1.000000e-04 reg 1.000000e+02 train accuracy: 0.226898 val accuracy: 0.222000\n",
      "lr 1.000000e-04 reg 1.000000e+03 train accuracy: 0.232510 val accuracy: 0.221000\n",
      "lr 1.000000e-04 reg 5.000000e+03 train accuracy: 0.129469 val accuracy: 0.141000\n",
      "lr 1.000000e-04 reg 1.000000e+04 train accuracy: 0.110265 val accuracy: 0.127000\n",
      "best validation accuracy achieved during cross-validation: 0.389000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of about 0.35 on the validation set.\n",
    "# learning_rates = [1e-7, 5e-5]\n",
    "# regularization_strengths = [5e4, 1e5]\n",
    "\n",
    "# learning_rates = [10 ** -x for x in np.arange(1,8,1)]\n",
    "# regularization_strengths = [10 ** x for x in np.arange(1,4,1)]\n",
    "from cs231n.classifiers import Softmax\n",
    "learning_rates = [1e-9, 1e-7, 5e-5, 1e-4]\n",
    "regularization_strengths = [1, 1e2 ,1e3, 5e3, 1e4]\n",
    "\n",
    "# results is dictionary mapping tuples of the form\n",
    "# (learning_rate, regularization_strength) to tuples of the form\n",
    "# (training_accuracy, validation_accuracy). The accuracy is simply the fraction\n",
    "# of data points that are correctly classified.\n",
    "results = {}\n",
    "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
    "best_svm = None # The LinearSVM object that achieved the highest validation rate.\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "# set. For each combination of hyperparameters, train a linear SVM on the      #\n",
    "# training set, compute its accuracy on the training and validation sets, and  #\n",
    "# store these numbers in the results dictionary. In addition, store the best   #\n",
    "# validation accuracy in best_val and the LinearSVM object that achieves this  #\n",
    "# accuracy in best_svm.                                                        #\n",
    "#                                                                              #\n",
    "# Hint: You should use a small value for num_iters as you develop your         #\n",
    "# validation code so that the SVMs don't take much time to train; once you are #\n",
    "# confident that your validation code works, you should rerun the validation   #\n",
    "# code with a larger value for num_iters.                                      #\n",
    "################################################################################\n",
    "#pass\n",
    "num_iters =5000\n",
    "for lr in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "        svm = Softmax()\n",
    "        loss_hist = svm.train(X_train, y_train, learning_rate=lr, reg=reg,\n",
    "                      num_iters = num_iters, verbose=True)\n",
    "        y_train_pred = svm.predict(X_train)\n",
    "        print ('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "        y_train_accuracy = np.mean(y_train == y_train_pred)\n",
    "        y_val_pred = svm.predict(X_val)\n",
    "        print ('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))\n",
    "        y_val_accuracy = np.mean(y_val == y_val_pred)\n",
    "        results[(lr,reg)] = (y_train_accuracy, y_val_accuracy)\n",
    "        if (y_val_accuracy > best_val):\n",
    "            best_val = y_val_accuracy\n",
    "            best_svm = svm\n",
    "        print(\"=\"*50)\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print ('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print ('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.381000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "best_softmax = best_svm\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print ('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF8CAYAAADrUz6WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsvXnMbWl23rXePQ9nHr7vu2NVV1V3O2k7SkcKEUImlogI\nJESBIIGCIRgUhCUgAsRklD+MMDKKsBSiSIBMCCgxEOFECINACIGEISBEYgd32j1UV935m8487Xlv\n/rjX53lOpV1d1+fUrW7X+kkl7TrfPnu/83nvet61lmmaRhRFURRFUZTfGtZnXQBFURRFUZQfZHQz\npSiKoiiKcgS6mVIURVEURTkC3UwpiqIoiqIcgW6mFEVRFEVRjkA3U4qiKIqiKEegmykRMcb8mDHm\n2WddDkVRgDHmkTHmD3yXz3/UGPPN13zWf26M+ZnTlU5RFBGdW7+BbqYURfmBommaX26a5sufdTmU\nN8tvtrlWlO8HdDOlKL8Jxhjnsy6D8nponynKDz4/iPP4c7WZevUvm58yxnzdGDM3xvxFY0zwXe77\nt4wx3zHGrF/d+4/Q337CGPN/GGP+g1fP+NAY8w/S37vGmL9gjLk0xjw3xvyMMcZ+U3VUgDHmgTHm\nrxljbo0xU2PMnzfGvGuM+V9f/f/EGPMLxpgefeeRMebfNMb8fyKy/UGc1L/N+L0fna8fleW/W58Z\nY75qjPmbr+bwXxGRv2OeK58drzs3jTF/SUQeisgvGWM2xph/47OtweeXj5tbxph/yBjzq8aYhTHm\nrxtjfhf97a4x5q++6vMPjTF/iv7208aYXzTG/GVjzEpEfuKNVuoEfK42U6/4cRH5gyLyroh8SUT+\n9He55zsi8qMi0hWRf0dE/rIx5g79/feJyDdFZCQif0ZE/oIxxrz6238hIqWIvCciXxWRv19E/uTp\nq6F8HK82sP+9iDwWkbdF5J6I/NciYkTkZ0Xkroj8DhF5ICI//ZGv/3ER+cMi0muapnwzJVZ+Ez7J\nfBWhPpOX69p/KyJ/SUQGIvLfiMg/+qmXVPlE/FbmZtM0/5SIPBGRP9I0Tatpmj/zxguuiDHGk99k\nbhljfo+I/Gci8s+LyFBE/hMR+e+MMb4xxhKRXxKRvyUv+/vvE5F/2RjzB+nxf1REflFezuFfeCMV\nOiVN03xu/hORRyLyk/T/f0hebpx+TESefcz3flVE/uir658Qkffpb5GINCJyISLnIpKJSEh//+Mi\n8r991nX/vP0nIn+3iNyKiPM97vuHReRXPjJG/tnPuvz63yefrx/tMxH5e0XkhYgY+uyvi8jPfNZ1\n0v+Onpt/4LMu/+f5v4+bWyLyH4nIv/uR+78pIr9fXhognnzkbz8lIn/x1fVPi8j//lnX75j/Po8S\nxlO6fiwv/xV0gDHmT4jIvyov/9UkItKSl1ao3+DqNy6aptm9Mkq15OVO3RWRSxiqxPrIO5U3wwMR\nedx8xLJkjDkTkT8nLy2PbXnZP/OPfFf76/uH7zlfv8t9d0XkefNqlabvKt8fHDM3lc+Wj5tbb4nI\nP22M+Zfob96r71QictcYs6C/2SLyy/T/P9Dr7udR5ntA1w/l5S57jzHmLRH5eRH5F0Vk2DRNT0S+\nJi9N0N+Lp/LSMjVqmqb36r9O0zRfOU3RldfgqYg8/C5nnn5WXloSf1fTNB0R+Sfl7+zbRpTvFz52\nvhLcZ5cico+k99/4rvL9wW91buq8/Oz5uLn1VET+Pfrt6zVNEzVN81+9+tuHH/lbu2maP0TP+YHu\n38/jZupfMMbcN8YMROTfFpG/8pG/x/KyU29FRIwx/4yI/PAneXDTNJci8j+LyM8ZYzrGGOvVocrf\nf7riK5+Q/0deTvx/3xgTvzq4/PfIy3/xbkRkYYy5JyL/+mdZSOV78r3m63fj/5KX5xb/1KvD6H9M\nRP6uT7OQymvxW52b1yLyzpstqvIRPm5u/byI/KQx5veZl8TGmD9sjGnLyz5fvXIUCY0xtjHmh40x\nv/czqsfJ+Txupv5Lebnh+eDVfwfBxpqm+bqI/Jy8HDTXIvIjIvJ/vsbz/4S8NG1+XV6aqH9RRO58\n7DeUk9M0TSUif0ReOgI8EZFnIvKPy0uHgt8jIksR+R9E5K99VmVUPhEfO1+/G03T5CLyx+Tl+ca5\nvOx37efvE46Ymz8rIn/6lafYv/bmSqz8Bh83t5qm+X9F5J8TkT//6m/vv7qP+/x3i8iHIjIRkf9U\nXjp5/bbAHEqfv70xxjwSkT/ZNM3/8lmXRVEURVGU3x58Hi1TiqIoiqIoJ0M3U4qiKIqiKEfwuZL5\nFEVRFEVRTo1aphRFURRFUY7gjQbt/Cd+8hf2ZjC/7u8/z9rZ/tru4TqbIZTF2IMFbWdv99fhNt5f\nF2G6v85X3v668vCcrYfvtldI1xVQGXb0XdPg85V1uPe0DN59p7jdX6+rC3xnPN1fR3N8P2zjnsRZ\n7q/LHO/rFO39deEjdp3T6+C703x/7U3x/Hzc2l+PrWJ//ed+7h/7JPGyvif/4U/9+L5D3Brv3SZo\n07ykdGgO6tJ0Vvvrfr3bXycZhmOzgANk8aXJ/no8R728DlIeLncu7rdR384E7VkPaXzcYPyJiNQt\n9FNuoX2jGs0VNRgXTmuDstI4uvHX++tAUNbaQp2jDM9vdujXrX+2v05n1f46Ge5jxEqwQT1/6uf/\n45P0pYjIv/Jnf3Tfn2mOufbA8ffX8wz90NiIYVuk6H/HoHxZdrO/boXD/fWH6ZP99cPwvf31aoU+\nSHuoWitHP7dCtMtsjnbnsSAiYlEqvvUcYyw+C/fX9mq2v76t4VT0wMWY2VoYM5mHuWY9j/DuFsZ2\n7lzvr8sA68M4RRkkQPtmGcr5Z3/6l0/Snz/74z+2f0E+wbvKFOP3ooexeRVj/fFXaF/fxxxpl3jO\nwqr311aFeWAZmuN9tFt7gTG0a9BPTgcxO8s57ilprRAR8QVzp47Qjm6GMdgqaS0IMR67gj5zR3iO\nWeMdlYX6Lzz0h+NgPZo9RlnTNvp+XKBdLrd4zs/+j790srn5D1B/jnz028ZFOQJBv4Ux1j/fxnz0\nPPTbOkff7kqsWRdDtOmGhmwco+0WC7SRReuaTz7rbkbVT1FmEZG4woOfzLEu3OnhN3RVoz51hb6t\nwsH+elAi5Nwuwv3bWbK/ThJas6lMhn7LixzlCTfow0mN7/5Pf/X//p79qZYpRVEURVGUI9DNlKIo\niqIoyhG8UZmvMxzvrxcFpItOAfOjvcV1dwwzefkEVrZyABPtquztrxvn+f76bIjnLCcwdUY2zLVV\nC2a8Ise7ogamyysX5udBdpBKSpwK5tTZOd5RFdij2g1MnLGg3OMcptVnHt5R+WSuJsPiaAZzsgPr\nrhgPMlF8BvOrTX4FVef0cdGcGxSiGKO+XnW+v7ZiyDaSPdtf9jfos1mf+n4HCbZ+iAqckym446PN\npxvYlQf1ByhbARnt+gtoz/CKpNwW2l9EJKohB0QpTL3LEO9rWuiQuHlrf50M8dzWNdqlHKBfG5Ik\n7lgYd7/ehdS0ySAR2RHGzZTGR784mXpwQLZD/VsB5mlO/ZPcQpJrFZgLVoH2WrewpFgBpOzsGvc8\nvAuJYbZGn7sdfO6sYaq3Buhnd4XnnI0xros1ZCURkSbAHHZd9HVxib7avIU2Ht5CJljXmEdVgVRi\n+Rr3WwPIDbJGSrGwg89dNKk4AcrnxRj/eQKZ7FSENdqr28McLGr0q6nx+SBAfbc55m9eo92WHcxH\nj+IsntMxiKc7jImowT1ZhDEe0Oc3PtbcboV3NR2MdxGRTo4xtQ4RAD30kW4xoXV2WGKddajvVwXu\nCUpcdxtIv8YiSXKF+Xg/xvqe0rGJska/jkhqOyXDCOtZ7mE982OSr1PUYVehHC0L5Z48xhrUucD8\naN5C327mqJvjYO67W5L2Kvx2byv0TfoB2u7uAO9dZZjjIiIZyafBgCQ/H+XoP8H8L0iS3tCYFJLm\niy2twfR57tD9tHT6S5RpSz+WK4O52e/jWMMnQS1TiqIoiqIoR6CbKUVRFEVRlCN4ozJfdg2TXpCT\nOXEAs9z2Fvd0yQkgrWGWLAt4PTlyub825G21nqFqtgOTXlrADF8YPNOJYMasScIYRGTq9Mi0LyKT\nBt5KAZmQswm8Et4hTe7FCM91VzBLhm2Yiu1fp3K/i/tzC1JXQmpjq8R++HoN06rTJo+xGcltJ2JZ\nwnTvz/Cu7AKSR8QyxxCm9+0jSBuejXoVd9EHrTl5y92DXPR8AdP7pkLf1+do/2SFF9fPUc6MvNTC\n+vDfEbchzORdC/2XOG+jTOQwlpNJ36pJAiHvmSYlM3EAG/OvGoyjYE2m9xzlbjLIWYMJtcWCGvWE\nXDRv76/zFAOsSTD+RzbG5oa8pBqSy60V7mmn5Nk3pPkVYL4/XKPtrtuQCb4yQH883pB9PsRYsyvy\nIm0dzs0zkhhLG1L4JfVVSBOpFeLd7gLzet6HFOW3yFO1QP/fGd/D/QHKHdqY42IgvZUZxle7e/q0\nnWkb/TFKkaM9oaEzI+X/3EKfuSO09R377f31WiDT3y7R7tMG/WFlJK3yUYQ+5NvpLZ7vPad500e7\ndfzDn6VJm2QrksJFIFtKiTX+kmSbboV3d+d4TvMuxvX1AuvveIF7yjH6JskgbZkClXPHmKfu38Yz\nT0kV0Hpm03GEJTrRJe/V8BxtmZb4nQ0H6J9ihe+WH+L3wR6T7O5jHF1XkN0iWhMr+XU8J8datm7o\ndzPGc0REdiV+4y3yvDZTlGnRRRtX5N3Z8bGmlgnmeEBjZL27pfsxxzOSQt0L9Lm/w7vagnVgGuBY\nwydBLVOKoiiKoihHoJspRVEURVGUI3ijMl91RgHXyCxZFDhxXzsU0G4Fk6M3wOdVBNNqJjAzj0gO\n2NjQZNo2BfGKYFaMLZj3Nmvcv6WgmF2Dz2/sDw/qc68FCSDZ4ln9PpUjwj1xCdPi/F3IUq1L1DP/\nYZhx7SmZ0ClwWbGE6fLWxT078jZ7uKJAZB2U4VRMblHmzluQXa0tzK3Gh1l1k0FqKfsw55oM9W2v\nyAOE9vnNGm07IK+dkp5//RzmXzshjcGmgJoVyYgkEYqIxBZMyZvWXfwhhLl6YcNcTQ58EsYwDc98\nSHIrch8JSdrbCaTHJsbzywbjo1+jTXtL1G16jrF/Us5R1mZBc6FCP0dtfF5TwEwh77egJO83nwKe\nUtDdfIf6TAYP99dj8qLc7jBGOiT51QUkCc+mAK7Zofz54SOUe3BOASQdPHdHxwJ2AQXnNSjrvQ7q\ntimwXHZTSCZrWsvaNMY6JHVsSOKPbuk4wt1PQRoqyBPSheRhG8yLB+TlVQt5O2dYH1+coZ9616i7\nP8R6PVygLkufgnaSB1pooW3PaS0uaW00dM80PwzA2rumgKFd8sBew5uvd45+DclzlI+QSIh3O0uS\nbyMKQBvg+TZ5+Da3kPxCmrOTKdbZ9gPM2VMSNRQs1sOYt2/Iw/0C864dkKy6hRTW7aJd8jtoF2eH\nOTUkL8ckJc/cAeZQOcd17WCtDNsoQ5ajbLbB/BU57PdhibHXFVprzjFWd+TBbTaYX6v3MA57tJy3\n6EhJQ0E7FyOsu8kUXqt1D++6T+uAIW/9T4JaphRFURRFUY5AN1OKoiiKoihH8EZlPlmTHBLA7H1n\ngut1CLPpcg1Toe+ylxRMlF3KkTZ3yURJ5m2X8gDWZMa+SWGu9ckj6QGZvRvKm5dlCNQoIjJZk2xZ\nkXdARIEiKUhoScEaKb2gWAZeKZ1bmFxNhvujc3jJTbcwOVuUP2hAXn4BSQ+uf3opYeOjjcZT9MGW\nZMd0DvO83UWb5AVMqW6Mz18s8ZzxEGbYakGeRCkFPIxxfynwqLoUtFVAskt8h/I7Tu8f1OexhXKc\nUVC5MXlRVlTPnKTK2w0F2/Rwf5bh/tYlTNIDF2bumcGYjbro+zXltVuTF2l2+3qB5D4ptzP0ySCm\nAH07vHtN3nPsXTtekexBbVeMYN5fNHjOmGQ+Q3UuaV7vaDD7G8q7FWHs+yQpFxQ8UkTkTg9ltWrI\nO9s12r4VYBKuKRdi3oJcczmHdHGP1henxjiXFuULDMl7iDxtJcHYiyhvZuf2MEDlKejMMRe273Lg\nRYzxdYaymRDjtAqxnvRcWn89SCHRAm2yjki+Xj/CPfNv7q+TFOXJOihP3sP6OXqO9XpgPTiozyrH\nWIvpKMeWAmmun6MPvBHW+47B+MoWGCNNQNLeM5J2HqCN7B1Jm2fo7ypC3cLrL+yvSzqickpMC2W1\nAvrJJok46UNu8wr8Fg0iXEdtrKn+d9Auqwt8PieZM8swb0a0Zj8NMBaac4yFeIW1Lx5iHtSH8ZFl\n5KF/0gDrnEPHBWY53l1TrtS4i3HbtjF3bsgrMCSP3VFMwaUTjJddQUeIKKjoTQ9tEc7el9dBLVOK\noiiKoihHoJspRVEURVGUI3ijMl/ow+To2CTnUa69PpvuY5gDkwDmROPjui5h3oxIIoymlOcrgFTX\n82AyDNiM24YpdWdDttmuyXQ5OPRKqAOYWZuM8kElFASPbJzZGXn8LWBmfdKGt0ZsQca6uIXZ83aF\nz4OYPK8qlGEbw1uhSlGf0D99zqjYR1tXHbTpnRv2nsA9HnltZAWuFwmZ1ck8e51grMQxyj8n+ctL\n4amyzUiyjSGjLGo8f4evyi48HPq+hfflS5KqXEiGlQcZqqpe7K9bJPFmLspRUWDQNIYZ2irh2Vhd\nw8ScUbDRJKF5IGiXYnb6PIsiIn0P5fZIhtySTOA2FGSRcmqZFHPNDkg+ovx6zQr1WQQYL84EZnW7\nhpQQJBjjffKq2k7xzBnldgvjQ2++ho4FxIZkoxbKtyAv2tCDLBFYeMdyjXl31YesEHWwBomFuWZ7\nlMuxwbuimDzSanrOR+TJU2D1UJeEAjtaLQowSfnvdjPM2bBEeSySc9YXFKT2knJO2pBsY4M+SC3y\n8oqxBiaUi9AjCf6JA7nQbJ4c1GfqoT8GAnneUHDlDkmnQw/vS3cYa/U7uL/3DNJh5xzjtxG0RVni\nOQF5gW+WKKtDddg2n05A3XqNcRTQ+NrSejls0K4FjbVqgb5a25jLkzZ5cBYY426J/LbrFuWDnWGM\ndyOMi3iKe1LKG5gbClBdHf7+PN5h7vTJ62+2Rduf38W64CzRV8bF2tzysAbZPfRJRcd3rmy0XUl5\nGsMlypREmCNt8oivPnIU5HuhlilFURRFUZQj0M2UoiiKoijKEbxRmc8hSefSJwmA8gdlPZgc36kh\nhzyB2iajBeQ2x4dc5uYw+93a8JArKniTxA1M+733YNJbbmACr3yUc9Qmr6DlobfGYkWBQcm7aU3B\nQBOSRuI55MPlHHVuk2TG0uauTcE8V3hOEMG07ja4PyDzc9WH2Xc2Q5ueis4W72oMTN3JA5i67wR4\n74ICHsZC0tEaJtl6CJP0vIK5uZyhv3c5vLm2F2iTzXfQFym91wrI5L/F55k5dDFJK3hwNQ7K93iD\nz+MzMpPPyDuP8opV5C1mRcjnNdvBrN6zKajoexgH1VM80/Eo71yJdum+TQkrT8icPErHDWSW2PvS\n/nrZYL5k5J2XuJCGaprLC5Lywy76qpyj/k6EOTELSSrfoL3yC3wez9DPlmDud5tDk/ycvIkuaV2w\n4TQms/EP7a/bN/QHj7wwWyRXUPmmCfrhwkU5mhkF6Y0xf1fktHcRUd1ycsE9Eb4HucQJKCgiBU/c\nOOizPkmz1g5yziQjWWSLeUqp78SiXJFhjvbJbbTJ7DHaZOdi3d+VlHNyTGOLJCgRkVaGPi8nGJvt\ngIMFo87TPj5/5w7Kd5thHLQ8tMW6xHVOP4neGZVbcI9lk6SWUh5P6/SemSIi7hiyWLKGJFWQk/aG\n6uBQAFurh3bp55C2btvoxF6EcTovyDs8wzGKZwUCpHbJS3lJee3GX8RYntE8WBSHAZLfDck7lzxk\nXQocvKTfxI6HsZqXGGNWiva2KP1sTjGXgw32ENYO7bKmOeJSjNi6Rnmai9eT4NUypSiKoiiKcgS6\nmVIURVEURTmCNyrzlRGksGFF+eIKmPfqBeS2mx7kI/9DFLWhYG9OCf1vxya6ISSZVkreDV2YIjdT\nmINXHkyPzgTXfgxTX+Ic5kWzfQpwhipIO0NZK/IO2CYwM5ZjmEptg+d0yOOiKWHStUsKXEeBxW6f\nb+l+lG9CHhQ/NMC7TkXYxTNDgXzmNeiz5xPymBmSh10f/ed38d2iwHfXc5hzK4Ep2W5RwM9neOZu\nAPkgN9SXFEjufYu8SFaHcplH30ktSFge5VcrqBnbEQK4fp0isIYVyZnfwBi3Y0izZQdSSqeGB2Z9\nhvp7K5T1DnlsrhPU85R0KrSr78P77bbE2DQ2xle4wL/Dln2M63pGHrIV2tG1IEMYmoMpjZdeRZJ4\niHnTydA3a/K28oeU3zM9zKMV36Ltew0F1dyir+7m5LUbwtZvcoy37ohkS0Hw2EFDkhkFdpUe2mh9\njvKN52iXyYR0MnN6aShpod37LuZaSt6lZkOBYx3yLiVZvLOFJ1zRJg9iH/0dOCh/Qh6hJXl5WXfJ\ns5L0zlVM3o4k69rtw75cGspZOUI/LbeYw8v2B/trJyA5a4MjHouAAuQOyMuXgrf6Of2G5JizYYrJ\nn7bRdjV5co5DrHenxCIJzwzRJ1mNMkU23n3hYE2t6ISHvUHbhwnW19Qir70W5Ti8JS+6OxjLFQVs\nlhXGdV6g7eIafdiPDiX4XQ/f98jzztqRdyJ57aY99JW/xtpxQ7n2BpQvsZ5hXJ01mMu3Lcr9l2LO\nehnmbELef6n/ekFY1TKlKIqiKIpyBLqZUhRFURRFOYI36823gEl0tYGJ0n1AARPJycpa43/6FEiw\npHxuOzqtv3Qhf3WXeGbiU3C3BaS9LeX163Nwsx7MgbMIssKEPARFRM4on1dRwCwZGkg0E8oldaem\nIJ82mcoz8j6jHFBRhro5lNNqvYB50w8gz+xyePZ9mYIt7ub4/FSsKUeeQ2Z5z6J+DdEfGwv1vdii\nLzdrtO+HlOfJWcM8u/NgYt5RoL4N5UBcbciTsSQPKcoVtwhhLm/gnCIiIpMBzNtf8in/nUu5/VYo\n04scEUC7FLQy7MBkvHtAwfamMKtnc3j5BUt8t9cjM7mDdkldCpi4OgwceyrKIco6K0leT2Hqr0kK\nnZOE1XtBdaN8Yf0lSYEkU7cN5uC6Q7koZxhHRmCG31Jeu22OOWGeYs7euoeBHl3y9LkYoI1z8hKq\nW7in5gCblPNrleNzj3JrhpS/0KYxWZFn43ZNc9nDOmAKtGNmkyvRiehQXRLyfrLJq7GMUZftDOPa\noyCcHnmCtUju3gnmyobu90macdsUmHSNuXIlWEOtPh1poDUzTw7XqxGtfU0LElGSInBuj+RiO8Ec\nuaKjH+0Ea9YTCpw7phyS6Q5t1zgoRzyisU+e2CV5pkmLvHRPyKqPIwUVBScWCoyZFVj/liXG1GSH\n8vUL9H+/j3FRu3R+gfKMBl3cM7NJ5qMxG3hY192KPKddtKMTHkrZZ5TnL4nIm69Cuy59tOXAx5r6\nnIIrdwX9U25ozNBxjqlL+4ASbeTTUaF8jrrVDuV1XZJ8/wlQy5SiKIqiKMoR6GZKURRFURTlCN6o\nzLc+Iy8un3Kp3cK0WFE+qH4DM7OdwVy3ogBdBUkSd1rf2F/XPoI7VuSdNadgdbWBybgeYV+ZkQvE\nagoT6KgD+U9EZNOF6T4gCeDmBubuogVpM4vxPiehHFATkjlDBKt08jO6H8Hh+gLT7SU9x1BerWxL\n3lnnp98zn/sU3I7MvrFPubBiynFGnkEp5V3bkUk2/ia1yQD9urqm/IPvwCS/ep88b1pon9RCeaol\n2rDYUI6w+FAuKykg642DPqttkq0M6mNTcNW6wedmSybtOczw2zOYsL+QQCY5H6Bv3ArPCbpor9kU\n465fHwbAOxnX6M/gDO20o3Ftk0eXZSiQokderlN4T21Jpq/PIQdUlMuws0NgwEmHZP2UAtxSfsyB\nQ8FM1/DgSp1Dk/wgoYCZFDg336F/fPKi9fp4dxOg/xPKi9aL8d1HNvrH32Hs+T2SdpcowwuSPUaU\nO5KDPp6KmjybvC36xqvJi2pCUgvdH5NHldCa6D5C37dHLK+jDZcR5ZCsqP27FOCVZMEBSfO2g+cP\nh+QaLSLbOcnCKdaL7jn6/GaH+dUjL+vBFvJy3qEArC1aOzyU+7KN36getVee4ToIUH+XPJlZmjol\nF+QBWbTQfgV5J+Yz9GfzFuba+ZIkLPKErbZor5ikymyLtq7J++3MxvowW5DnNHmfZxV5ZJJn7nJ1\nuGbNK6x/8QDfLwuUKbBxPVlR4FkaY4khj9Eu+pkD8+6G5Gl8ieslfTduMC5aXbTXTfZ6ntNqmVIU\nRVEURTkC3UwpiqIoiqIcwRuV+VoFvLiaDsypmwAeHvUWprVkBa+1tU2eJdYX99dRl3IsOTD1pRnM\nhC0HpuFdABOyU8JU7yQw741CmP2mD2A+9ZaH5uedCzNz7sILwL0LM2ZzhXpOScb5Kizc8mJMueoo\n592EHH0CH+VLJ3jXkjzUQo+9k1DuiGSVU7ElD5C7yRf21xtDcumaPEAoWGISUqDOJcaEcVGX2Ryy\nSxFCdglIUSoewDzNAU6tDa6zAn1se3hXy4HJV0TEI5O5n5I0QmbsboP+dq/Ql/MO2roVUMBByus4\nXiFhlHEp+OUUcmEVkGdbD2bocQkz93eKw4CGp6I9xph3MsgVLIcI5aArNiQfWJAelg5J3EMsL30b\ncyctqG4u3nuH8tTNd1gTnBa8tjbkdZuSTDciTzURkQV5N1kl+tAbkucheRUt1nhfbKO934opz2aN\n8fPVPvrwG5STrUfP8Sk3YR5T0MspSRK0Bp0KE0Oq8QuUbSMY8yvKO+ZTrtOsT16NH6Ddpj3cX1zh\n8x4F1FyQFHR7i8WrJhnNa6O+2xr32P67eH5+GPxy00I7dgUewi4Fi+1eony9mvJA3iNpuqEyDbEe\n7XKsL++SF+hTOo4QdimnIHkLbijo5N3y8BjIqajI63EVY1z3Fvhds0L02+2SvNdrWufIM72msZCT\nV7RY6J8FsPhQAAAgAElEQVSHJOFepuT9SB5yqYX1YSvkBS14TlMeeqyOC/LUJq9ze4PvbAqs/9YS\n7Z10cb9Vou0TCopbttAW3SvM5XWFcgwu6JhKQ2sKeaOfG/Kc/ASoZUpRFEVRFOUIdDOlKIqiKIpy\nBG9U5osCmF8bPuFPOdmihjzsLvD5poLJ8X4H5sQ1STqbDsyB9Tnl47rGe9ukeD1PYbrtkafehgL1\nXewoAJxzGHzM20GG7FH+uDSEyblPXoveGObkFynMmPcdSEBXNQLc1RTQMJujfHGH8hReo555A/N4\n8y55elSnDwxYzMmLZUDB2iYwQ5eUO6pnvbe/3mZwt4htyC7+BXkm5pDU7qb4nJyKZFShPS9rknzI\nq/FFTHm9fIyVoIP3iohkOfqyMqhbSJ6Tfobnhn2M05EFD7Zsg/67R15hUQCpynXJW6xNnpAJcuJV\n739tf12EqMN5m/JinZBsh+du6J9Yhj7fBjDDk5IgfQvl7gkFVS3RFoX/9v6645NHJslH5SOa+wXl\nsaQ8gHGDeXBzi77pWIfyZ0VS7ZSkBG8KL0GrC2nJtyi/okcehj7l0FxBkn1/jXfXFcZzElCAWfIY\nbZZ4TkZBhwfXuP9UWBusOdM1ScdjzM1WjXa/tnDPdoq19crFfGnl6I8dBWpcsTcbzetmjDKkFgVw\nzCGVOue4v1dj/cx2h33pWpSDMaCjIhSEMryAtCkV3mfTWu5UWDw2t5j/Q5pSO5LLfJJ86gXWlygk\nLzKSvKry0wmom1GOuC4FhS1cCmZrob3PUrRL1KCe6zblNaW8g36C9SisEc146eCZpsS7ZhSkNlwi\nALHXRx/6CR/jOfz9ScnzrrhCGwcT/B73H+I3bk75Ts/Ig36+wOeRjzEzowCzy7dpni4oIniJ3/KK\nggJLRN73XQrI+glQy5SiKIqiKMoR6GZKURRFURTlCN5sbj6hIFgG5rTeEJLfZA3TstnCXFetYBr8\negq7bPwWzHtugmurpuBuPkyd1zZ5ZDXwUFhMYPa7N4TJ+IpMmqPkMIjXpI93eBRYcJHCPDgMYFp0\nKYBcRlrKk4K8QMhLLnsKs+lZB6ZIwzn+uhRUktz/EjLLBuQldSoo7ZjMa5heB2RKd1yUx8me7a87\nDSTIsI12C9ropzZ5klxlMCW/Hd/fX29SmNXfo0CdxZfh8WMvUFCWoOYUyFFExPXQ1uc+ZAy/IlN/\njH5d3Ye88fCG+szCu+/syNzeg8Ry3sY4MEv034sKpncOHNnfQi57RPknT0kWok9KGo/jISSWrZB3\nk4O2MD6ZzDPyqiH5r9rg/rVPOdlu0CcFed0VS8rtNse8e7/EWtHdQLa43R0uZeV9tLF7jbrtBpCJ\nxjWe6xnU88kcgXMvegj+a4/QLg55AFXTh/jcovx085zux3PWdNZgFp8+COt2iXE3Cygw5pyktynJ\niyNI3CEF480KzEFrRbkbl1hnayp/n7xltxV51/Uwlqfk2RVQjsY0hCy79jBuRERqWr+sDtZcM8fv\nwJlgLZi46Kd753TcgcpNsaHlWU3eu0/RRnPqp8jBfO+SNLsZYFxXNUWTPiFBjfe55M27FKyXQYhy\n5wUdHTBYO4cJ2vuygJRt2WhToZyotoXfrqun5IEaY71reSTTf5s8Rynga3t36LEa0G/coKTgxy20\n5XJDgTqFAipTgNzaoWM9DfYNwxD3fLvG3By5qNvMQ93iAda1i5TybO5eLwirWqYURVEURVGOQDdT\niqIoiqIoR/BGZb455dUKSQKqcxTDhLgecDAtn6Qqys/UISllvoXZLyIPwakFM16HAixmbZhuEwem\n4ScWcvxdLGEmrwaH3nw9MmvPHXhBxAHMr9ME7+sknBuJgiQKTJqGTJpveTC/rxZ4V0AeQxcuTKM3\nd3G9DWAq93wKvHgiOiX1B3kJrSmYo3VDMswZzLAeeRc6JcppGfRrQF4VDyy055YCKto1JMKojbb1\n13j+hCTIcRdja1N+xDPTQn0iA7P0TY77zrvog2gG7zwJYQ4OBpRzcoLPO5RTyqaAeUkPUvOdAONx\nPoFZ/XGF9o28R/Jp4FJOwVEHssdiAZP8pkNyA3v6dDCvmy31LSmpawPTu0v/huteo27bgoJE7kjO\nayCf5BvK60ieTcmEormKiGNhLdh2ULdggULd+mjv2IPcYM7xPpvqXNiQBvKEZKkA46oXoL0c8kIN\nGnzXI3mm6RzmFDwFqcHY6ZDXVj5FQNH8PspZ7lD3RUKeWi7Kv7DwzP4Z+m+a4R5yHJPN4p39tW3Q\nT90Ka1TSkJxuUV6/HnldiUjs07/5l9R2bYzBGwpqTCci5JKGhedgXocZSfM3mKfeXcho0YxkK87j\n6tH4uKLAji207ylptminbYqyDjuUd5ByZS7pd8mjozVmjbpVFFy4v8C8e0LBbw3nvqNhWpAXnVlj\nPmaUu3Ize7K/DurD4JfJgIP5Yl24nuJ3qt/HcwsPY5KDNm8p4K9Lnp23HbRRL0GAa2lBau9RrliX\nAmg/c+nYTKy5+RRFURRFUd4YuplSFEVRFEU5gjcq85U7MtfZMKdFFCSvILN0QsG0OuS1llDQuLkD\nk96IAnJeU2C8TgiToe/AhNwt8bl9B02xLiAR7ChIpL85PN1fFfCy6kbwlFglFPiNZKn1DmbDMQUu\nm8Hh4sArISsp6CV5T+1yeButHJLPIjz/iyRPljF5a5yKEGZSf412uJxTIMUGbTpcwtR/Tl57kQ15\n1U5g5v3OGnUPKT9ck+O9d3OSIUhSsUaQCL9gIJV6Lszw31ke5ubrUmA820Y/m+e4rxNTvsMeJKLK\nx+dBCgnAo4CBIXnYNOR5Y1MOq6TBQDCUB7JF0nRtnT7Io4hI2Ya35aaCOZxl8WqFvp1SG5XPKQfd\nfcrJlUPC7JPnlk1etx8WkEu95+iraIjvPp6S9xjNxw3lscwM7hcR8UkpygULwzCCpDVaIYDnVPB5\nWEHTmJtf31/v4h/eX5+1sZZ15yjU1qf8ZxHKfU3p5jzygBo/fT0p4ZPQbMjji3L/eW2MZYukLT7i\nEFrkeUf/1q4jCg68oKCaEQIOz2n9HfTQDk6Czqi6NLc2WJeuXHhd3W1j7ouIRCSlPb6P34f2I5TV\n3ZJnNnlt+fQ74FKO0lWK3xyh/IKXz/Gc0QDr1HiNe64zrCMXDsb+avJ6QR4/KY2N3yNngDa7aWHt\nP6d8l06F9l4/oyMuI5RvVGJtviZnu/EGY+TxlNagHXmTbzD2n9sY2A4FSz2j38r8/PD3J7jBXJuN\nKAcf1WFekKd8SoE0R+hPb4dx2+pQYOMF+nZFpznCGb576aD+PkUNcCkw8zY5zMX7vVDLlKIoiqIo\nyhHoZkpRFEVRFOUI3qjM5xuYb5c+juUXS8pnNSBvrTVMjm3yABsZmD3nL2Am98mTqk1eWCbDc246\nJM8YmAmD5zDpLTMKLLeB9GL1D6WhzMBjwRNIA5ZN+afYU49MtPYMdeh2cc+SAoOOKA9Rk6GsmQsz\nZlpjP/yAvG+eXpP5VA6Dpp0Cq0ey6BXq2yKTuXOGnHUxmXAvBf39bog2XSxw/0UHUubCwru+Aius\nZORFt7sD8/wZyX/eHUiEvRKf98JD769nKfqvQy4jDy1ISZMtxkJGXmXDBnW+IY/NUYU+u8zRry0K\n3ioWyreg/HC1gzHhUzDLuIO2PiWJQTDQmnJc2gnmUU1ysTfFHFldoA5j8rycVzDnb3v4brHAPO1Q\nkMhJjWeaGfrWeUHB+Ug6rDyap+FhENbAQjutXLxjm0Am8mLINZEH78EwhuS0zNEPnQzyltenYKY2\nnmmtUbddiD4fVLjfm1BwYZKIT8WW1Im4S56tW6wVgYv1oUlxTxpT+Vc0R1JuX9wz6JB0GJPUNv2R\n/XWb3+WhPRMf7dz2IfFmt4eBaR36jvcC/ZoOKHjkGjKy1+Bzf0SJ924oz6aLubxs4X0XCfqJnNyk\n8Eiyp9x3NuWBi+98Orn56g7lPq2wlg9L1G2wRN0KCk5qvU2S6Q5r2SV5zkYzzM0P1pi/I/Isdzx6\nTg8DLPJIUs5RhrQhz/UXh8dj+h0KZvwcf3Mv8KySPC8dqtsl5ceNbLxjnWMuFxQEPKd13fZR7m4P\nR1PEwu9pn46deMHrbY/UMqUoiqIoinIEuplSFEVRFEU5gjcq8y0pX157B2nA51xdlFcqsGF+zkkO\nujbkTdIhE3UBU+9kDRP1OQVr9CuYk5sSzywpEFlOpssdmVj7HwkMmJOJdzZEuSsO3BfA/OhewSw7\n8WHezLwP8Y4Cz3QymDqXc7SX66B8wy48r1Yp6tPjgG4tyv13IkbkYbL10QcZSQxbOEvJlILedUsK\nsNZACuoECHwauRRgbw0z74KC0zXkRRjv8O+CFQWIbF3BhPuo+M7+unQowqCIlPKt/XUyo/63KIhb\ninasUgoAeksSEdW/dvGckALWWhXe/YS8Fts3MGFPH+LzbhceUweRME8J5aNrh1/eX5cVytS+QF/Z\nCTyJHpDH34w8MpszkjMvMTZNC3Uz5AkXtjEnbqeQyJwh5XvcUEBCkgWlexiEtbkH6eZtCtCYWyh3\nsqZgtm28o3IxrkyFe9I+jghM5pC07lFw4aaFeZFSgEojGJPGhZQ0Xx+uKafAJi/oa14TPFpPKJDi\nklLh9W8xN+8UGONPA/LSzZEfs+rRnNhgrbNbqFfZR59Va/RrNKRgvCu0c3D38GcppTIFEa01NC82\nJZ47rHD/5RX6pkt5EwdbjAPHwjxd9GicLqkvI4yPgOq2o+Mdl+WhR+mpMDXaLwxRH+cxyjEfYEyN\n76Itb+eQwkIPv7NtO6XP8fy+hzpsUjxna+jYCLnUulRl20WbNiXWhDujQyl7nZKU6GKNqAvUbUUe\nifZbGLf9DOXOKP9jl3LRrkIKsOqgrL0YbZQ0GEdThwJQ0/Gg7Q3G0SdBLVOKoiiKoihHoJspRVEU\nRVGUI9DNlKIoiqIoyhG80TNTaQF9uaHEqsMe9PgoorM4LiUopeMR7ZR0TUrAWa4pmSqdoejtoIlP\nAjontcV76y6unQWd+7Chp07Lw/MqhpI1p9cUIZbenVf4/hklJa43FFl6g+dsM+jOawoHEdH5Memj\nPpfs0r3Ge8MKWvnkkjThE+FSAtjJAlG/h1vo4aspytaEOG9yRS7H9yhMgPsOJcSscD+HhUjonFrR\ng6tvcQ19Owno4NLlr+wvl3QupiOHfTkv8dwmp8SvXZxLWG6orVOKCE3narz0AzzTxVi+tdHfow9R\njqs2znFcDXBWJ3yMcwLTr+BMQymfjvt118H5kyWFaIginGNaUbiCVpuSfFME4TLDWHApeXRyS2OZ\nzs/dCCVKdfDd4uJtXD9/tL/udyjcAs1ld36YzLtF59tqm85nWhh86QX+LemsKco0nacLKBxKu0Ao\nlLbBWcWkj9AIGZ3decemcxx0JqighLMZRUw/FSsKSTHKKVk6jcEluZU7fxMHX5bnWH8KarfzGj8V\n5ZiS4a5xNi2gxNNZj8JfFGhDz8MBrRUlSR4PcUZm5RxGEn/7DL8bNwme232C8bWls6aZhTNdA0Fb\nJ1306y2dkwoaXHf6b+GeGfrSpjO1CbXLms7R9gefUmgESio+2qHNVm/TnKLMHlc5xl3Ypv50sL5k\nOzp7Stc9CovS3+Jdly161wxzP6dzUmc7nElNKBSIe304N4MR1vxJgflfU8icDmUUMSvq2zb6alDT\nGSga28YgxM69Ns5pNwHGS5ahDu0F5ktlKHxRphHQFUVRFEVR3hi6mVIURVEURTmCNyrznWfkahjj\n1bEHeepDNi2T3DCbkKs0RQ12G8hBtQVzdURy0GOSKm7JNbcscH+XpIoqhwlwt0GZdwVF0xWRkCJc\n522YVvsTvK9NUaOXPXwepTC/yxneV6zwjh5FjK9zlO8+RYCfkTuyQ8ler9cUBfzs9C67NZXzLRcm\n2e9kaN9kgzKk12ir7hDm1seUoDKdwyV/7Hxzf70qYZ7PG0gGjk8u9iVMvgFFfL+m71prhKCY+odS\nQrD8wv76+QByU3+K/vPn6IMtRXtu7zAe5xQmIiH38+LbSCT8jBKulgGZwJ9QpP4A97xzhTYNv4By\nnpInG9R5nJDkZ1C3lk+uzyPEvah3GAslKR1Ois+dFvrntiap/QoySWBhvLRnmHcpjf2co3tn5AId\nHUaG31GS0taIpEr692NC1w7JGMZBnb0YY7skWb+g8AbDBmMsCSFh7Er0bWtE7vczrBWBfXqZL2ww\ntp9TnwUNSRgLSgBMUfXzNmVLcDHurBYdIQgpe0OBufztEu8drtGGdheym9XCPfGawg3QWtFLDl3S\n7TYlonXxt+UdyDP+FG1aUkiSNjVvvUQ/dSgEwDLGc8pLlHvYRhLuZoE6Ly1Ebt/28d32FnLvKbFK\nzJHmHG0R0bGOmo6BuEuKXE5hHBzKuhFwEt+IwkFQSJp5jPpEKxypaL+FNiophMlig3bZhfh9iyli\nvohIOUc54nOKeh5ThH5KoJxElHC6i/cZkudiCpPRULT2+Bb1MRGt37T+WxR2yaIjHtm91wtDo5Yp\nRVEURVGUI9DNlKIoiqIoyhG8UZmvMJRosIFMsDOQqqyEJJMIxWvdhUdH5cNMuLt6ur+uC5giEwcm\nypkDc3ttwTw/MDAfTvpkwr+EiTEoYSZM0kMzbm1BPrDIrL0+p2SMLplQdzCnNzF5U1Ck3TDGM/sb\neMrszlHPJKAo0FPIZxuKcOySp5M9OX2iY3+INn1GUZxtA9Ood4br5+SRUc3RVrXBfn5tw6xediDh\nWELPTyj6MHnh3CEp4Tr9W3iXDS+XbULJcyn5qohIEcFT7+IpJeYsIX/NyXukEMgbNUUTvrlFf9gW\nTMk3W3y+iNEuD0nCup3jvd4F7rHuUrRm99PxGDpvYbysC2gjfQosHoaQRspnMOk3YzKrb/CcjUMJ\nmjdkYqcIyvmIPHUyzF9/DnO+Rx5mASUSNj49s/mIBN9gXE1C9JXTICL+iJK99niN6GCMheSB26W5\nWfqQbZc2yt0v8N6swXtvFmg7j5Kt25evF2X5k/B0S5kEepQhgbyXqxjraRCjvwcW1qV1ibrMF2jf\neyHuz0leeocyLG/GWKM6Je5hWTA5o2TR5LErw8PEuHOSAJsUbTegTApNB/1nFZizXQ99sxGs3zsb\n74jpd8lYo/31IkNZGwe/RaFH3sUvMAYnvcMxeCqiCu+wtyh3I6hbvab1i7J2rOmoQX6G34G6Ie/J\nFdr0PMMa3AoheXUD1G2aQMLLM6zlqYXPxzP0x3xweKTCGaL9rBLXuznmZnhBR2Jm6M8FJVLvRCRz\nJpRtwce4ehKhD8MW1tSCvD+7FXlsOyiDXOO9nwS1TCmKoiiKohyBbqYURVEURVGO4I3KfM6OPNju\nwBSXs/xnwfwYkRTmZpA31g4kgCCGRDjNIftYBtJAZkF6OHcpeaODZ9rX5J3UgmkwX0LC8MNDr4SM\nEjRLBq8fm1WiCeqwoGCSD6ludkIB9M5QhyvyYrEo6XEl5MWSYT/sleT9h0vZdE7vzZf45I1pYNJ/\nTNLJVtBPLnmL3foIqubMyYPjIUy1W6iakmToM/vLMNW2C9x/+fRreKaB2XpOAUXtMcZftj4MZGot\nUNaUEu7WFMCxRR4mswLm7ZuUJOIrvC9P6R09vDtq4HnyaxRs7x2SDn0HMky0wlge3oeceUryFcbm\nF7wrfO6j/k39zv7a8SCxjEiOv0UziksBLNmrqiTvGc8mU/0MY2f6kDwYn6FdVm20dWuKIISL1qE3\n372YguUW7+2v7QjvcEck+dH8KslL0HYouXNe0udYv5wI42JJCVSjNdplMMCYXEIhFJmfXoLPUpQt\nmmGc+gHaurIw7zY0ljsW1kTvi+R5d0PHEkimbt1gvVrR2vpDlOh2TYmwLylRr73AnOg5kCNNfih9\n1imCMYuL8d+i4J5r8rS8SSjAKHn5tWOsO5mQJ/YM13c8OoIwR5+tXPS9u0S5By2SmrcUWPmEmAF+\n4zwHZe3kWOTX5ME4JWnanWLiPagx9p+Q1/X5CH1iEsicUYq2yxdolw0dwQiusE7ZEcbU8wyDPNwd\nJiFPbIxPu/3V/fWAPADXZOaJ6f6cxmdJa4cToS3OLYyfFzl5+9NRgJaLfqtJRu3VSPieWYdy8/dC\nLVOKoiiKoihHoJspRVEURVGUI3ijMl9QwERXG5gHexTUq7iLe3KD4mU+BeGjvD8Z5e9rk6mvpHx0\n/R7uKS3c45I3UDuk3HoUGG7TgdkzIRlRRCSinEl2nzy01pS3rIv32TUFG7Vgcg4pcJmbwRTrUD6v\neIR9byug97YgQ1yRF8M2h0dbWh7Kk6cgoByKVx71WQATcztD3a+7kEHdjAKmkeaxWsM7JarQ38sQ\nstODOerbkBfS5RJ1N/eRQ83KYaqdrNGvjhyacF2h/JBCASMd0ktnMBlvyHvEp3+SPO9RP5EXUn1L\neSn7MDdf1JS/DbfIgxjy2qZLkknv9HkWRUQuHcyRmIJt2iV5VXZghk9a6J+4ggx5l6SOyxbqvxvh\nOR0P99fPMDbnNvK2jWv0YTREw0wz6L9hiM9/2D/0pFpTbsrWOxif0QyyWlhQUEry7E3uow6xQZ2D\ngPowxPjJdyRL3eA6OMNYqHOUxw8p0GF5+v6MyWNuM6C6uNQmDeSidYK58zyFBBs0WH+6lO+t56Md\nagqgbGgNfUYyXyzkjfkYa12fPLSDDcZT1aaJICJtyqm5mkMWt1bk8eqT/EX5TRPBekqnPcTfUj0p\np96O8gWWbbRj6VEuzpSCwFJ/u59CAFYRkdYSRwdqCpC8uYvjGws6UtDJaM0jWTskD+/hEvOAfx9C\nD+vrakmeye+hD6IbtPuCxnJIOTdjDzkOV/ah53S4Q1v653jWuoL3uplizb+N8XlXKDdhAq/Vto0x\n0wpw/9hF/ecUp7QXYMx/e4M6dCm49PY1t0dqmVIURVEURTkC3UwpiqIoiqIcwRuV+Z408MoYC2xu\nYx9eNXYNc7tDZsnakOfGGtJLm+KBrc7xzILM2+5jPCcbwwQ4Ik+9ZEyShEuBuxJ898KG95CIyPSG\nJAOy9Nc2TJcZ5dHz2HuQ8h75W5jTb7+ELrm7gPl116VAijbq+XiIF7c+JM/DCvLk5TWCkp2Kdygf\n2zPK59RfIejbdEUyioV6JazI9NAmxQ4elS9SeO1EFczEv1bA/OuSV+DdELIC5z4z22/sr2c2zP89\nh2z+ItI6Q/tOlhhUHnnJXDWo27YFyc95/v7+ekN5IFdzkjwpB+Gmg3aJMjzHPce/bawuPHiGA3g6\nXZhDz5hT0ZlSPeG0Jw93lPOQ5pRQUE2fgvjNXJTbkNl/SFJttX57f72jnJa/M8f6MCMp8Iok2PGH\naIsJeWQNzKHM13RIbiV5thVQjsg2xuR0i7kTJbR2+HREgPKCjTco36JDk588htYZcuEVUzrKQOPi\no4GAT0FBQZCDa8o5OEJ5KsqR1xesd76HNnFI5totafySBJun5M31Ftal+oqCsdrkRdWnNZPmkBth\n7ktF0rqILOb4fhOjPzYuyroRzP9IIMGHFOQ1beFzm/K6FTPy8qXfgXKCNhpRAM/dCu1oBhi/Lj4+\nKWufghOTfLqD45l0bNxjLNzzYIPfltJHWaOHaFOvxLxrqE27AxxXmaTo/zat5dUdrHcueWC/T79L\n3eLwSMXSRxsXdIwiIi/qYYl+Pqcgvwn187omL2/KL3lp4/fuPgXsra7QRrcj7DP6JcozLTEWnNfc\nHallSlEURVEU5Qh0M6UoiqIoinIEb1TmG5Mp3p5AZknehldCvMTnfg+mOCFpLyEJ6yqB+bElMCFP\nN5CGGh+mvi+SLXZGHkZWjqYYOTA3dmOYgzdTcgcQkQdfhKnfqvF9m8zUd2q8u5uQ/JRC3ur38dxm\nis8D8kJsSBqwHDzn7QLtNY9wXe1gDv2SdShpnYLNO2iXAXnJPSfT8KKgXGAbmF5rF/Jc3kffW7fU\n3zZs2Cl5+sQ2ZJrph/j8O5TLyVCwxLwLGcLcwsyb3D+Uy6YkF3spzMeNj3E06+D72299uL9u52hr\nj7w6C/JCWpNpnGKxym2CtvOfklfNPbSXR942pv1F+TTIXfy7KiKZNPshSJvVE/R5mOLa7WPMtiKU\nO0lxbTL0bWGjvc4KzMG5g7EzpPVhJpizW3KYuneP8jrWlNtNRPIAnqFnDaS0ogtpaRvg2muRp26J\nMWnamOPtp6jPrWB8+lvIfxkHoX2HAoG+wHc7EcbXOkY5T0U5RP+FOcpzt4V37VzywiuwXjkeNfCE\nfx5w7ZGXsddGvZprklNLzDu/gtQ4aLBel5QDtRlTcNj1ofdx3IEMtaFcgNKQxPYBSXUtCvKYkpdg\nSd5/lI9tauFIwdntI5RvSJLnBmOztjD2rRuSBf1Dr7VTYZEH86SN8q0ot+GQvGjvrCCFbweUA3eB\n8k0NnukNIQtHOT6/Iel/EGIc5RQQNyG5v6Lgn4MN5qPvH3o5Oh7GwLhNbWljHVl2MXeiNj7PSG7s\nWRiH/RnW0d0F9geyhiekCdFGgYc1ZRXh/nCGPq/918ubqZYpRVEURVGUI9DNlKIoiqIoyhG8UZkv\nGZIXSw3TYr+GqfCa8vAMbmB+Tn2YWZ0GJt13fJgDn9UwD/sRzMbeDubdK0oP2HZJXgtg6msayh1W\n4bv3LZixRUQqkurSBczGGZnQ6wp1S84pFxgs3CI5BTNt4f6GVK9sAtPqsk/eiXNcF1uYNIMZSVId\n1OFUxBTkMPTRLuceeeRRUMR8jDIki2/vrwvKLcg5DXMHpuFlhOeMemjD/Nco75iNvl+5MDFbgnGT\nkifX9lAVkiAnrzXKvZWtKIBri4Olot1jqkNJgQQrg+vaR+V2FABwbWB6H3m4Z0PekhmN/TL5dPJ/\nOT7Gb1GSl9Qz5BG8N6A8epi+kj+DG8+Ogrk2OdrLIe+6MeW4/HXKg9cL8N4byiM2pNySFuX5skiC\nOwmjgwIAACAASURBVFt+JG8myVWrCe5zLzAvbAeT8GGKMfY+ed1WGGISN5SfbUiyxwZl8iM6CnCN\nMWLmCE67a2O+GI8WpBNhUjwzrzGuX9CaEN/FWPNtzKnMwjgoyUvXsTBmS8pd6ueU95PzMrYgwaQO\n+nhH6/uoRvs4JN+sosNjCX5CEnEFecbroa0XEeSiWlD/McnRH1CwzWGXvLQ5hyat6RkFiIwNyldH\naMdgizG7ag6DOp+KuvUtFK+AVNtJ0fZbg/benH+AMqE6Yu3wP4aCBVPcTPm2A1l//AD3WyTTVyTT\n39mStBmg/p0hHY/59mFuWOsca8qqi7Yf2Wjj33GLOfviAebXXRrDQkdothSE82wHWX83gPd+lFIu\nQwoobT2Fp7FNfbievt5aq5YpRVEURVGUI9DNlKIoiqIoyhG8UZnv9hrmwfdCBH6TCSS23h2Y+ooZ\nvDh2LXiARQHMr4kPs++ZBVP/NoGZcdGCJtHNYNp/saXAdQOS6Si4Y58CcIbtw2Byz59QQLwW7svJ\nq6GboG6jBGbJoo86zCqUY1BQ8Mnto/31eYX6lAuSiSyYru91sTf+xjVJMs3pg3a2HJiVtz2S8Hqo\nV36B4WW2HAyNcisu0IZZH9e9LQWey9Cvs4K8h87QTysyT6eUE675FsbTtkX54eQwJ1paQXrcUM6/\n8pK86sgL0VCTXo8wTlsT9EHvwdt4zg51uMxQ/7tItSUemaqjiALsZXjv7hz5605JQTnyIljGJU1p\nTpEUQ10oVB1xyJvNBOQhuEObviBl66KD+eisMK5NAR22LDCO2EPS21LA2tGhNFSvKU/YHfT1PerD\nhSGP4hRyxeCM8rndwOyfuTF9jnJsc5S7tiGxFGvyJHUgz3g76GFudeghfArqDPNo5VButhifJwuS\nMzJ4pvbab++v71Cg5C0HV6RjCVcUXDcmD2ITYH1w1qhjl449TGKMrQ21w3ALeVFEpCTZjlIiyvYZ\n/Q5QQF2XAqp+0EHfj1fog8bD+7IU5U4NnhlW5Hm4omMADnmNd8jbd374+3AqrAbv65D8XZDk35+h\nT1YXdGzkkgIED1HWhM5UXC0ouK6FebcIUeeeD6nOxmkSib+E35mMnN+KW/zmtseklYuIe4Y+MZQX\nMSKJdf5lzPlWhjrY99C36xsUZHCOtmiWFBR0gf3BvIWyerdoUyuGzLedYo5L/HrHY9QypSiKoiiK\ncgS6mVIURVEURTmCNyrztXyYKE1EgRUL2AeLW0gmZfiV/bXlwaQ3oTRcrRXMzM4ZTNdpRTncKMhn\nXVCgs3sw85/1IKPNcgruWMGLYbM6NMnb5zAnns1JGnhC0hDlUsvGqEO2gpbSD2CK3G0oN2EEmaC0\nIIckhuTCGUygX9/BtD4lLzFncihpnYJbctC4y6b3IQU5/Buo+6WF62UAU+qDNupYFMh9uPIhMaQD\n3OOU6DOKKSfuBCb8+ppyApLnTY/+7RBWhx4ma5Jedg0qNCKzsk2B/uRHIBeNyZvrpo/vFi76O/Ix\nvtrkeRayx5APL5flA3xe98lzsiGXqRNSBOQJS0Fhxwm8MzlH2kOSwkvyTrwZkTdYBlP/hmTVfkZm\n/znlc2uhf+oORVHNYP5PSObokOzmZBSoT0TOWpRLjjw90wqSUVNDlskL6K3WFnXYLrFelDHK6mwo\niC55D4mLsuYO2q5K0aYleSTOy8NxeAr8CP3Upbx4GQWwDGoaX2eQPGYF1o2M8sCdTzEHV+/hni4V\n37ugoMSUP5SH7FOSDscNxlBA3lVpdPhvfKehALsFeYl10E9fpYChv0LjcZBTAFrBb0KHpLptSd6/\nLvq7Ik+9jLw3Z88xljOSwgblW/JpYJE3etWGnFX7KPfVCO19kaPBlx7Wv2hJwSn7JLVRe1FaR+lT\nQOGmxB/Kdykv5y3JjmvKPRvgGI/bO+xPk2NeOMUX9tezAO+zIvRnkKGeIeW6tVsUYHOGe+qAJFwa\nS8UVyprn+K0pFpSXN0V7eRklG/wEqGVKURRFURTlCHQzpSiKoiiKcgRvVOZrhAIgOiS9efj8PuUG\nmgk8NMIdpA63jz1gKyRZYQ5zqF1ToLc2yT4B5fKLYLpcr2D+H7iQy+Y1uS6M8F0REfcS5sGnIf7m\n5yQNRTCV2zWkPUOyXbWG6XZKuaskg/zX25HEuCUPQ8qF5lac+w+m+KJNuuiJKMkzzpTIBZUmFDCP\nrN63C5jVv3RL5tkxzLM25WkK13jm8wZmYZtkULuBebpzBnn4WYmy3Ulgtl0JeR02hwHZEsoL53oU\neHKF9g075KmYkjQdvb2/fivG58aFPNFycD18F2Va3JIr3BB1u7slD9d3yEuK8kaekqyAFNYmb9mC\nAm/2SAp8Tubw+oa8Nrvof0p5JeUWfZuQF1JRoV2KLUzvQsE8b7aQZ3oR5pYxlBesc/jvQmtBc6GD\n/mxfQSYuOzQv6KiBTYFXCxvjKhC8e1lRwN4Q/Vabb+yvN9fklWZIwiCPttw6vQRf7NC+foM6uhQU\nMR3hnpj6Y+ygLh3ytDQRymnRWjQjOa+fknc05QScUA61mKIVrygiaj2HzHo2Psxxt3mC79ddvCOc\nQl79G1u8r9OnAK7k2ZhQ0OVqQusv/QxWPuUBJa/gGS2/LnkdV5RfMAnJE+yEJJRbVm7o92GDNTLq\nYHyVGeYmRqxIq4u1dkMemTsbc7Y/QZ/sYsqtSu01pu8agzGekfe2t6ZAtvWhN/mKvAf7HcwFr6I5\nTF54TUHHbprL/XWZ4P7ERf0jC/3g7XBPtsF3g4AC85LkJ/dpbXr+enkz1TKlKIqiKIpyBLqZUhRF\nURRFOYI3KvP5a0ga6zXMlfNz2FDLNgU9XMN0t6S8aE1GniU2THTBJcxyiU8B/WwK7hbALDmawYy5\nJO+kokvS2QJlbmWHuZd2DszRXzIwLX5QwIyZkmdgZeDhYJEZdEeejaMMJu18DZPrty5hog26FCyU\n8hfmlKuoII+poqJoaiciItPtZY53nQ3hXfkkhqTy8GsIDFi0UXebJBKzwf2LO2jbsxnap6R2DskD\nb5HAhG9ZkGlWJQXeI0nl7gXJayLSWDD73pLHWC8l78T7JNNmGLOdMclc1Zf3114b48teUjLAIXkz\n+ihHcxdj//wCcl7PxpidLD+d3Hz3hhRMcYe+TUIKQGsw7+IpxlQ6phxuq4f765qk8yjHdRChDtsS\n490StNeGFLj7JD21N+ibKEY7zr95OMbLB7gvmKON5wHy0FmUV66s0f+tNSSGoiKv4z5pPTsE5/SE\nJO8cRwdaMUkSLZRhd4U634lO35+lg3Hu9DCnmiXWxz7lX8wCjPGG8mCmlHOzqFHmZo72GQdYf6Yx\nxkFBkvpZgDJMSqzFbo0+6ztYQ+YUTFlExAzg5dpJ8NyNQbl7dMyiXuEdj0MKkFqiXSqS/3OS430b\n745JUt9MMH47FLB43kObtnfP5dOgvaU+odyP2y3lSPRxT9nBmIoo7+DzW8pHSGuWUJDTJKJg2iu0\nUeuc5HUKZJ2W6P8wpLFcYrw788P+bJ+hLVfkqRjSUYuGjscEFHh1Q7+JXeq3jPKaLncot5NhXCzp\neNC9x9hPTDvoz3ZG3tit18ubqZYpRVEURVGUI9DNlKIoiqIoyhG8UZlvQSa3fgfmwaBBAEQvQaCs\n5yHJXzXn5IJJPrPJdBtAGmoCloZghrc5EFlCz2zhu8Gc8vxsIHmkN4fSkOfBwyFvYCoM2vAALJ5D\n3okfkJeYA9NnUZPnQgMz5opyDTYt3J+XFNyQPEuW34Cpt93He32SDk/F9AXKdt5DGeZ9eF699wRm\n5et3f/f+er1DWxvyNhoElCOKgrTeUp6+DIqP5OTB0ypgns0alG3yHryx3nbhUeY2uBYRmd3iWb+T\nPPKGlJ/JqzGmNiPy4HoA+cud4Lu9cwok2qd8j/TegMzTbhdjuXMXc6IiqZLCF56UIkE5BhblM6wp\niN2O5h3Nry71Z+sK9b9+hzzGasydeo02rRPI9FP2Ot1ibp7fUoBMGu/XDcrWZg8uEVk+hoRndSET\nrWtIMT3y4JvSGrHpkcdkQnkwbzEmrRySRIvWoGlGOT5zPDOcYQyXFt4bZ+xvdRr6lBOxXEJ2jChY\nYlJDju97WIvXCUnWMZ4zXWMenbuQcv0QfXCP1r3sFt55OQVsLUqsddLDHHpM+UO9yaEsdHYPffl0\nTd6/BR33KCHJUHdI6znq4NXojwVJs6nQurzDl78ZkhctefYt1ihPRutOTsE/T4mVYb7ElBQ0i1GH\nokI52pQrcrHGGuwPSNpt0A9ZG79jFnnFtjaY15FAwl3auD+6JQmOvNiHdKTiUXyYs9CmnIz+lIL5\nVmi/jDzWJUcd4hLjlr3r0w3qU64xJu0x+qo/Qf0fBRQINCePRBvta6rX87RVy5SiKIqiKMoR6GZK\nURRFURTlCN6ozOf5JJ/Rq4sZmck9mACjHGbgsgVz3SqCmbEpIG3ZGZ5TO5AJojmeeSvkkUPBwAx5\nmbDHWElBBYOITI8iEuWQLhYW5AqPPPjcMUzIyxX2rlsKjjYg75AXXUgA/hpm1t6YpJdH1F4Jnh/3\nYOpOblDuWf8w2OgpWEbkJURSWJRAnrrsoX1GJcStey7axydp75ZMu94Oz9ndR98PbJizfcrntYxJ\nKp3hvQ/78EjakjdpsDr0ohp/kXJsUbC64oYk1RDle/sBJDxDORHzO2hrh1K2RQM8x4QYaxRHT7wU\nkkxM8nASkFecOcxBdyqeX+Ed3h0EAyy/hXYqv0w5zDYkpa4xpz7sYe7c/9toi8VDjNmUAtYutpAG\nnA3607HQz08N1oHgg0f76/whBVqtDvNm9hq05dUN5WesKXAh9XN7TJ1F9y8poLD1LToucIHxtiwh\nN5mCAhca1KduIE/4Fr77wsDj71RYOdoiDWlN6KBNOyTDJFuSUYZok+0c7XDXRltbQwo6W+D596Ai\nyjrEnCjWX9tflzHWgWiBORF5KEMdYfyJiDwi6bhcUABHh+QmIW9J8qYeDSnwJnm2TSq8b1jR70lC\nRyVoXS6X6MuFRxLhEmt0bh+OwVOxfIsk7B3qaZPHdnxGQWcfYc7alHev7WCc7gz6rVthLZveUA7J\nBtcTDs5K+U6zByjPZIF29IWOt0wOPW0bHxKwTevlJXnLR1uMt4oCRC/sr++vx3P89l2SJNemgMLp\nhgItU27d8QD9+fw5yuDSsYuW/3pBWNUypSiKoiiKcgS6mVIURVEURTmCNxu0k/J/bSM20UMi61Be\ntNKBie7WgVwTFDBvOilMdKs1TLSNA9Pl/Jo88mLKW0VB5vIdzJ5N8wzPrH9of93NDoOyTci7aUNm\nY3cIyaBzg3puEtRnRTJZ4MPsv55CnuvMyUS9RFelHj4flzDjvtihPnELZkw3P73HkNdDH0woiGq1\nQd3fjSFtJJQrMGvQN+4Q9epQ/i97hvZJUpj9R+SlKDM8p9dGIsDlGfq7pnyN5wF5kfUPPYaqGHU4\n25Ks8BXK5zeEubl1jbFseZRrzYEJO7kDebLb4N8tAZnVZ2OYxmvydErmMElnMcZ+fPcwb9mpOD9H\nXy0KCkhKist2jnm67cBDLqQArptrCkY7IMnzCnNnPUfgzIq8d4sFxmnZRfDAnkEfbD30f/Mhrt9v\nDqWELi1tuy7mRUNetwl50ZoPUW4npCCcJBOYFuWS25IHEB01MD55qgac8+1b++vOc5Q1LxAI9lRc\nkxdhQHLOOkMdwxb6xibZaknyXL5F3Rvy+JMK18MrPPPFQ0h4Vgqv3oaOPVgdzJt0TrJuinV59SXM\nGxGRrCHvypQk3z76Y5g92F9/LcUaFFH+TqsFb8MHFtp9M8H985KCt4YYX8vHlGeTAkjnS8zf0P10\ncvMFJGGWFDCz6tJRGTruMXcwHwcexumKvFdLG/UpBfPAGuL5t1e4HsaY+4sU362vUbZ2B2M/LXHP\nxsJaLiLSod/a5ROsi7szqhsF8G2e43dhZUO+t1z6naWcto1P6wgHzW6jXQoKfuwPaHyt0YeZhXn9\nSVDLlKIoiqIoyhHoZkpRFEVRFOUITNM03/suRVEURVEU5buililFURRFUZQj0M2UoiiKoijKEehm\nSlEURVEU5Qh0M6UoiqIoinIEuplSFEVRFEU5At1MKYqiKIqiHIFuphRFURRFUY5AN1OKoiiKoihH\noJspRVEURVGUI9DNlKIoiqIoyhHoZkpRFEVRFOUIdDOlKIqiKIpyBLqZUhRFURRFOQLdTCmKoiiK\nohyBbqYURVEURVGOQDdTiqIoiqIoR6CbKUVRFEVRlCPQzZSiKIqiKMoR6GZKURRFURTlCHQzpSiK\noiiKcgS6mVIURVEURTkC3UwpiqIoiqIcgW6mFEVRFEVRjkA3U4qiKIqiKEegmylFURRFUZQj0M2U\noiiKoijKEehmSlEURVEU5Qh0M6UoiqIoinIEuplSFEVRFEU5At1MKYqiKIqiHIFuphRFURRFUY5A\nN1OKoiiKoihHoJspRVEURVGUI9DNlKIoiqIoyhHoZkpRFEVRFOUIdDOlKIqiKIpyBLqZUhRFURRF\nOQLdTCmKoiiKohyBbqYURVEURVGOQDdTiqIoiqIoR6CbKUVRFEVRlCPQzZSiKIqiKMoR6GZKURRF\nURTlCHQzpSiKoiiKcgS6mVIURVEURTkC3UwpiqIoiqIcgW6mFEVRFEVRjkA3U4qiKIqiKEegmylF\nURRFUZQj0M2UoiiKoijKEehmSlEURVEU5Qh0M6UoiqIoinIEuplSFEVRFEU5At1MKYqiKIqiHIFu\nphRFURRFUY5AN1OKoiiKoihHoJspRVEURVGUI9DNlKIoiqIoyhHoZkpRFEVRFOUIdDOlKIqiKIpy\nBLqZUhRFURRFOQLdTCmKoiiKohyBbqYURVEURVGOQDdTiqIoiqIoR6CbKUVRFEVRlCPQzZSiKIqi\nKMoR6GZKURRFURTlCHQzpSiKoiiKcgS6mVIURVEURTkC3UwpiqIoiqIcgW6mFEVRFEVRjkA3U4qi\nKIqiKEegmylFURRFUZQj0M2UoiiKoijKEehmSlEURVEU5Qh0M6Uoyv/P3psHW7en9V3Pb81rz8OZ\nzzveoRu6CRIxJrFSEogFBguliEElxkqU/JMgEodQoYhixYREMyhGpYhSaixMEDFDhbJSkWiJZTQS\nYmNX0/Qd3vHMe57W/POPc/p8v/va3KH3fs9t7OdTdeuud5+11/rNa+3n+3ueR1EURdkAfZlSFEVR\nFEXZAH2ZUhRFURRF2QB9mVIURVEURdkAfZlSFEVRFEXZAH2ZUhRFURRF2QB9mVIURVEURdkAfZlS\nFEVRFEXZAH2ZUhRFURRF2QB9mVIURVEURdkAfZlSFEVRFEXZAH2ZUhRFURRF2QB9mVIURVEURdkA\nfZlSFEVRFEXZAH2ZUhRFURRF2QB9mVIURVEURdkAfZlSFEVRFEXZAH2ZUhRFURRF2QB9mVIURVEU\nRdkAfZlSFEVRFEXZAH2ZUhRFURRF2QB9mVIURVEURdkAfZlSFEVRFEXZAH2ZUhRFURRF2QB9mVIU\nRVEURdkAfZlSFEVRFEXZAH2ZUhRFURRF2QB9mVIURVEURdkAfZlSFEVRFEXZAH2ZUhRFURRF2QB9\nmVIURVEURdkAfZlSFEVRFEXZAH2ZUhRFURRF2QB9mVIURVEURdkAfZlSFEVRFEXZAH2ZUhRFURRF\n2QB9mVIURVEURdkAfZlSFEVRFEXZAH2ZUhRFURRF2QB9mVIURVEURdkAfZlSFEVRFEXZAH2ZUhRF\nURRF2QB9mVIURVEURdkAfZlSFEVRFEXZAH2ZUhRFURRF2QB9mVIURVEURdkAfZlSFEVRFEXZAH2Z\nUhRFURRF2QB9mVIURVEURdkAfZn6Ehhj/ktjzL/3cZdD+egYYz5pjPklY8zMGPP9H3d5lA+HMeaJ\nMeaf+LjLodwdxpgfMcb8N+/z988aY377HRZJ+ZgwxlhjzBsfdzk2wfu4C6AoW+aPiMj/bK39jR93\nQRRF+fKx1n764y6DAowxT0Tke621f/vjLstXImqZUv7/xkMR+eyX+oMxxr3jsih3iDFGfxwqyseA\nzj19mRIREWPMbzTG/P0baeiviEhEf/sDxpi3jDFDY8xfN8Yc0d++1RjzeWPMxBjznxpj/hdjzPd+\nLJVQxBjz8yLyzSLyF4wxc2PMTxlj/jNjzM8ZYxYi8s3GmLYx5r82xlwaY54aY37YGOPcfN81xvxZ\nY8yVMeZdY8z33Zifv+oXijviG4wxn7mZT3/FGBOJfOActMaYP2SM+YKIfMFc8+eNMRc31/mMMebr\nbs4NjTF/xhjzzBhzboz5cWNM/DHV9asKY8wPGmNe3qyxnzfG/I6bPwU383F2I+v9I/SdW+n3RhL8\nmZtxMbtZr/+hj6UyX4UYY/6SiDwQkb9xs7b+kZu5968YY56JyM8bY367MebFe77HfegaY37IGPP2\nTR/+ojHm/pe4128zxjw3xnzznVRuS3zVv0wZYwIR+asi8pdEpCci/52I/K6bv32LiPyoiHy3iByK\nyFMR+cs3f9sRkZ8RkT8qIn0R+byI/GN3XHyFsNZ+i4j8ryLyfdbahohkIvI9IvInRKQpIr8gIv+x\niLRF5DUR+SYR+ZdE5PffXOIPiMjvFJFvEJF/WES+8y7Lr8h3i8g/KSKPReTrReT3vd8cJL5TRH6z\niHxKRL5VRP5xEfmEiHRE5J8TkcHNeX/65vNvEJE3RORYRP7tV1cdReR6H6OIfJ+I/CZrbVNEvk1E\nntz8+Z+W6/7siMhfF5G/8D6X+mfken3uichPichfNcb4r6jYCmGt/b0i8kxEvuNmbf3pmz99k4h8\nrVz36Qfxr4vIvyAi3y4iLRH5l0VkyScYY75NRP5bEfld1tq/s53S3w1f9S9TIvJbRMQXkf/QWptb\na39GRP7ezd9+j4j8pLX271trU7l+cfqtxphHcj0gPmut/VlrbSEiPyYiZ3deeuWD+GvW2v/NWluJ\nSC7XD9c/aq2dWWufiMifFZHfe3Pud4vIf2StfWGtHYnIn/pYSvzVy49Za0+stUMR+Rty/dLzfnPw\ni/yotXZorV3JdR83ReRrRMRYaz9nrT01xhi5fln+wzfnzkTkT4rIP39ntfvqpRSRUEQ+ZYzxrbVP\nrLVv3/ztF6y1P2etLeX6B+37WZt+0Vr7M9baXET+nFwrCL/llZZc+SB+xFq7uJl7H8T3isgPW2s/\nb6/5v621A/r77xaRnxCRb7fW/p+vpLSvEH2ZEjkSkZfWWkufPaW/ffFYrLVzuf6Ve3zzt+f0Nysi\nayZO5SuC53S8IyKBUJ/eHB/fHB+953w+Vl49/GNkKSINef85+EV4Hv68XFs3/hMROTfG/IQxpiUi\nuyJSE5FfNMaMjTFjEfkfbz5XXiHW2rdE5AdE5EdE5MIY85dJqn1vn0fvI6tzP1dyvd4e/RrnKnfD\nR1kj74vI2+/z9x8QkZ+21v7yZkX6eNCXKZFTETm++eX6RR7c/P9Erjc0i4iIMaYu15Ley5vv3aO/\nGf638hUDvyRfybXl4iF99kCu+1PkPX0q15Nf+Xh5vzn4RbiPxVr7Y9babxSRT8u1rPdvyXXfr0Tk\n09bazs1/7RvJQnnFWGt/ylr72+S6L61cS64fldv5eLPP8Z5cjw/lbrAf8NlCrn+wiMitww//WHku\nIq+/z/V/t4h8pzHmBzYp5MeFvkyJ/O8iUojI9xtjPGPMd4nIP3rzt58Skd9vjPkGY0wo17LA/3Ej\nD/1NEfkNxpjvvPkl9YdE5ODui698WG6khJ8WkT9hjGkaYx7KtY7/xVg3Py0i/5ox5tgY0xGRH/yY\niqqA95uD/x+MMb/JGPObb/bSLEQkEZHyxpLxF0Xkzxtj9m7OPb7Zo6G8Qsx17Ldvuem/RK5fassv\n41LfaIz5rpv19gdEJBWRv7vFoirvz7lc7zX9tfhVubYs/lM38++H5Vre/SL/uYj8cWPMmzeOIl9v\njOnT309E5HfI9bP4D2678K+ar/qXKWttJiLfJSK/T0RGcr2n5mdv/vY/icgfE5H/Xq6tFq/LzR4L\na+2VXL9J//tyLTt8SkT+L7me4MpXLv+qXD9k35HrDek/JSI/efO3vygif0tEPiMivyQiPyfXL9pf\nzsKvbIH3m4O/Bi257seRXMuDAxH5Mzd/+0EReUtE/q4xZioif1tEPvlqSq4QoVzvP7ySa1lvT0R+\n6Mu4zl+T6/V5JNf7HL/rZv+Ucjf8qIj88I1E/s++94/W2omI/EG5fml6KdfrLG99+XNy/YP1b4nI\nVET+CxGJ33ONZ3L9QvWD5teZZ7xZ3yqkfLncmJ1fiMjv+fXmhaB8aYwxv1NEftxa+/ADT1YU5ZVh\njPkREXnDWvsvftxlUZQvxVe9ZWoTjDHfZozp3Jivf0hEjKjZ+dctxpjYGPPtN3LvsYj8OyLyP3zc\n5VIURVG+stGXqc34rXLtnXAlIt8hIt/5IV1Ela9MjIj8u3ItI/ySiHxONA6RoiiK8gGozKcoiqIo\nirIBaplSFEVRFEXZAH2ZUhRFURRF2YA7TeD6/d/xTbea4tRDrLVm+uj2uNZZ3B7P3ez2OBsVt8ex\ngzh7dgGZcickyfLgNlexLL3m7bFT4po9CmS/OMZ33VM0S+YFt8ehSdbqk7b2UKZLeIDOj+Htad/C\ndeP6HF/eRTmKMUJxLHO83yZnON+hOL/eDG2xoks6PVzTLBEgetlGRX/yZ/8eByf9svmTf+xbbyvm\n29s4bTIZDm+Po7COz0uk0HLjc5zzFO27eIx6eaP27XFwb3Z7XKTUTyXauTjH9QsKZFAduPjHM/pD\nOFmrTxjjWtGqe3t86U1vj9tNNHY+wlgoUoyvYYCx0zDYPvf1IdpotIPz7SnOyV+Dl7ddIGXVfNW6\nPS5DnPMf/Km/s5W+FBH5N/74z9827CREewcrtKvTRR2aC5QjwcfiumhXf4k/rDz0reTokzysl/Ag\npwAAIABJREFUbo+XEcbCvSXOeTa6uD0OW9QWFQLZ5/l6IPNaiDXCpTarVyjHMEMf1t3e7TEtF1Lk\nWI96Gf5QOZizM0vX8dD/kwJt1+1i7LltdFstw3z/N7/7m7bSnz/+E38T6+wYa5Qzf3Z7PN9BW1c5\n2se1CJW3ctHuNQ/1nQ5oLu9iTvgT1HGeIEJMFWBupQ76u1Oh7oGHsTKLaFETke4E5xW0jlzEmDvH\nFebp3EFftmgx8Ck+67mPcerRGK/5KF/RRru0aK2hJVpqHj43XTxzfuB7ttOXIiJ/+r/6/G1/pjWM\nr2mA9b52gvniUwHHFeqQZuiTvcPT2+Mhrd/ehOb+HJ/XutwnGDtZhfEyjzAPGivq22DdZuP6KIcz\nQ/nmDThNx5cIjJ/3Oyifg/m7fEnrQhf9ZiPM06aD7yYHKEc1Qn2WFcZ2M8Q4inKsNX/4ez79gf2p\nlilFURRFUZQNuFPLVLpPv/5PEUjV+rBUlCv8kgja+FVV28N7nzvCm6S7gzfJsIY3ZrfC26lr8Xla\nxxv8SvBL5XAxuj2e1HGvXo3eZnNOBybiRfhFk72GKPn1BC+xwRv49fQyQ7DXYIE6N2uoT300vj0e\n9PFru6RQoFkd1p8MRhTZy/DLaL6HX15df2s/km4J95FpZTl96/Y4zmCNWjmwIu0vYWmpLFkKu+ib\n2hTD0a3hu5MZKukV1H8F+iY6wK+i2oTaP8Ovt2kf9w3KdSvjaBd9GS/w66c/x5itLXCtqoG+zPv4\nVdRYkcUqwvhaxrj+3KI+PlljyMAhez5+seVoUgkn1OFbZGbeuT2uVmRRol9q9kWfPofFJgxwPJ7i\nl2ArRnstlujbhOoTpxjLvRnmwVkdx16G71rBfFqcoZyuRTlFRMo6fnkXCf2S3kX5bIZfnhVZrFZX\nGKvTkO4dYI3IXPyyPZvu3B43qBO9I1yzdonrvFzgvoclWU63xPlL1DeJsJ4kLZQzylF+tvY6wdXt\n8aqidWOE8gddWLrPX5CVrYW+8RzUMbdk+clRnrKB8XE+ucQ5K6z7IiIDspQ6l2iv+j7aepKibmWF\ncpz1ULfmEPcI0ZUye4i2WLkYH405xuAzF2tNv8K4y2lt7S5hEdsmFwbljuk5sEPhMBdtsoug+8Up\n8Y+I2nF+gnm372KteeqhPvbq/0EZAlqnaLy0Uswzr8TzZ9RCnzdG605ubgPXomEo7eIXcL8Mbe9M\nkc6vMUW2r2Af7VLlOD85R/8va6gPPY6kXWBt7tQw32WA949Ter5/GNQypSiKoiiKsgH6MqUoiqIo\nirIBdyrzNXOY32wI+SSxMK2GTRTJlDBF2grmvawNW2fPYKOyxDCzzhIkld8vsLGtlcC0PGvDNGzO\nIdW093FOHMHk7LvrZj8vQh0cB3bjnEyUzwraIP8EJvQL2ryejmE29+nz5Rgm7b02No9mOdqrV6G9\nTjuwmTauYAIv7fbNz9Up3sOb4aPb47lDm1lpo/Gyh/JkJWShbgoppO7DDH2F5ONiDeq1T/1xtiIp\ncAETc4uSv0yew5wbUxmSgnYZi0i3QP+PAuhQaQhZouZirLUclFVIxmi00Pe5izJNU1yzKuB84R9D\ntnMKzI+shvoX5HzhdWgj9xaJV+ireR3jPx9S6qwEYzB1qH/eRbvGLjaONnZxTjJFu/hCm4WX+Jzv\n2x1CCihrmDfTJdq0ZjCOJjF5k4iIISkiCOG9EdIm6ckezdkV2vWSzP4PTlD/kYsxIg2M89Xgye1x\nnTZYp7QOLJoYh7sJyprE6+NwGzyr0RYCwZg1I6ynKx/t0ImwnpzNIM81SL5NK2wbmJKianuYB5MI\nkoq3Qh09krj9FGu3ucTYjzyS0L11yXbkYGxGXbRpUOA7jot5uqC4yfO3yQHoAdb7aQ1riplj/AYG\n9ZwZbGUYGdQn5j0XFeZ1ufwclfo3yLbwx1hrM5Ldp4L2s+Mnt8erJvqzc4H5MqS1+TLFNRce5kFI\nz6srD7J+/wXuW5AjVd+ib1Je+x2UbZVgPIqI9A3NwWWTzsO9hwNIeI8PUZ/V/DO3x57BGmHqmOO5\ni/o0Atq+McSzckmvPuEI/fmSxnDTfLS1Vi1TiqIoiqIoG6AvU4qiKIqiKBtwpzJfXMH0l3rklRHD\nS86fw6QvTZjoyjZMt70RTOblkjxOLMxyr3Uf3B4HsG7LLILZsxGTF9abaIquhaR45eK+tZTcJESk\nqnCt0kMdWm2YnCOSD58fwDvvtRTXPaFYJVULps4DMnuOKkh1SUDy3xx1fuTAZDpowhweNkmq2RI9\n8sgi6+mazJFlMPt6FNcpLtG+JzV4g7RJqktdknDIE8xwXCKqexmhrZZjFG53D5La8hnaYX64/jui\nOCPvsSbiF9mK4peU5OlSkGTYxbiekDTduKL4YfuQT6J98ni8IsmghT4uzlDWXo3M8OH2ZSERkcpC\nnitInpICdXbJ8zCbUVy1JcpaRog7k76ENBJa9L9bhw57SB6M8wKS0TIjGYri48iK4hfRGGxU62O8\nRvF4vC7G4eIc3+9cQM4rmxhLRy8gK8xJooh99ijGdXZdHM8SlKNBce9OB7hOrYO1qRutxzvbBrFB\nvUYkhfVa5EWbol+fJWj3iOIvpRT7KWugPasJxUOzGNfhU/JAraH/nBxtOyV5eBFhK0ZUwhu6uEL8\nIBERjzzDjKCsl+TBFpM85Ze4tzVYU14MsS43m1h/6+R5yAreyENbtOl5lc5x7Dap/6av5nGaGpKt\nCpL/c3zudjBmd5+hXc7vof9dsp30hvh8QjHTGgXGTiPDmlB38LmhNWHsoE2rGWTUhcXcb81JHheR\nZIJGnri4bpM88nboGfp8gXVnl6Ta5QoLQM1Fn8QF1tFyhedIQp6KHQupctglT/AXGAvio/4in5IP\nQi1TiqIoiqIoG6AvU4qiKIqiKBtwpzKf14J5e3UFE1qcwyQ4b5L3W5dCx59CPsgpgFi8CzPh0SFM\n7AuSFJsxTMOuj2s2PNyrJBmt58OUWJUwpZoGTMMiIvM6zIatOeQdQ+lhFjVcNx/BJJxTQMM+mZZz\nCsPv1GGiXk3IE5K8nhJKg+KS18u9EGV4mm1fGsp6ZPYdox1zF1Jms0Mpc0b7t8d+jv6oU/oGQ9JM\n1sDn3QGleKhgnq2Tt1ivgFxos8Pb48jCI2VO6SEaM2p0EYmOIA2sLvA334f3YNRAnYMFxpfJIUu0\nDOqZtSiNBkmzwZy8EMnTafdd9HH5GtqoLHCdIHk1U9adYKyZLgWSTGACrwKY3n0KtmfJGyot3sU5\nM8zrUQsm+b3PoR2zBurmU4qlxKI/xktK/UHpeiJKOVMI5VsSkdwj71HyPN0tUfALShFih5RyZgX5\nKUnRLhNKg+TmmFMmwnh5cIQynU7Rn/0G1i9ed3JvXQLZBpMlrl/MaasEeSCmCXlFdlDOCaUxqgt5\nkaa4Tp7g/NhBG64M7puQZ6qX0/ggWTD20Weph/Wzf0j5iUQkGWDrx4jknPYI62/UpoCRDtr6YEYB\nGX2Uw9CcuiTJ/pBS3zyw6NczatO6YGx6Qmt0e/t9KSLSnWIeXTkULJfGTjzEGF+6WPs7CdV/ju0x\nV3TOw8kVfU7rAAUObkzhFTrukCdcgnNKi7LVHFzHWa4H7ZwdkrT3Wfxt/gDHIY23HgVCliltzdmD\nx59Q6qqsjrnZzfCuMN7Dd12a70uSEVMH20J2SP79MKhlSlEURVEUZQP0ZUpRFEVRFGUD7lTmm9HO\nf3Ep23YIyaTB3mDnMAfWSM6bU+AyfwqT3tW7eDdsPyATYAVz3T7lS8s9Mtc6FPyzgmzj1yFPBPP1\nHHfOFeUJq5FJnMzdIcmNu3UKpEl5+sZLmEdL8vIbNXDNvRB1eLaCHHK/hfZ6Z0SeZztol/2Y3J62\nxDSHZBstYRqd1SHJXJBHVnGfzOTnqFdzBZN84cJDbHcEM2wmkF2dGObpiYOcTT7lhQp8SLPvknfd\nLrl/rfL1gGwlBRM8IC/BQY2CfrZQpmYHY/BqhL70ApJyezh/nzzeJKCAlxX6Mv0UyVanuGajiXPG\nzVcjJZxHGIMVech6Jeasm+DeqznaqxVDOtuhnJjsbXMckPS0g3ng99DWCQWX7ZAUPG3g83ifpNYT\nzNmU1xYRadLY69Zx3rykPJgT9FVICv6ki7bvjbAWnJG3sFt9/vbYhsgz6o/RLpGlviooAGYG2Wb2\ndPv92aB4sgUFoz2j7QFtah9DHoi1NtqnTNFPThPnxzEF+RxRgNAFyfQRxvjcI2m+gfZZDDE+OIeg\n28SaICLi4HbSv0I7Bg4F8EXVJJhC8l80KQ9mjnuPC6wXTR/z7pLytbbmqHPo474FyWv5lAK/plSI\nLTInT8UWBW/2FggqerJAuXsxxvWSnOMDF+t0kJK83kNf1QeUb5bOWbYpl+MZBfglaT66oLEToN3b\nHXoWi0hMeR7nTbTrzkvMr1GbthpQcOZkD+Nt+BTnt/ZR/9mcvHQPyPvxXXhplx3KM5qgbjEFix7P\nPtpzUy1TiqIoiqIoG6AvU4qiKIqiKBtwpzLfrsW7m1eD7XZgYOKNPQ6IBhPgYood96+vYPa/dCAN\nVZTzbWcOGcosYK7NSfKqjUja81GG8xbMh7tjkjai9XfPskPBOYeQopIO7OyWcru578IMOqMcUE4N\n5vTidZSvNSbvJo8+pyBuJ3TffgeS1oBki/35et6ybbDXgBSUkmdMWqCOxSWksJWhfhW0aed1mHCd\nKcZEmqO/wysy88aQXVyDNnHqJB/MYFbukseW00H/lUvychER9wpltZTnKdpFXx5RYMe3XJS1tcI5\n6R7K1HsKk/HJMTqkQWOqPSaPkQxtVFLusGmBe0XNV/P7Z8/Au3bcQNsHlPuyvoB8Uu6i3ONzzK/e\nlDwbM7R9UYdZvTnDOF1WuL5PMmxAksl9Ct5rzkhGjNEWR5N1uWzSxPjMKGhgWFLQ1wV5bdL4aS/Q\nFqWHtmjO0YfOEH2V5fBgPCHvtmYDMpQlz7gV5blrUw6zbTEl2aJOa9beGOvj3IPMFa0gR0c0Hu0B\nrjMhr8CMPItbOxjjMwoQWotpC4VFv/LcD7u4fkRez5lgfRMR6VL+0XkMb9miScEcZ5i/z2uYv4cX\nuEe1S4EtXczZIXl+BguU76XB8X6I9kqmaC+/hnFW819NQN00wzOkMUf/JBHmUUxSahZS7toh5K/C\nRZ8kFCB3Qeuu08Icd6ndkxHW4FmEOR6+Rd7OFBzXz7EGz+z69pi8RL+1aUvMiw7O61N+xUGAOuzH\n+G7tAGPSkrZNTuTiDrDOh0e4ziRDe7UnqH9eUU5Jl1yWPwRqmVIURVEURdkAfZlSFEVRFEXZgDuV\n+aoYZryFA1O6DWGuKxySAk9RvMYBzJtVSnm+DIJEehnMyZ+j3EufoMCAS4fyHIUwey8Nzi9SmN6H\nETwgshXMhCIiQoHJugVJFwuUtbeEef9ZCFOkT94XTgemxfhtXPPCJ68iCtbGTkKdfZIPYkgmsaA+\nVUGeEVtiVuL6IQUpFUO5zPbIS4jy7hUzMiWTmfcsRX8fLGG2N4/Qr/YUJuxygXrV+vhuQMHzyhJj\nKx2Tudys/44IkVpRxhVMw/sW5T4nz5g2jZc8Q18ux2TS7qMOQUaST4cCKSaojzWUB48Ctu5mFGxw\nTAXdIldd9NUOeYWKxfiXAXnxCMbszhTtVQh57YUYm/WSpLNLjNNehbbwc8oXRrJbvUL7Vg7OaZKE\ntTukMSgiY5L2PAq2OqUAgB2f+oq8KvcnKNNkiXKPItS/UZEHGEnebfLwDbtoI99AMkgot5njrcvN\n2yAjb8zAobyEHtoxsCTbVDi/3EGfdZ5hbc3fJBmcpMwFS/yU16+Tk0R0RDk6r3BNiaitHJyfTtZz\noIqQdyl5Cxck4bsk+d2rsFY6u/jcJPh8REGBsxJbJaIIfdagAKHFlNorgFx2GGFtqk6335ciIrZC\nHUYnuEfagzzVq9G6OMb4fR6SByfFKa5RPtj5HJKct+KxiXpeNOnZMsjoHIyLS9rGEw3Q57U31uXP\ngLZtnAr6pEN5c+cjzOf75JnOO1Y8Cjab5TQ3KSKA60KS9Ea0BSPEXBjX8TxqJmhTm6L+Hwa1TCmK\noiiKomyAvkwpiqIoiqJswJ3KfNMMpj+XgoP1dvBOR2nexKNccz7ibcmiBQ+SyqecePdguuxRFLer\nBe5btSnHVEZeNW9SwLEJTPKrJcyBlgK9iYjEPdzvBQWEo3RAktTJq+WczJ0hzLVx9cnb44s2vBN9\ng3POHZjfA5dyE5I0NMgoV9UcXbsKth8YsF6gDDOSW7KcPIMyksIs+mMQo62KKfqj04ANNyeZIH0J\naaZeka2aAlj2KNigTGHOHTloQ+tSrsBy3SQfV5TnL8YgXJLnZLOLYIJXM5inI8rROKHAkw2LNirJ\nk+ZohXFz0ob0m1Ogu6+hfIQDF2Mw7bwaKSG+QhsPyEzuvMT9hglk0uOcAqBGFAB1jHqmFEi1M0N/\n1inn5n2a44sAYyeeoU1DkhImQ1zHLpGDcUlBFUVEXEuRK1PKHyYw4x9ToM+pxT36E/JQi0mOn8Fr\nL6c52Cf5PyOpfZaQVxpJUrtfhzG/XK7n+9wG3grr2oryjLo5BZtskefdDuqyGFAQ1XtoU2+J6+QR\n1sdmjv7ukyx21iDJ3sfc9CjXo8QYHx55xR1P1/NmTj2sLzl5/fUW+E5SYmuFqZN3bYRytM9IzgxR\nn9c85GIsJ5Dj0xnaaObiOiVt3ZhV9Nw4hFy0TYbnv3J7fNjFmO+20c9lgnGXU37FvZw8mI/hCZkv\n0UaBg/EoBa01tNUiIm+5Bnk5Vk1cc6eFsTAg+Tea0f4IEZknFFB3gvaLKfD1Tg9lOqTnxdMY9653\n6EFLdQ4S2kYR4V7TFPcKw7duj/3nWBMuSgSCrqKPZmtSy5SiKIqiKMoG6MuUoiiKoijKBtypzFeb\nkufHI5jer85gom348CzIMxRvsEfBukhHa5KZVchkWCS4viGLfxLAHBj5MB8On5FsQTmsmiEkn9W6\nyicvTnGPvQByY0U5p2YzMmULPINOyXvFq2DerDJIDIMZ5fijHGYReYZdRmivWobzTQBPQPaw2hYs\n4SxzSJPN9HWUwaFgeD68/ExG311CCpmQ15oToz07MTyAfPJC6luYcIdUx+ghrl8fwxst7uNzl/KR\niYis2ujLgDz1+gnM3k+m1NYuBZis47tHJBfJAuO0SHGdhYMB2Tf4/DIjz08P7eIL5kc9eTX5v1q0\nEiQzlG9Wol07IaSE5T4kk9YE/RYF6KtkRN6Tezh/30c/XJKUHceQwesx2rFc4b6tGsoTzCk4ZWdd\n5qv7JFfQvKgbrBEPKXjgW1cUDJTycTY9jJ9jAzlgQfJWJZA6UgoGOVmir3bIW7J6gfZaeO9ZVLZA\n0kN7TS3WnL0mBc9MSf6hXJS9+yhbPqTAtF0KdnqG9oz30H8ueVS1SDoMhpBdkgMKKEmecBl5dZb7\nNIdEJC8o8CIFknQceGpFAdZEE6IvfdrukTbQLl2LOftyhDXXpQDKTkU58SzlXGyT7Ehe3ME5JMht\ncuCT1x55Zl+cos08l7z5MvTVAbm/jRPMQU4zWysoDyptJ2mHlJvQI5mzg7lVG9LD1aIP/H0aX/G6\nN59H2z+GPRon5Nk6z/GdC/J4PibPzipBmVoB+nkVY+1Mqe2SEfpqMcM1d2uUi5fWmunzdXnyg1DL\nlKIoiqIoygboy5SiKIqiKMoG3KnMtxKY1rokwyUke8gQZtZhC6a+WgaTW3MFT4xFTpLJIwqyRV5u\nLwPcN7QUxDFn10GYw7vlfZT5Ba6zMu/JcTeBlHbZQ1Pesyh3NaX6FJBrPLKzLmqUY2gOD42IpEMz\ng2m1QUEIZ+RZ4jkoz9JFGRreugSyDXyS2BoC8/ZgSfnbSpikvUOUzY/QZ8aBSb9Gr/ZJADmmSnGO\nRznO/BBtngdo5+oS7dNqwEPulPIx9SinmIhIoyKP0ooCypIce+8YOb8un5MZm4KxBr+COs/2McZ7\nlLfsMsB3GzWSgStIZGFAudMoiGQ1334uNxGRVUJ9Rfno6hS4rxuj/eYJxvKeh3ZJyXuo2cc17ZTm\nUYyxcM9FPUnBFVPA3F4uyVOWUhkuDWRXuSKPJBHxcvRv/wD9kE9wk3EDc2qHvCpPoU5KQzD2Zhbl\n7gj687wGWWWHxlFDHt4eT32SySLU52FKa9+WyAPIZ4cL1DcnD8e4j7rXL9A+wzblR6N5VDVQr9ou\neWb2MD7GKep4SB6Ubh99c2DRr1fm2e3xLo3rQX89OHJ0SR6PLgWDzCl4qIdOq8hrK21ijNgFPOEa\n+6hbM0UbeT7a7qz8Au4b4pruBa4TkKe4bWC93ibLOgXt9XG/8QLPo4MY490v0D+TXXgg2wXGcvsQ\n11kajOX5kD1Ncc5sjjL0hPL91cnbtYk+cCjQ6Gq0LpdVBW0dWGJCDy2u1a4/xxdcyhdZ4bm2cjHe\nzADjcETev3WSBRs010jxlCF5ajop5mbaWs8p+EGoZUpRFEVRFGUD9GVKURRFURRlA+5U5vM7MNed\nzshjZk7B4XyYAH0K9pYvyPzIQf+aMLmOMpj3lnNcx6UcOwWZDH3K8zX04ZFTxnjHrO/gu+b5upQw\nSSFjRDWYnC8tSW8kKzohJDDngsyVGcyjuz5Mt6kL74gsgvw3pyBr3Qht9Hky1+5bmLer9+YU3AL1\nEvc9p1xdk32U4diDabhc4PPlFObjOnl6TByY8+sDDM1sF+38ZIG22qccfG6H8jj2cU1DHpTN1aPb\n41W+HhiwVnFwT0gOERQsGZ2TPEm51rxz9NPZAcbdoiSP0hHGUb6DdhkX8DyplTArz3zcOBf0fb2P\ncbpNljnKsY+pKYNHmFMXT9DeeyHK5JCUEJNWl5+iXWSIcW33cM05ed31YpKGEkhqEwow6Rb4bjNB\neYoFpHkREYe+c07riyXvIfcM/TwluSbx8N2KAqyGJHNWlO/Tz9C3baH8hS7G86ccfHcomDuJQwNs\nS5gG1odTB1Kam6OfoiWtp8dYr5oTCqL6mOayCwnrLEcbTp9izNa7mINdB+tsOoS31LCPa3Jg4YWP\nvrBnCKIpIvKJJubzJQXSTO+RxDbkHHyY24bWdXeP8gVeYb4nAfrSCbBe5LSdxC7I+7eJ9loEWI8m\n9GzZJvkQfbgK4J22b1G30yb6wScJ+sChZ84S9T/9DHkv76Hta5SnsSCvOBlDjl+5lN8247yOlIOR\nPPOcs/UtFeLRM36XAupSkNCrJZ7BfXqGziiIcvgOBaY+oCDKNHzSBrzI0xz9ZsiLOiIvvyl5qUcJ\nBSP+EKhlSlEURVEUZQP0ZUpRFEVRFGUD7lbmI1ecVhO2uCXJUK0M3h6Wduv3u5T/ysAs93KO98Hl\nmDwLfHgQ1FIyE5KUYj2YSasarpmQB0zNw3dfOjCxiogs9nGP3XOYNU8jnLdHOafGA3i9Lerk+UD5\nv05nMMXv1mBylxrljEopt9sU9z0MKVcVyVCr+vaDyT2vYA7dK9AObkUSDqXhOif5Z5/67yKl4KUN\nmKcXD1Bmb4agoDuHuK8JObgmjv0WvOLEfeP2sEn525JwPTBgQMHawgXG3cu3yYsyoDFFMueLmPLl\nLcnrZ4JrujvULkv0cUV5pLokVcqQvD0pSK1fg1l8mxw8pnyP5zDLF118vmMg1+TPIcM1+uRtQwEg\n53P0c7GP/ilSkiQod+VOBlnleQUZsYoxFmbk7Rqu0I5lZz3/ZEz5vAx5tk7IIbfqonw2wbhKKEHo\niw7m2lFBEjx5JwYkN80oaGvgUb40D3O5ou0Fe33UZ1usPoN+Ckl23rG4V0ZBNXsztPuKAu3WyeN4\nGmBMHNbR1ivKpxiN0FajNslL8T2UZ/zk9rgj7I2LuZXLeiDTlyvKS2pwP0oXJ79ag/wTDWn7RRNe\nYc4ZysE52+oX5An4HOtm7THGjfcc1+QtJAMKsrz7ahxtpXiKdh3vYR45Icbj3hzrlAlQ1uk5jWvK\nqdimLRUBhotM2+iTirZjmCbqWZbH+LwPCbJc0Hwkb9q8S+6xIjJ6Rt7WHYy3bgNjMj+n7RIUkNQp\nMBbcFMfxJQVtdukdok9S3SkF1J1gDmYUkLV2gfK8FHqAfQjUMqUoiqIoirIB+jKlKIqiKIqyAXcq\n86VdmM1qMUyCJoUZsNlAkS4XML+NKSdRTIEUyxWZ6wTmwxoFA1zuUsA5g3sNBzAHmkuYdBeCc7qP\n8HmxJDlHRMIlzJIzUhlqFDv0rRym1cjBtQLy1PMpKGVIAf1M+8HtsTcjU3QAE23kQlZ6SSbQg4Ck\noRKm2G1R78Cs+vQ5STgtmJunTQqMN4VXRWJJ8lih7s+vIMM9pKBygaDdxiGkmb0QpuQT8gI9olhr\nbhdli2eQo5zpekC2RUieHuRt0ryPsZY+p8CTEdq3pPxXM8rftiKT+ekFxmO4h8HSsZAkn41IUovR\nFkUf15xXNLi2SMMlM34DksnhBG0/Izk+fAyTuU+/yUYUeDSjXGieQznZGrhmRh6oQwr42t6BtHdZ\nkfcnBXe8cCFbHU3WAya+NJCuIvKqKxvoW9ohIPMO2tVabEGwc3yekielS+fnJFVzENZQMOblEO1y\nNMI4ypfr8uRWeIS6d8kranwf46sxR5s8qZHsSFsr8inOaVMgyHYTslO2QjvETfRTt411aX4Fz+KV\nhavoBbUhpcST2nvkMjvGvTsNjKPlAPPuHm99iNC+3SnW0DGtKekYbdE+wHqUUBzY5IIC8BbkvUvP\nn0UD9Xw0/2jeXx+WRYZtDqlDEpYhj/CS1oUFPg9pDY4pkGY+o60v+2jTxoS868grNCgp8LNP2zGu\nKJAtydcVD+uMtl2ISKfzNsphsO5kC4zbMKJ8gSRP1wWDY+KiT/pjlG/QxrjtTsjDf4XP8xRlKgzq\nOSDv6lX50fKgqmVKURRFURRlA/RlSlEURVEUZQPuVOZjk3lCptLII5PwBeyDAUlVLpkCAm1EAAAg\nAElEQVT0Mwem9CGZfbsU3GtZkgfIKc65qsOUOO9DOttZwvw8CWG6HJPldtZeN/ulI5TbE3x/+RI2\n6/Ax6nPe5eB+kEk8yvnlU05BbwUPl3kX1y/Iy8KtUYDQAPda1HDN5mz70tBkinZ0yK3Gobx2nbfQ\nDvku5WBy0O4pJUnqL/BuX1Uw4eYVBfAck4y0CxNuw4G5eGbQ35RyTZw98gRsUiI4ESlJ3ijPUY6U\nPLuCzqPb4xcUXPX0H0CSalXwdJmFlE/SUq5BkgP8jALg1WDCz0gOKYeQv/Im8r1tk9crzJ23yVtr\nSjnWMpI2/Sscf47kgCMXJvwsoCC3DupzPof04JaUF62O4+SMgqqSZ96CcmdVBcr5wkXwXhERKSGx\n+QvKMbegsdQmqW7J4w2SUe0TKHdCAQp9ygW2PML16w0OyIkxGVCQSHMMif/A3b40tONSO5Ls2Jpg\nrPUrjNlJhfOzBGPfp/0KzQjtNuAcbw8xwcZfgASTGUhkDZKmFjG2HAQB2iGkwMrTwbpk6/oYR2e0\nXtRXkPCyXaw1RxkFcw1Qt3ZGjzvKX+gOqe/pvr0c11xMUR8nRHn2+/zd7QdHFhFZ+LS14RT92bcY\nU8MjrKlxiLZ06blkyAN72Ka8fgU+X1Bw5T0KTlpUkLiTCu3iCtrXoQDSNoWkVmute/PZgLZtGPRV\n/V3K5dlH3XwOJDpEW4QLrK9BF+O5Qe8K5wbj2dB4a9WxjsxeYDz3ycv+cqpBOxVFURRFUe4MfZlS\nFEVRFEXZgDuV+coCJsE4h+nuCZlZO12Y6/05oonVyDtnRa5zewJTXFVHdYIxDLZ5g4IKRjDjfaKE\nZDJ1KWcUeYzNpjBJ1pvrUkLRw3d6JN2Ux2RCdCBLfeIS5m55BJNmNsJ3U/KsWFyS6bJO3mcFyuTE\nkNXKFCbdBnmVnfukdW0J9opMyDhuZjDvXvgkZw3xuUd5Fm0Ek/FZBx4Z9SEkg/gQZut7Fdq8ViPv\nvADnt/ZQtrxEO6R1mI69+Xrwy2RCub120NaZj7HGIm9+TkHyWpCFBuQ9klJOqrxH0ssMZnVL3nk+\nBVoNKK+fdSHZxuV6sNFtcbWL/mmRN9CqCWnIewoz/mQP5Wut0F5nU7RXtwOzekKB8QYJvhvPMH4X\nE7Rp1cf8KE9Qzhks+1JFkCoWBblhiUhwhjFWhRRIlcZka4HPlwXOj8gD1CePXZ8CmI56mGu7Jeow\no3ydDZ+8ENtYBwLyLhTz0TyGPgwpBZT1yEvVzUhqDXDfkGTQOEBfOm2M3yVtObC0tuRvYQ6GlJqO\ns9TNyTtSFujjYIQyzEnKL5rrQYa9kFzDLjCOwjbmeUC546Icc+2yjvrvGsy1gII2vn0f884Z4jr5\nBN8dUm5CO4SUH5H36opyWm6TLEF72B7aabqH8h37CEhalChIVNJ67KCfPxGgH06a6K1DjpdKuVXj\nXdy3ucJcySkXZYO87oxLbeTTc09EVhHW+fAcc2e0h/KFJO3WfZKYZ3iWtfcf4fySvFBpV8sOvXO4\nSxpvFJxWKKCuG9M8PafF5kOglilFURRFUZQN0JcpRVEURVGUDbhTmW/epTxsFqa+oEfBL4cw4/Vr\nMG8+J3mjRTnc2uaTt8eDCOfsdeB9kE5xzX3ycitWFKzMg4SXkJmwQ15VT4v1nFF7RyQhGZgrvfL1\n2+Oe9+7t8dCD2dyLUKY9CuLIhsX2azDjXuTkAVTHvcYUfCysUJ+np2iLKFwPULkNTIH38Iy8gSYG\n7R6NUJvLmKSHJjytgiUkMsdHW7+I0VZvDGH+HT/EWPHIuyoIYIZvcBlCXD8dkPiQrvdltYOxGQpM\nzHtL9NMZBVhsDSCF5RHMxC+nqPPLDGPqk5ck3x5h7JdnqLNjWf7DcY1zELYgd2+TWoj6rzj4bUKe\nWBRMsVWiXbI2+mEvxZwYd9AW1TvoN9uBbjc5x/VHAcbF9JdxvMww1pKYvGDnkCDT92gspUOeVRe/\ncnu4+xByiEv9c0WBCPcrXCslmTd0sO2AhrNU5NkXUH4x7w3IR36Bc8oK9em0t59r0SzpXrSFYton\nz9SUAtPWUeasjrEZkRTudEj+W6EdzAzX8UMamxHazcxpvab5WJC8skuyoynIk1NEZnPOL4e+yTiQ\ncY71znRx/i5tJ8lLrBFDynsaXVFwzgnG1+wprm9rVKZ9CjZKQZlnzquZm9Ul5T6l/J0h5bENMpSv\noHyyiwnqdtTDBF6Qp2JnjLk2apN8vcIabEJcp0le83NeK703b48nbep/WZeyGxfkndlCX33Sp6DI\nFIx7FiHAt3+M53drgbHqUN7MOq2jB+R5msTkjVtQpACSFOcl6tmuUaDSD4FaphRFURRFUTZAX6YU\nRVEURVE2QF+mFEVRFEVRNuBO90zF5IJbUkTkzgI6Zb6DzQiLhKL3hpREs08JiktEh+6W2AMRkAt0\n8AA6/Yhc4HcpcnNJ0ZcP3TdujxMKxnsk60lJGy4SUC770HVTSv7ZEuz7yimqt7gUroHyJ9+nhLNz\nSuQpJdyLlxGFHBihC+e72MfTGNN+An/77tcT2seSLtB2VYg2WXUo0S21XZs2nMwLHJslJTSlfR/m\nGPq+R3srZhSF++gQ7R+uyCWd3POLA+zVCf33ZFO9oBAFAfog2cM94i+gP55X6LSIksbKCveLKSTF\nxMf1dyePbo/TPgZYRS7DzZL2ZAUcVoGiim+RRoR2rdP+rrGPubmzh3YZnGCvnqGIyJN92gM3wzWL\nLvYw5cuj2+OXHUp+PqQ9gl3UM7lA1OzVGNd0qJ9C2gMiIjIKsa+n72IODhfk4h5QRPwdSmhMYUvc\nOfZZuOTu7dbQtyVF+67T5845rr9DicFnPezjcFpoi20RxmhrnxJmBynmbFhSdgWLtag/Qvk7DexP\nySlq9XJKkdFb6APHwzxw56hX2kMZOhO0Z5mhLwchyvY1D9Yjic+H+PdBhWutBOV7SMm5rzxc94qS\n0z84wdxMKTHw0qPo9CHGURSjvxOam66Ptih5H87BR9tj82FpH1CIHMqQ0cqxNiUO5t0VhVtpt1Cf\nNMEa3Ewxv4aCPrxnaV9vgxIGj/H5mY/7Nmk/W7CD+Xs/x1p+ka/3Z62NPVpHV2hLMRiHgzrm6RuU\nZPnpDPeYvcTxahfjx6WA63mLwmyUqHM6xUlNatMRvWeE5qOttWqZUhRFURRF2QB9mVIURVEURdmA\nO5X5kgGklfo+THenAYrxegaT+ZySmno1yIK2hJlxSrJY/+L57XH5GCba0oeZ8VFMrrkpJVUu8Xne\ngBwXUtLX58P1COjHBmbGuA25amxgoiwoMmtIkYMbZCqNjmFaX5EbsV/ARNmnSMzTCYUWaJEkMYJ8\nYJv4fORtPzTvLplJR2NKUNyD/HO/h3qNFij/YoHQBSaGLBjFiKDru3Azjoa4ZurCbN17E32fljA3\nTwwnqIQJ356gDKuUzMsikjUQQTs9I9luD+P0so/vVGeQnp6cQGLw93BOm1zg/Tk+L/ZRvk6CugmF\nhhiTqb5pULe6T2XbIlUT43dFCaRnS4xZ2yT3c8q3bAeQdOIUbRHG5HJObs+JgzF+dIA14ZyiEnvv\nfhbXP6KI+UPcKzyASX5yjrEjIlLN0ZaDLq67f4S1ZkwJgR82MSZ7Gdrba6IfJgFCkhwOMZYum1hf\n3AL9U3/4Gs7JIdPvUH8W2XoYgG0wpL5sJCjPgkIRGEo+3Kco4Z0rrIOLLiXqdim7goOxbCkURN2B\nTFMIrcVD3HfSwziotdEO987xeThY/43/Rh/9fP4Sbb3TwfoyJLmpeYL6t2jJviLZqu1QwlyK4E/R\nT8RrYU14UMM2gncpUXtVYZ0KzzFXtskORXRfkNzKbvzVHFsQ3Iik9jmkTVunpNQr2kLSxhxMZ+i3\njOTinNb4nosxG+1SYu8FnpuWtrE08/UMHC/o+RiMMbcDSuhM+ZNl5KLOEw/zt1VD2KHlM6yj+R5J\n9iPUM22hTOYCn5s66rYjmOORqxHQFUVRFEVR7gx9mVIURVEURdmAu5X5hvCmcuqQsHpL8qp7Dfa9\nsID51fMh13SbMFG3FzgeHOC7xyPyEntMsk8Baaio4xxLUdKDFKbb1FBU9XvrSWZd8hIKyMuknsHk\n6FDU8yqDebjow4Q6rmBmDcjOXMQwS8cRSUYVSX4Zrl9vw7yZXFCi2JgyxW6JQRNm9cUMHpUtKkNK\nkWgdjpTbgin54hLnx+Rh5xjyKAtR3/199MFigmvGJBVXFNHWUCJZh0zH4pFnpYhUU/L+NDDpvziD\nqb+4oAjlNBRiQeTf4QoSbKsOLaxdo+TULsb1fgP1Pw0gVXWnOIdybcsqhAfTNsnP0R4pJXjtxpAz\nRxOM66MVpLO5Q7JzgLZ/1oTskU1x/S6NhbMDyII7DYxZKR7dHs4CjIW9fUpg/BjtbubriY675AHU\nqmDen5EM+ZjluUOsIzFtBbikRNyWPKAuKMF6P4Xn4GQPa0LtKQZJ9RDf9Rwaq+H2PcDsAGtl5qPP\nGjHKs0vJkH2KJL3oQ45sjDDw0jaumVLk+fuXmCsDF+tMQpHRm/sYs6tLyCvOCeZH0SU5Xda9v4pL\nfN/zP497TCD/VSXGV+WRZP0FtLu/g/LNKRvulBLYH1ygXyfkNV1RWowdenatCtqWgWGzVZYxnket\ngvp2hv7MfdS5Pn90e3x5BSmss0uJ1wXXbJA3dl6n9SVDm44XlFS8jnavPUF5mvfQby+fUuLw98hl\nCW3DuEpwXjGgZz+t/5/dxXiLKLPHE/KQXi2f3B7759SHOWVPGFO52+i3mov5uKRn60i9+RRFURRF\nUe4OfZlSFEVRFEXZgDuV+dxdmM3CnGyiIeST5RLSlk9eBvEQJrp8RpJXF3JIYw5ZwW/CjHdh4AHU\n7cJMWMspEN0Y3jwZBVuMXJLdxuvN5XRgZjxc4Pg8hqlQapTsNIBnRTWFGTMkyWy4i/vdKyH7PElg\nQq0ilKOgc0LyHFxYeEZkv7L9oJ3pEn1wGMHs/fIK/XevAbNyK4QUlE0hqVTHGAflHJJK7KGfghx1\nLF6iDX0fn9c8mPDL+NHt8XkBmePQIc+++noA1heXaKPHHqSO1QgyxoJkammg/saFRLFXYRzt7eAe\nqwRm+LiCGXoYoF/LAv06akISzsjD80xQtm3itylhbY65tqQy+YK+rXJKFH0IeS47R7vUSBb72oeQ\nFV7+KgUeFHhz2h7ulXwCffs6JbfdpePoESXiPV/3tF3GFDDU4DvtJxQsuE+SBklDjkU53gwwl08d\n8vLN0RYdkoMcWi+8Cp5BLR/tVQnWqahgz9PtEMeQNUMONEz94VMi4iE5Ue1SNmtSmiXmuUMJ38c+\n5NUFBWestdFW8xVkmqZDUr6P7QHnGfql56xvp1guITF1yM1rWOAeMWlsLy0Fs8whU9dpGaxc9H2w\ngAx1RlsuWi30TV7SHoEr1H8W4FkRl+tea9uiSx6v4Qp9OIvQTmlJCZ1JejyntaagoMhphDqXFxgv\nboW2Hu1QUFuS/JozyNo795A4/Jy2XXQM5MXZYr1dnpMXbZBhTS3ka2+PnQxzu32J/kwdrC+rU4y3\n0QL3e7RP3n/PMfYy6tuIAllPWxQ1O8ezLLTqzacoiqIoinJn6MuUoiiKoijKBtytN985zHu9Lsxv\n9ddgrkvmkEn8BPJJSR5/cg9mycohSc6BdJaQR84BWfHaPsx4noVp9KxDJvxzyE2f2cGXH5N3oYiI\nm0DeSFwKiJfDPJguIXWJgWm9TNAWT6AwyBGZip/Qu26HkgTOAwqgFj5AGQxM2rUCps6LLiS2bVFR\nPqt5iGHULGFWntbxeX0CM/6qiXbsWJQzLiEZnGUUtLNGHk8J+ikhr73BFaTDIISpunsAuex8hWsW\n03V5xQkxBt+lQG+rGcZpQLmjUpI0wgUkn7BJMkmFMXFIgf6KCAFl90/Rryfk4dnMUdZhArlov47+\n3ibLAbWHR4H0yEtulWJuyiEkBv8l+nDPJa9Ycs6bUtu3ezDtN3cx12ZPSSbaQ1t3DiC1GlxGOj3M\ncfcR+l9EZDJGm41KmPT3WjjvOU2Lgxbq06lT3sEU9xiSF1JOQVUvMoznrmDuex4FW1xAMmlUuHFC\nOQG3ha1TsOMVjv0ZxmYRQ2LrhBSwNiTJ9hR9OThDO9RehwS7IukwdNHObg0y2ugMfVyEuOajGu5V\n83DOdLYeZLhD2y6GFXmJkUelc4E6tJqUy65OAVhDrB2LM8hIoybW6IM6xkGZQI6dO5R3sI81oU31\nny9fTW4+28R4MQXWfocCqZ6McHwwxdg3Kcp0FVBwUgqMuupjvauRh/roCdo0XqBvm0dor5OnkOmr\nHHP28z6t/ZT7T0SkQWv4lLzX0/Af3B7vxpDt3yKP7CMKhDzMMC7qK9z7bVqPihTr7pTm3b2Ac25i\nnh7sQFJ0n1Bevw+BWqYURVEURVE2QF+mFEVRFEVRNuBOZb7LNkyrtgYz270BBbfLKH+Og/MbDUgA\nKZkJWw4F/TKojjeBuW5pYQ5stmG6reowPzaHVIYOzIqvJTBv93rr+dwqivvoBxQ00FDAUB8mREuB\nIr0YZT0qYIrsNmGuNS7qvFpRsDYKENogr6IVdecwI9cV8iLcFnFJbUHy5ZxMsuEKbeocPro97l39\nMs4nbym3BrP9Q/LAGgtynNVWJKGSp9KM5EWTkmRwVvuSn3s5PFJERIoU8o+JIb2NnqL/GjUKjHdA\n3lnUx5xTqnlK7dIiLzSShewjjEdDKkEVUWBPg/4rjkmz3iJtDvh6ikpc+JBi2gbyTjpE/3fI0+u0\ngzb2PZjqgxhz+TKC3FTL0bd+9A7O76CtC8o117uPdmlO0S6L+XqAvZ0WTPT2HOUr9jBOdnL0zz55\nEgoF1ZyGaJfXMvK2HKFdBiT5OSQZLGL0uT+CHDInL6mYPJm3RbJkD2KsITHlJUxPsc4mPspWv0T7\nOnXMzVYPbRiPUeaScl8WbZL/ziFldyKsjcESOu2oontRMEp/tb6d4pJyOXo1yJbuBXlaLml9NFiz\n2+RtN1/CqzugINA7tP1k3sA64PbYMxOy5Swh6XSFPq68dal5W+ymaG+3eR9/uMBa2Bphfq32MF9O\n3sb60pxRYFO6fr7EXL7MMT9sF+MlC9BGCQW5dC5xpcrS2t9EHyzLda+4tWdEgblzabAurDyMJYe2\n0Aws7pc4OCenrSAJeeHN57h+SM+afIjPq0eYI9MxnhfB4UezNallSlEURVEUZQP0ZUpRFEVRFGUD\n7lTma05IbvNgNr2oU461K8gvuySfTH2Y90wLXkVmAk+EjCzmyxaqdj+FSftJDvNu7wymvrxJJkAP\nJsN8hTJckilVRMSjwJiRAy+rWgCpY6cOk/VqgJx04wU+X+zj3k0LM6NXwpwsNdShTTm2TuDAJzKF\n+TQOIB0m248LKMmM+qNG3i2U42x1ijKsriAleORRNZiirdoOZJvhDJ4XNXLuadcoIJ9FmzgWptrJ\nGI2SkAzaiNGeC7MetFMMPMxWJ9T/NYzNegjz+RDOUGsD77iDclxSIEwvwudtCjx3MUfl9nP039LC\nTP6E6nB4/mpy85XPMTenNcwLj6Sqaoz+jKcUSI/qHMwpGOIOBfz0YXo/THHOXCABlZSDUVKM8XAO\nc/6S2r3RRX9wri0REUNSzAFJN6Ml5VLbheyRF8gT12hgXvc/j3KcGvQnD586eRWuTkhKK1FYb488\n13wKOjxeX1O2QZGiXSYrtOkj8kyePMSY3Rl+3e1xWT69PV7W0TfRDOtbWkP7jC8xPuIaxsciQr3i\nBbwx5wnOqQeQ3fIxyYiy7hXnWYz/qwrXDUPyEqRgoHaFc1LBWHMoD+IzctTar0EKW5CUawcU+Jie\nP05AnmNr3oWo2zY5DtB+VzUKRktBOLuvo87Dd/D54x3yZibH0XAAaW/axHd39rGWhzn69jxAO4bs\n4VugbLUG+sOtsJ6MnHXZtkFzx3i4x5sBxsBL2vNQXGE8v0gwnhspvpuf4fwBbTXwLMZFPcS8a34C\nbbQ75fyNmMvm5KNtqVDLlKIoiqIoygboy5SiKIqiKMoG3KnMtwwgz+24sDl+4SXMqc3m49vjoQMJ\n78EB5TZ7SbrVLnmcFLAfPlvALLlDngjdEMermDx75vjcDWAyXHbJi3C6bsYtM/ImIe+Dlkf5/K7I\nKy2AhFdQQEs/gen2xIUJdUco+Bx52VxSkLnMoExeC9LGjGS49Jzkwi1hGxTkkIKyTchkGvbIQ8zi\nvd0lee5Nyms3XeKarQ4kP4dy0xXk7ZlRW/kUeG4awoRdm6Jfl0u0w9hfNz33KdecoWiTzgoBHBNL\nsoKHvjcOzhkmqGc7QlnjMcp30oBZvV574/Z45kPOkAKB9w4FHlA1Q4nUtkhG+amiS9Rhegm51aFg\niLUd8jqlgKenZJ7fzSFrp1MKNkpzZTpFu9crylfp4fMmSdbVBGb4aQN5CmsUtE9EZE5tWVxCzosE\n3888yvdZx7WmOeo/2UOfV+Sd2p6QJ3CCcxY0LxzKP7ojOKfMaBx1th+0c0yBDXv7kFvSAe7V6GJM\npeTNVzSQa60iD9mU2sSdoq2DI6zLtqBjCmC5DCl/6h7mypyCIzsWcy721yXb6RxjIYnQlw556rba\nGI/DAuNrLFgLFhXWxCatKWND3oZNPKPmlnK9UrDY4YI8h+vo4zl5o22ToI+1pjeg7SG0hcQlz/Is\nxlaZOW0niWgrS9jDvD67ILnsEfrWJ9fkoMDcnCTfcHvc38O6NvBw33L85u1xM36PlJ2i7YcRxskL\nyjvYTrGmtGjelQ7lNSWJ1TmClN8ir+M6bYnxLAXOvYRXZEJ5PH2Lsnptiqb9IVDLlKIoiqIoygbo\ny5SiKIqiKMoG3KnMF9cgJZwJmX4DmBldS6a7Cibhc/JU49iZ9Qzn5w14FnyS8nH5DkyJiwSmPnKM\nEJ88Gl7MYN7zBCeZ9ZidUgxw7+MS97jYgzk1WFBQO/JKsexV1MF1ejWYK0+fw3Z77wHaqz4iqdLg\n80EEGaYckmm1vn13vtDDfU1GeRZbFGxxgvIMW6jvfXqFv4xQl6aQdyVLs5QTL/dRrx6ZhZ/l+O4+\nLLty7uHzgPLxhcG6zFcEFPC1hLnejalMLZQjIq+S2gz9Omnj2CFvk4lgLDsF2iKyJIVmlLuxhCzW\n6OO+S/tqgnYWK8oteQAz+cFLkgAmJH++Rt5aKeWQpH64ojnY7VIeLZIbOj5kAp+CMNZISokDXGfh\n4XPvFP20EIxBEZGA8kXOK8hBVQvSUhNFksKB5900g8TQr1D/hPIOLpsoh38KeaO1RL+tjuAh+m6E\nfjsiD7uoWs9btg1qOa5fjnH8gnKG3kvRf/M5PQZ6aMe4oHmQofzLCH2ckezUTHGOqcFTr05rffIU\nMnW5h8+nM5TBmPXH0srFvFix1+4+Pl/mlOMxpsCLV1hDR5SnL6KgjecB5LxjCsBqKAfqxRm2jeQ9\nkj9TyF/x+D0PiC0RUPDMySXNHUFbFhSQMutj3XUjLIaLJ2ivcYC+7e5hXI/Jcy6mfJKtR2iLZQ5Z\neHpJa2ofz98whMR/MVvvz/0mnnEJeUv7tG6vIsz5LELdCpII71PA26cuxnNUQMIbFSjf1xxhDc6X\nmLPBPXjyCo29KNagnYqiKIqiKHeGvkwpiqIoiqJswJ3KfBEssTJdwD2iTSbzoAnTeKt4dHtckIdd\njzxFHIH3SUH51qaUC61JngFVH/eqk6l33oOE4eWQHoYrmIx7s3XPm/gNXNcjD5f2FeUg9MmTqIIJ\nNacAZQXJCiczNFLHUI6hBXmVUQqoijxo3Gdk6iYvDpNv3/wckfyRV6h7SXntDihQZfcKHlnPQ3iS\nHJYw8z7JYXrea0F2SSk/XEZeK3kb5+8vIE8klCtunzx4lhS8MSa5QESE1GVxBffO+/iDN8f3yflI\nsj7GSD2DyTyjIHZRBVN1PcSXl+QxZXOUqXYE+cCSl2pjuX3vLxGRY8rWdUKmfpcCiZYtmNWXnJPr\nPuXHbMO7qfMEkmfioP+bL3CdwOD33Cgjbz4Hx6sU1zwkb86pj36K5+ueVFOSiY8CjMnhCQZQQqtf\n8BRST9TC+YYC+CYJeWEu6PMQcyHtk/fjOer/oEnrCwUqNIYWxS3xPMO6sUOBFyckBXkk29V92tbg\nQJ6zM8ypc4qc2yav2xl5Wa9IKt6rIBU/yyhw5kO0c528jEODOTQI1/MsVk1cyz+nnIIkkYdHGEfZ\nEwo020V/xBcYR19okBc4jbXRClLVklz4xrt4LtUocKzjQVKbv8cLcVtUNchW5S7mUTB+dHt8GKGe\nsyV5NgYYg2EP3w1DzJeE8guGpLUFOdrl5AL9HBjMX4+8XYMFedrGmH++v77Wzq8ouHSJ8tFlpSCv\nvRWNpY6Fl/e8hs+bFGy16WNuBrsod5NkQX//07fHUYT5XgtRth3apvFhUMuUoiiKoijKBujLlKIo\niqIoygbcqcyXkdmwlcFkXjsg7xBLQdYoh1kwII+GFkyIaQUT3WEEc/KMvIoaDkx39VPyWqLgmtUI\n9w0pSGIvJ4mpWpdY2u+wBxHKF3dR7tyF6XOZo6zpHObhNgXbHBUwv5/UKM/Z2zDjRvdg6m5SAMC0\nDRPtG+TMdxa/Jw/dFqg5qO/bh2iXvYS8bVy0gxeiTfvkkRYmMOnvkefc5G0KDLhD/UcS2WxAQf/6\n6D+/Qbnfpgh+6VCQ1SwkVy4R8YWC9RkKkjlG/1kHEoXbhHl6eon+c3uUg+8C5uaqhz7wcEnZrWh8\nUN6xEcm0D0Y07vqvRkp4Z4DregXKcUnmenOO4/0DtN8qfff2OPllmNJDyuvXXaH+5z6+u6A8dR2S\nzvwl5a+jPp8XX7g9zign4vyM9F8RCWNIAG+51Ccl7p27kM5jQ3LmAvW/Kim3ZoBrBgn6beaQFyK1\nox/QvUgWT0tc5/gVqLYllXmwQJ/t1TBnFxFJsA3KrbhC2cSjdWwImWu2C4nILBUR3kAAAAsaSURB\nVNDuLnlWX1KQXt/gXtUQ54wMbblosSS8npvPeYr2XXTx/e4An48vaR3cQR3MKdaFOeVvjBaQfxYz\n9M3smII3Z/BIm59D/htXGO9OinEdha/mcbpP3rwN8sI8o2fo5RBzJKTchP0FtibM9vB8qKj+MXlF\nNs9Qh/Yu1rsZe0WTN3KZYqx5FCCzlqOfbBNtKiJSFrjugUfe+5SP85c9lO8ByZZVA+Vbki2o2SIZ\nPcP1jyraUuFjnd6nMXkcI8DoijxVV631cfhBqGVKURRFURRlA/RlSlEURVEUZQPuVObbp+CL0wim\n2DkF5zxgjz+Kq7hLXl+pj2IXZN67vICZMM4o2ObrkAPsOQVfq5EJP4LptprAfDzy4GXSGK/nGLoI\nYeL2KJecTCnAXQjPmnoKk2tOqcSWY5jNSfUQd4CcUasmBbi7InmygS+kJZlASTK6oiB+2+LtAhLn\nDuUpkx551RjkO+uTC+JLgRl2SXJR9xkFsIzRr/tjyv2XQB7tdNDODnlkvCAPqc4SfV8FlFOLJFQR\nkYo8yVo9CrqaoN3DEnUeUd61kDwquzPKldhFMLhaC2PTrVPwuDn6cmGPUYYhrmMFY+jFilxetkgj\nRz8UlEsrO0Qd3AxS82BGwSkpZ6UlyXqSo39OCwrOl6P/pUC7TCkQbNzAcUoBcRsUeLE8wyQyJeav\niMi4g3t0T9DGGQX3NCQzlTWMt5y2FDRIksyvMA5zCvLpl5DJOKhqRMEj5YzGxSHyMY7Mer7PbZAu\nyLuUxvKoj35tjMmL1IXk4Y3w+YiCrgYupBbvAn2ct9Gvc2qHvS7W92UF2TyJMJa9BT0PnkMKCqP1\nbQnFAQVappyjBeXpqwrOp4n71fdxzjihfIR1eGw7NawXS5e8brsklw3QT/MQ83Q2RBt9bkbjeosE\nTXhYDilPXUUBklPKOVmSZG0CrKmxfe32uNZFnecUCFf6GPuJj88j8v50yYMxiWgdpNyaDx2MhYGs\nPzfdLspnM/Tbr5JM2L3Avase6tmm52ODtoXMaQvRcYax2mihPlWdvM7pvtMA61p9F2PkXkNz8ymK\noiiKotwZ+jKlKIqiKIqyAXcq88kezMk+ee3lM5j0hyQBeBGCcCY1mFZdH6b6GnmDSQ3XvMxhMrz6\nDN4ZD++9fXtslpQjKyA5qA7zXjVFEzl7yEkkIjKbIZhasIBZe1WDibKWoW5WcE7+Evc2BzAhD8hz\nwQSoW+yQF+IQ9Z87MOmXY9SzXYNp9XFCESa3RM2FWd4jz6BwiLabOzCrvktBVP0Ess0p5fYSii0a\nUETFCx/XKeowT/cqmIunE/JC8SGbTikI3R55NV4t1wOy7Xkw+2dT1G2S4N7xMdq0M8D5q97j2+OU\nPFD3Y/L4K2AaX83IS8pHOXZYCiQPq2cWUlPz7NX8/gk7uK4rKFP27J3b42RJ5vZ7KNOMgjsa8oqN\nBHVwm5g7mYs57riUE5E8w1bkCdicQEowO7hOQsk1O2frEktCUnLoUDDfCnLz3jl5Kx1RANAK/X9F\n+cKO65inp+coU9ImuXlJnoOCMeI1Dm+PrX12e1xlH01K+DDENYxZx6Ugjzmk7TF5ZOUjkinnqPCK\n5L8dapNTko4imu/LAnJJmkPKDgP0a2OG9WphKeDnIQV/vFz3zIwoSOZFm+pGuV4DmrPzBepwMqdg\nkymum/vo+zF5dh06qNu7LzAGoxLtmBmsIxPK6VlYeA5vE2eOcdS8R16PA9R5N8N49ClA7JTk+xZ5\ndk5C9HkjQ92c+zTX5nhGuQb9KV1ItbsJ5P7SwfOtIE/53FkPwurMKXAu5W99HOFa+Zt4RtRDCn48\nprrtY52nONCyaOB+YQP1aQ0wXvxPYs22dZwfFSibXdDWnQ+BWqYURVEURVE2QF+mFEVRFEVRNuBu\nZb4Cgc+GBcypD8mbIjdPb4/bFcxs4wAm1N0xpITMwrSYhRRYbArZLtqnYIgU9CwUeAD4EQW0ewIT\neGOJ43f8dU+qwEJy2J2ibg4FEKx28f2hQBY0JFVOriiwHgXeHFNgtRnJkEkHJlTzDgWffAwz5tmU\nPJUo79W2yHdxzeQM7XhJaZgelTDJTpYIgCfkGRSSjNKgeo0d9PGSxso0hdeeR04iBUmBuQcvSK+J\ndlvRNYtiXUqYXMKMPd1FOx5SbrrsEm360qC/ayvUv0PelacU2LHdQx9XKQUPXWEclDHlHUsQJPFx\nhmtO/Vfz+ycXtLG3oACTdRy3KI/WYghTf9chr6IGzsnnkNfiOTyJTOdXcGPKhcYBACckQ3khOtqp\nsFbskHyQd9e94uqUt615n9aIC9xj6aGfY0Pesi1IQPsZxtKMJDnTx3ebDYxb/wXapXiAsZeWqMOS\nPEzj9rpX6TaoLK0htIUiyknKjDFmqyWvObhOSXLeyZzyRnbIm4+2azhLrKGXI6xLLgXdtTHG77HF\ndbIJ5t9JRN5lInI4x3cqkrxCyp32Yo5rzSKsO/4KY6Q6RR9EDQrmW8Nz5pzyeuaUgHOQUiBICmRb\nJahbewkpd5ukIXmm0vNo1sL8qgc0Ni/g/ddsYewfkVx+GmA97pI38vwC/Rkf0rwhb9wZyfRt6s+K\nxvWcvHFfL9e3VLiCsRF/I86b1zDGRi/xeScgL9o27v2Qctdmr9H2Atry0SCPvNUR2u74mNY4Cgrc\npYDNTkdz8ymKoiiKotwZ+jKlKIqiKIqyAXcq853lMN+GFpKG75BURbnwTiewOQdU1PwFzLXVHkyL\nuYU0UhXY6V9NIQ2UOUx3Uwuz3+SccvIYmPZjgbeBHa4n0kpGuO7ZPUh+AQUvWw5hcn3bwIT8afI2\nyjvwMLwYIE+Q40DSSoZoI4/kxvOHOOdoce/22J3CjLuiwGrbojZDXyaUaykiT8N8l0zvJcynUUrt\n24QJtyop51cFE7MluaF7CZN8Qt6OQwpCd1TgXssMEoyzi3Y4HKMMIiKn5PXRXMHjq3Rx3STHtRoY\nFhIYmP1P2hRsjgLDWcoP9/+2d387TQRRHIC3pbS0RQxoVGwi0RgfgPd/CB/AC2OCiH+pQIBYul6Y\n+DskJlwM3n3fVUPapbM7e/ZkTmdmWsqfo7LQ6rjsZ3Z+mfLP11WG88eL/zSbby8Nmm2WEshW2ra1\nVYbxR3k9/Jn76EnZa/DzPG2b1FL+OjNzDw5Sbl2eZGj/6meO82z+7u/r61U+O3hQ9jsc3V7ocafE\nmuVljvtsJ31m9S0lyX6WuLDxKeWDX4P0heudlBJfrtNvj/L2bvGynKOycPC47OW4tVn/3t275TBt\n3zsui1aOE09v9ksZ/bL8FGGU7/nkrMTfsofa6ZeUBSfTxKV12eNuvZFYNzjKtZ/up8T99jj38t5O\n4lVfFjfuuq77McmM0utJyoGnJ6WcP0rse1Bmby5Ln70os7Rv1rmW84/pa+elvLy7nXZuDnK+PpS9\nXi8O0rbF8P5nTf85bs5x/zwxcneZuDtY59n36rA8K9/nvv62l5j6+iYz507LvfPjac7FcFRmlk8T\nv96UhYNfDNP+s3Ep2T0sz+ivtzt5P8u16of539NJ7qnZPDFoWUp726vEo6suCwp/v0rb9g/Tf8qv\nTrpVKVX2XdqwmOf7PXpc9pY9uz17/y5GpgAAGkimAAAaDPq+v/tdAAD8k5EpAIAGkikAgAaSKQCA\nBpIpAIAGkikAgAaSKQCABpIpAIAGkikAgAaSKQCABpIpAIAGkikAgAaSKQCABpIpAIAGkikAgAaS\nKQCABpIpAIAGkikAgAaSKQCABpIpAIAGkikAgAaSKQCABpIpAIAGkikAgAa/Ae4MSLggIKYhAAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ec83225240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
